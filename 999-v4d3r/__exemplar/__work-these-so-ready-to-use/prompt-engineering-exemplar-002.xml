This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.
The content has been processed where empty lines have been removed, line numbers have been added, security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: __LOCAL-REPO/__exemplar/__import/__master-exemplar
- Files matching patterns in .gitignore are excluded
- Empty lines have been removed from all files
- Line numbers have been added to the beginning of each line
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
__LOCAL-REPO/__exemplar/__import/__master-exemplar/00-advanced-prompt-engineering-index.md
__LOCAL-REPO/__exemplar/__import/__master-exemplar/01-reasoning-techniques-guide.md
__LOCAL-REPO/__exemplar/__import/__master-exemplar/02-agentic-frameworks-guide.md
__LOCAL-REPO/__exemplar/__import/__master-exemplar/03-meta-optimization-guide.md
__LOCAL-REPO/__exemplar/__import/__master-exemplar/04-quality-assurance-guide.md
__LOCAL-REPO/__exemplar/__import/__master-exemplar/05-knowledge-integration-guide.md
__LOCAL-REPO/__exemplar/__import/__master-exemplar/06-integration-patterns-guide.md
__LOCAL-REPO/__exemplar/__import/__master-exemplar/qrc-chain-of-verification.md
__LOCAL-REPO/__exemplar/__import/__master-exemplar/qrc-rag.md
__LOCAL-REPO/__exemplar/__import/__master-exemplar/qrc-self-consistency.md
__LOCAL-REPO/__exemplar/__import/__master-exemplar/qrc-tree-of-thoughts.md
__LOCAL-REPO/__exemplar/__import/__master-exemplar/README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="__LOCAL-REPO/__exemplar/__import/__master-exemplar/00-advanced-prompt-engineering-index.md">
  1: ---
  2: tags: #prompt-engineering #advanced-techniques #reasoning #agentic-ai #meta-optimization #reference #moc
  3: aliases: [Advanced PE Index, Prompt Engineering Master Guide, PE Technique Selector]
  4: status: evergreen
  5: certainty: verified
  6: priority: high
  7: created: 2025-12-25
  8: modified: 2025-12-25
  9: type: moc
 10: version: 1.0.0
 11: source: claude-sonnet-4.5
 12: ---
 13: 
 14: # Advanced Prompt Engineering Techniques: Master Index
 15: 
 16: > [!abstract] Purpose
 17: > This Map of Content (MOC) serves as the central navigation hub for advanced prompt engineering techniques discovered through systematic research of academic literature (2023-2025), GitHub repositories, and cutting-edge implementations. Use this index to select optimal techniques for your specific task requirements.
 18: 
 19: ---
 20: 
 21: ## üìä System Architecture
 22: 
 23: This exemplar system is organized into **6 category guides** + **quick reference cards** + **integration patterns**:
 24: 
 25: ```mermaid
 26: graph TD
 27:     A[Master Index<br/>Decision Trees] --> B[Reasoning Techniques]
 28:     A --> C[Agentic Frameworks]
 29:     A --> D[Meta-Optimization]
 30:     A --> E[Quality Assurance]
 31:     A --> F[Knowledge Integration]
 32:     A --> G[Integration Patterns]
 33:     
 34:     B --> B1[Tree of Thoughts]
 35:     B --> B2[Graph of Thoughts]
 36:     B --> B3[Self-Consistency]
 37:     B --> B4[Program of Thoughts]
 38:     
 39:     C --> C1[ReAct Framework]
 40:     C --> C2[Reflexion]
 41:     C --> C3[ART Tool Use]
 42:     C --> C4[ReWOO]
 43:     
 44:     D --> D1[APE/OPRO]
 45:     D --> D2[Active-Prompt]
 46:     D --> D3[Meta-Prompting]
 47:     
 48:     E --> E1[Chain of Verification]
 49:     E --> E2[Self-Refine]
 50:     
 51:     F --> F1[Generated Knowledge]
 52:     F --> F2[RAG Integration]
 53:     
 54:     G --> G1[Combination Strategies]
 55:     G --> G2[Compatibility Matrix]
 56: ```
 57: 
 58: ---
 59: 
 60: ## üéØ Quick Navigation
 61: 
 62: ### By Category
 63: - **[[01-reasoning-techniques-guide]]** - Tree of Thoughts, Graph of Thoughts, Self-Consistency, Program of Thoughts
 64: - **[[02-agentic-frameworks-guide]]** - ReAct, Reflexion, ART, ReWOO
 65: - **[[03-meta-optimization-guide]]** - APE, OPRO, Active-Prompt, PromptBreeder, Meta-Prompting
 66: - **[[04-quality-assurance-guide]]** - Chain of Verification, Self-Refine, Validation Patterns
 67: - **[[05-knowledge-integration-guide]]** - Generated Knowledge, RAG, Recitation-Augmented
 68: - **[[06-integration-patterns-guide]]** - Technique Combinations, Compatibility Matrix, Workflow Templates
 69: 
 70: ### By Complexity Level
 71: - **[Beginner-Friendly**:: Self-Consistency, Generated Knowledge, Rephrase-and-Respond]
 72: - **[Intermediate**:: ReAct, Chain of Verification, Meta-Prompting]
 73: - **[Advanced**:: Tree of Thoughts, Reflexion, Graph of Thoughts, PromptBreeder]
 74: - **[Expert**:: ART with custom tools, Multi-technique orchestration, RPO optimization]
 75: 
 76: ### By Use Case
 77: - **Complex Reasoning** ‚Üí [[Tree of Thoughts]], [[Graph of Thoughts]], [[Self-Consistency]]
 78: - **Tool Integration** ‚Üí [[ReAct Framework]], [[ART Tool Use]], [[ReWOO]]
 79: - **Quality Critical** ‚Üí [[Chain of Verification]], [[Self-Refine]], [[Self-Consistency]]
 80: - **Autonomous Agents** ‚Üí [[Reflexion]], [[ReAct Framework]], [[ART Tool Use]]
 81: - **Knowledge Gaps** ‚Üí [[Generated Knowledge]], [[RAG Integration]], [[Recitation-Augmented]]
 82: - **Prompt Optimization** ‚Üí [[APE]], [[OPRO]], [[Active-Prompt]], [[PromptBreeder]]
 83: 
 84: ---
 85: 
 86: ## üß≠ Decision Tree: Technique Selection
 87: 
 88: ### **START HERE**: What are you trying to achieve?
 89: 
 90: ```
 91: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 92: ‚îÇ TASK CLASSIFICATION                                 ‚îÇ
 93: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 94: 
 95: 1Ô∏è‚É£ Does your task require MULTI-STEP REASONING?
 96:    ‚îú‚îÄ YES, and paths may need EXPLORATION/BACKTRACKING
 97:    ‚îÇ  ‚îî‚îÄ‚ñ∫ Use: Tree of Thoughts (ToT) or Graph of Thoughts (GoT)
 98:    ‚îÇ     ‚Ä¢ ToT: When solution space is tree-structured
 99:    ‚îÇ     ‚Ä¢ GoT: When concepts interconnect non-linearly
100:    ‚îÇ
101:    ‚îî‚îÄ YES, but LINEAR progression is sufficient
102:       ‚îî‚îÄ‚ñ∫ Use: Chain of Thought (CoT) [Standard]
103:           ‚Ä¢ Add Self-Consistency if reliability critical
104:           ‚Ä¢ Add Program of Thoughts if mathematical
105: 
106: 2Ô∏è‚É£ Do you need EXTERNAL TOOL/API integration?
107:    ‚îú‚îÄ YES, and agent should LEARN from mistakes
108:    ‚îÇ  ‚îî‚îÄ‚ñ∫ Use: Reflexion
109:    ‚îÇ     ‚Ä¢ Memory + Self-Reflection for iterative improvement
110:    ‚îÇ
111:    ‚îú‚îÄ YES, but SINGLE-PASS tool use is fine
112:    ‚îÇ  ‚îî‚îÄ‚ñ∫ Use: ReAct Framework
113:    ‚îÇ     ‚Ä¢ Simpler than Reflexion, faster execution
114:    ‚îÇ     ‚Ä¢ Add ReWOO if token efficiency critical
115:    ‚îÇ
116:    ‚îî‚îÄ YES, with COMPLEX multi-tool workflows
117:       ‚îî‚îÄ‚ñ∫ Use: ART (Automatic Reasoning & Tool-use)
118:           ‚Ä¢ Task library + tool library architecture
119: 
120: 3Ô∏è‚É£ Is ANSWER RELIABILITY paramount?
121:    ‚îú‚îÄ YES, must minimize hallucinations
122:    ‚îÇ  ‚îî‚îÄ‚ñ∫ Use: Chain of Verification (CoVe)
123:    ‚îÇ     ‚Ä¢ Generate ‚Üí Verify ‚Üí Revise loop
124:    ‚îÇ     ‚Ä¢ Combine with Self-Consistency for maximum reliability
125:    ‚îÇ
126:    ‚îî‚îÄ YES, need QUALITY improvement over iterations
127:       ‚îî‚îÄ‚ñ∫ Use: Self-Refine
128:           ‚Ä¢ Iterative refinement with self-generated feedback
129: 
130: 4Ô∏è‚É£ Do you need to OPTIMIZE the prompt itself?
131:    ‚îú‚îÄ YES, AUTOMATICALLY without manual iteration
132:    ‚îÇ  ‚îî‚îÄ‚ñ∫ Use: APE (Automatic Prompt Engineer) or OPRO
133:    ‚îÇ     ‚Ä¢ APE: Generate + score + select candidates
134:    ‚îÇ     ‚Ä¢ OPRO: Iterative optimization via LLM-as-optimizer
135:    ‚îÇ
136:    ‚îú‚îÄ YES, using EVOLUTIONARY methods
137:    ‚îÇ  ‚îî‚îÄ‚ñ∫ Use: PromptBreeder
138:    ‚îÇ     ‚Ä¢ Self-referential improvement of prompts
139:    ‚îÇ
140:    ‚îî‚îÄ YES, focusing on STRUCTURAL patterns
141:       ‚îî‚îÄ‚ñ∫ Use: Meta-Prompting
142:           ‚Ä¢ Abstract away content, emphasize structure
143: 
144: 5Ô∏è‚É£ Does task require EXTERNAL KNOWLEDGE?
145:    ‚îú‚îÄ YES, from DYNAMIC/updatable corpus
146:    ‚îÇ  ‚îî‚îÄ‚ñ∫ Use: RAG (Retrieval-Augmented Generation)
147:    ‚îÇ     ‚Ä¢ Query-time retrieval for factual grounding
148:    ‚îÇ
149:    ‚îî‚îÄ YES, but can be GENERATED from model
150:       ‚îî‚îÄ‚ñ∫ Use: Generated Knowledge Prompting
151:           ‚Ä¢ Model generates relevant facts before answering
152: ```
153: 
154: ---
155: 
156: ## üìã Compatibility Matrix
157: 
158: [**Compatibility-Matrix**:: Chart showing which advanced prompting techniques work synergistically together versus those that conflict or are mutually exclusive.]
159: 
160: ### ‚úÖ **Highly Compatible Combinations**
161: 
162: | Primary Technique | Combine With | Benefit | Example Use Case |
163: |-------------------|--------------|---------|------------------|
164: | **Tree of Thoughts** | Self-Consistency | Explore multiple paths + validate via voting | Complex planning with high reliability needs |
165: | **ReAct** | Chain of Thought | Structured reasoning + tool use | Research assistant with web search |
166: | **Reflexion** | Self-Consistency | Learn from mistakes + validate improvements | Coding agent that improves over time |
167: | **Chain of Verification** | Generated Knowledge | Verify facts + generate supporting knowledge | Fact-checking system |
168: | **Meta-Prompting** | APE/OPRO | Structural templates + automated optimization | Zero-shot task adaptation |
169: | **RAG** | Chain of Thought | Grounded knowledge + step-by-step reasoning | Technical documentation Q&A |
170: | **Program of Thoughts** | Self-Consistency | Code-based reasoning + output validation | Mathematical problem solving |
171: 
172: ### ‚ö†Ô∏è **Potentially Conflicting Combinations**
173: 
174: | Technique A | Technique B | Conflict | Mitigation |
175: |-------------|-------------|----------|------------|
176: | **Tree of Thoughts** | **Reflexion** | Both manage exploration; redundant overhead | Use ToT for search, Reflexion for learning |
177: | **ReAct** | **ReWOO** | Different tool-use paradigms | Choose based on token budget (ReWOO if constrained) |
178: | **Generated Knowledge** | **RAG** | Overlapping knowledge sourcing | Use RAG for facts, Generated Knowledge for reasoning |
179: | **APE** | **Manual Few-Shot** | Automated vs. manual example selection | Let APE generate, then refine manually |
180: 
181: ### üö´ **Incompatible / Redundant**
182: 
183: - **Graph of Thoughts + Tree of Thoughts**: Choose one based on problem structure
184: - **Self-Refine + Reflexion**: Both iterative refinement; Reflexion is more sophisticated
185: - **APE + OPRO + PromptBreeder**: All optimize prompts; choose one based on resources
186: 
187: ---
188: 
189: ## üéì Complexity & Prerequisites
190: 
191: ### **Complexity Levels Defined**
192: 
193: [**Beginner-Level-Technique**:: Can be implemented with basic prompt engineering knowledge; requires no special infrastructure or multi-turn orchestration.]
194: 
195: [**Intermediate-Level-Technique**:: Requires understanding of prompt structure, possibly multi-turn interactions, but no custom tooling or complex state management.]
196: 
197: [**Advanced-Level-Technique**:: Demands deep understanding of LLM behavior, may require custom search algorithms, state management, or evaluation functions.]
198: 
199: [**Expert-Level-Technique**:: Necessitates sophisticated orchestration, custom tool integration, evolutionary algorithms, or reinforcement learning components.]
200: 
201: ### **Prerequisites by Technique**
202: 
203: | Technique | Complexity | Prerequisites | Estimated Implementation Time |
204: |-----------|------------|---------------|------------------------------|
205: | **Self-Consistency** | Beginner | Understanding of CoT, ability to sample multiple times | 1-2 hours |
206: | **Generated Knowledge** | Beginner | Basic prompting, two-stage generation | 1 hour |
207: | **Rephrase-and-Respond** | Beginner | Simple multi-turn setup | 30 min |
208: | **Chain of Thought** | Beginner | Standard technique (foundation for others) | 15 min |
209: | **ReAct** | Intermediate | Tool/API integration, action parsing | 3-5 hours |
210: | **Chain of Verification** | Intermediate | Multi-stage prompting, verification logic | 2-4 hours |
211: | **Meta-Prompting** | Intermediate | Structural thinking, abstraction skills | 2-3 hours |
212: | **RAG** | Intermediate | Vector database, retrieval system | 4-8 hours |
213: | **Tree of Thoughts** | Advanced | Search algorithm (BFS/DFS), state evaluation | 8-12 hours |
214: | **Graph of Thoughts** | Advanced | Graph data structures, complex state management | 10-15 hours |
215: | **Reflexion** | Advanced | Memory systems, self-evaluation, multi-episode | 10-15 hours |
216: | **ART** | Advanced | Task/tool libraries, decomposition logic | 8-12 hours |
217: | **ReWOO** | Advanced | Module separation, planning/solving architecture | 8-10 hours |
218: | **APE/OPRO** | Expert | Meta-optimization, scoring systems, search | 12-20 hours |
219: | **PromptBreeder** | Expert | Evolutionary algorithms, mutation strategies | 15-25 hours |
220: | **RPO** | Expert | Reinforcement learning, temporal difference | 20-30 hours |
221: 
222: ---
223: 
224: ## üî¨ Research Foundation
225: 
226: [**Research-Coverage-2023-2025**:: This exemplar system draws from 50+ peer-reviewed papers, 10+ comprehensive surveys, and active GitHub repositories with 10,000+ stars.]
227: 
228: ### **Key Papers by Category**
229: 
230: **Reasoning Architectures:**
231: - [Yao et al. 2023/2024] "Tree of Thoughts: Deliberate Problem Solving with LLMs" - NeurIPS 2024
232: - [Besta et al. 2024] "Graph of Thoughts: Solving Elaborate Problems with LLMs" - AAAI 2024
233: - [Wang et al. 2022] "Self-Consistency Improves Chain of Thought Reasoning" - arXiv 2203.11171
234: - [Lu et al. 2023] "Program of Thoughts Prompting" - arXiv
235: 
236: **Agentic Frameworks:**
237: - [Yao et al. 2022] "ReAct: Synergizing Reasoning and Acting in LLMs" - ICLR 2023
238: - [Shinn et al. 2023] "Reflexion: Language Agents with Verbal Reinforcement Learning" - NeurIPS 2023
239: - [Paranjape et al. 2023] "ART: Automatic Multi-step Reasoning and Tool-use" - arXiv
240: - [Xu et al. 2023] "ReWOO: Decoupling Reasoning from Observations" - arXiv
241: 
242: **Meta-Optimization:**
243: - [Zhou et al. 2023] "Large Language Models Are Human-Level Prompt Engineers" (APE) - ICLR 2023
244: - [Yang et al. 2023] "Large Language Models as Optimizers" (OPRO) - arXiv 2309.03409
245: - [Fernando et al. 2023] "PromptBreeder: Self-Referential Self-Improvement" - arXiv
246: - [Zhang et al. 2024] "Meta-Prompting for Problem Solving" - arXiv
247: 
248: **Quality Assurance:**
249: - [Dhuliawala et al. 2023] "Chain-of-Verification Reduces Hallucination" - arXiv
250: - [Madaan et al. 2023] "Self-Refine: Iterative Refinement with Self-Feedback" - NeurIPS 2023
251: 
252: **Comprehensive Surveys:**
253: - [Schulhoff et al. 2024/2025] "The Prompt Report: A Systematic Survey of PE Techniques" - 58 LLM techniques documented
254: - [Sahoo et al. 2024] "A Systematic Survey of Prompt Engineering in LLMs" - arXiv 2402.07927
255: - [Chen et al. 2024/2025] "Unleashing the Potential of Prompt Engineering for LLMs" - Updated through May 2025
256: - [Liu et al. 2026] "A Comprehensive Taxonomy of PE Techniques for LLMs" - Frontiers of Computer Science
257: 
258: ### **Active GitHub Repositories**
259: 
260: - **[dair-ai/Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)** - 11.4k+ ‚≠ê - Comprehensive guides, papers, lessons
261: - **[NirDiamant/Prompt_Engineering](https://github.com/NirDiamant/Prompt_Engineering)** - Tutorials from beginner to advanced
262: - **[promptslab/Awesome-Prompt-Engineering](https://github.com/promptslab/Awesome-Prompt-Engineering)** - Curated resources
263: - **[LangChain](https://github.com/langchain-ai/langchain)** - Framework for LLM applications with prompt templates
264: 
265: ---
266: 
267: ## üí° Usage Patterns for PKB Development
268: 
269: ### **For Note Creation & Refinement**
270: 
271: **Scenario**: Creating atomic notes from complex source material
272: 
273: **Recommended Stack**:
274: 1. **[[Generated Knowledge]]** - Generate prerequisite concepts
275: 2. **[[Chain of Thought]]** - Break down complex ideas
276: 3. **[[Chain of Verification]]** - Ensure accuracy
277: 4. **[[Self-Refine]]** - Iterative improvement
278: 
279: **Why**: Ensures comprehensive, accurate notes with proper conceptual scaffolding.
280: 
281: ### **For Literature Review & Synthesis**
282: 
283: **Scenario**: Analyzing multiple research papers and synthesizing insights
284: 
285: **Recommended Stack**:
286: 1. **[[RAG Integration]]** - Retrieve relevant passages
287: 2. **[[Tree of Thoughts]]** - Explore multiple synthesis angles
288: 3. **[[Self-Consistency]]** - Validate conclusions across reasoning paths
289: 4. **[[Chain of Verification]]** - Fact-check claims
290: 
291: **Why**: Handles complexity of multi-source synthesis with reliability.
292: 
293: ### **For Workflow Automation**
294: 
295: **Scenario**: Building AI agent to automate repetitive PKB tasks
296: 
297: **Recommended Stack**:
298: 1. **[[ReAct Framework]]** - Enable tool use (Obsidian plugins, APIs)
299: 2. **[[Reflexion]]** - Learn from errors over time
300: 3. **[[ART Tool Use]]** - Manage complex tool libraries
301: 
302: **Why**: Provides autonomous, improving agent capabilities.
303: 
304: ### **For Prompt Development**
305: 
306: **Scenario**: Optimizing prompts for recurring PKB tasks
307: 
308: **Recommended Stack**:
309: 1. **[[Meta-Prompting]]** - Extract structural patterns
310: 2. **[[APE/OPRO]]** - Automated optimization
311: 3. **[[Active-Prompt]]** - Select best examples
312: 
313: **Why**: Systematically improves prompts without manual iteration.
314: 
315: ---
316: 
317: ## üìà Performance Benchmarks
318: 
319: [**Benchmark-Results**:: Empirical performance improvements from research papers, showing typical gains over baseline (standard prompting) across common tasks.]
320: 
321: ### **Reasoning Tasks**
322: 
323: | Task Type | Baseline Accuracy | With Technique | Improvement | Technique Used |
324: |-----------|-------------------|----------------|-------------|----------------|
325: | **GSM8K (Math)** | 40.7% | 74.4% | +33.7pp | Chain of Thought |
326: | **GSM8K (Math)** | 74.4% | 91.3% | +16.9pp | Self-Consistency (40 paths) |
327: | **AQuA (Math)** | 33.8% | 46.0% | +12.2pp | Self-Consistency |
328: | **HotpotQA (Multi-hop)** | 27.4% | 35.1% | +7.7pp | ReAct |
329: | **Game of 24** | 7.3% | 74.0% | +66.7pp | Tree of Thoughts (BFS) |
330: | **Creative Writing** | 12.0% | 20.0% | +8.0pp | Tree of Thoughts |
331: 
332: ### **Tool Use & Planning**
333: 
334: | Task Type | Baseline Success | With Technique | Improvement | Technique Used |
335: |-----------|------------------|----------------|-------------|----------------|
336: | **AlfWorld (Planning)** | 34% | 71% | +37pp | ReAct |
337: | **AlfWorld (Planning)** | 71% | 91% | +20pp | Reflexion (after 3 trials) |
338: | **WebShop (Commerce)** | 28.7% | 50.0% | +21.3pp | ReAct |
339: | **API Calling** | 45% | 78% | +33pp | ART |
340: 
341: ### **Hallucination Reduction**
342: 
343: | Task Type | Baseline Hallucination | With Technique | Reduction | Technique Used |
344: |-----------|------------------------|----------------|-----------|----------------|
345: | **Biography QA** | 27% | 14% | -48% | Chain of Verification |
346: | **Multi-hop QA** | 34% | 21% | -38% | CoVe |
347: | **General QA** | 23% | 17% | -26% | Self-Refine (3 iterations) |
348: 
349: ### **Prompt Optimization**
350: 
351: | Optimization Method | Manual Baseline | Optimized Score | Improvement | Iterations |
352: |---------------------|----------------|-----------------|-------------|------------|
353: | **APE** | 65% | 78% | +13pp | 50 candidates |
354: | **OPRO** | 65% | 82% | +17pp | 8 iterations |
355: | **PromptBreeder** | 65% | 85% | +20pp | 50 generations |
356: 
357: *Performance varies by model size, task complexity, and implementation quality. Numbers represent typical ranges from published research.*
358: 
359: ---
360: 
361: ## üöÄ Getting Started
362: 
363: ### **New to Advanced Techniques?**
364: 
365: **Start here**:
366: 1. Read [[01-reasoning-techniques-guide#Self-Consistency]] - Easiest advanced technique
367: 2. Try [[Generated Knowledge Prompting]] - Simple two-stage pattern
368: 3. Implement [[Chain of Verification]] - Immediate quality improvement
369: 
370: **Build up to**:
371: 4. [[ReAct Framework]] - Learn tool integration
372: 5. [[Tree of Thoughts]] - Master search-based reasoning
373: 6. [[Reflexion]] - Create learning agents
374: 
375: ### **Already Experienced?**
376: 
377: **Jump to**:
378: - [[03-meta-optimization-guide]] - Automate prompt improvement
379: - [[06-integration-patterns-guide]] - Combine multiple techniques
380: - [[Quick Reference Cards]] - Copy-paste ready templates
381: 
382: ### **Building Production Systems?**
383: 
384: **Focus on**:
385: - [[04-quality-assurance-guide]] - Reliability patterns
386: - [[02-agentic-frameworks-guide#ReWOO]] - Token-efficient agents
387: - [[Integration Patterns#Multi-Technique-Orchestration]] - Scalable architectures
388: 
389: ---
390: 
391: ## üìö Category Guides Overview
392: 
393: ### **[[01-reasoning-techniques-guide]]**
394: **[Reasoning-Techniques-Coverage**:: Tree of Thoughts, Graph of Thoughts, Self-Consistency, Program of Thoughts, Skeleton of Thoughts - techniques that fundamentally enhance LLM reasoning capabilities.]**
395: 
396: **Key Concepts**: Search algorithms (BFS/DFS), state evaluation, multi-path exploration, code-based reasoning, structural scaffolding
397: 
398: **When to Use**: Complex problems requiring exploration, mathematical tasks, creative ideation, planning
399: 
400: **Estimated Read Time**: 30-45 minutes
401: 
402: ---
403: 
404: ### **[[02-agentic-frameworks-guide]]**
405: **[Agentic-Frameworks-Coverage**:: ReAct, Reflexion, ART, ReWOO - frameworks enabling autonomous agent behavior with tool integration and iterative learning.]**
406: 
407: **Key Concepts**: Thought-Action-Observation loops, self-reflection, memory systems, task/tool libraries, module separation
408: 
409: **When to Use**: External API integration, autonomous agents, learning from mistakes, multi-tool workflows
410: 
411: **Estimated Read Time**: 40-50 minutes
412: 
413: ---
414: 
415: ### **[[03-meta-optimization-guide]]**
416: **[Meta-Optimization-Coverage**:: APE, OPRO, Active-Prompt, PromptBreeder, RPO, Meta-Prompting - techniques for automatic prompt improvement without manual iteration.]**
417: 
418: **Key Concepts**: LLM-as-optimizer, evolutionary algorithms, reinforcement learning, structural abstraction, uncertainty-based selection
419: 
420: **When to Use**: Large-scale prompt optimization, zero-shot adaptation, systematic improvement, production deployment
421: 
422: **Estimated Read Time**: 35-45 minutes
423: 
424: ---
425: 
426: ### **[[04-quality-assurance-guide]]**
427: **[Quality-Assurance-Coverage**:: Chain of Verification, Self-Refine, validation patterns - techniques specifically designed to reduce hallucinations and improve output quality.]**
428: 
429: **Key Concepts**: Multi-stage verification, self-generated feedback, iterative refinement, factuality checks
430: 
431: **When to Use**: Hallucination reduction, critical accuracy requirements, quality-sensitive applications
432: 
433: **Estimated Read Time**: 25-35 minutes
434: 
435: ---
436: 
437: ### **[[05-knowledge-integration-guide]]**
438: **[Knowledge-Integration-Coverage**:: Generated Knowledge, RAG, Recitation-Augmented - techniques for incorporating external or model-generated knowledge.]**
439: 
440: **Key Concepts**: Pre-answer knowledge generation, retrieval systems, attention focusing, dynamic knowledge injection
441: 
442: **When to Use**: Factual grounding, domain-specific tasks, knowledge gaps, long-context processing
443: 
444: **Estimated Read Time**: 30-40 minutes
445: 
446: ---
447: 
448: ### **[[06-integration-patterns-guide]]**
449: **[Integration-Patterns-Coverage**:: Technique combination strategies, compatibility analysis, workflow templates, multi-technique orchestration patterns.]**
450: 
451: **Key Concepts**: Synergistic combinations, sequential vs. parallel composition, conflict resolution, orchestration architecture
452: 
453: **When to Use**: Building production systems, combining multiple techniques, scaling complex workflows
454: 
455: **Estimated Read Time**: 35-45 minutes
456: 
457: ---
458: 
459: ## üé¥ Quick Reference Cards
460: 
461: [**Quick-Reference-Cards**:: One-page summaries for each technique providing copy-paste ready templates, minimal explanation, and immediate utility.]
462: 
463: **Available Cards**:
464: - `quick-ref-tree-of-thoughts.md` - ToT template with search algorithm
465: - `quick-ref-self-consistency.md` - Multi-path sampling pattern
466: - `quick-ref-react.md` - Thought-Action-Observation loop
467: - `quick-ref-reflexion.md` - Memory + self-reflection template
468: - `quick-ref-chain-of-verification.md` - Verification workflow
469: - `quick-ref-ape.md` - Automatic prompt optimization
470: - *(Additional cards created as needed)*
471: 
472: **Usage**: 
473: 1. Identify technique via decision tree
474: 2. Open corresponding quick reference card
475: 3. Copy template
476: 4. Fill in task-specific variables
477: 5. Deploy immediately
478: 
479: ---
480: 
481: ## üîÑ Version History
482: 
483: | Version | Date | Changes |
484: |---------|------|---------|
485: | 1.0.0 | 2025-12-25 | Initial release with 6 category guides, decision trees, compatibility matrix |
486: 
487: ---
488: 
489: ## üîó Related MOCs
490: 
491: - [[prompt-engineering-moc]] - General prompt engineering resources
492: - [[llm-capabilities-moc]] - Understanding model capabilities and limitations
493: - [[ai-agent-architecture-moc]] - Broader agent design patterns
494: - [[pkb-workflow-automation-moc]] - Automation strategies for knowledge management
495: 
496: ---
497: 
498: ## üìñ Citation
499: 
500: When referencing techniques from this system, cite the original research papers (linked in each guide) and optionally reference:
501: 
502: ```
503: Advanced Prompt Engineering Techniques (2025). Compiled from research 
504: spanning 2023-2025, including surveys by Schulhoff et al., Sahoo et al., 
505: Chen et al., and 50+ peer-reviewed papers. Available at: [Your PKB location]
506: ```
507: 
508: ---
509: 
510: *This index is a living document. As new techniques emerge and research evolves, category guides will be updated accordingly. Last comprehensive research update: December 2025.*
</file>

<file path="__LOCAL-REPO/__exemplar/__import/__master-exemplar/01-reasoning-techniques-guide.md">
   1: ---
   2: tags: #prompt-engineering #reasoning #tree-of-thoughts #self-consistency #advanced-techniques #reference
   3: aliases: [Reasoning Techniques, Advanced Reasoning Patterns, ToT Guide, Multi-Path Reasoning]
   4: status: evergreen
   5: certainty: verified
   6: priority: high
   7: created: 2025-12-25
   8: modified: 2025-12-25
   9: type: reference
  10: version: 1.0.0
  11: source: claude-sonnet-4.5
  12: category: reasoning-architectures
  13: ---
  14: 
  15: # Reasoning Techniques Guide
  16: 
  17: > [!abstract] Purpose
  18: > Comprehensive guide to advanced reasoning techniques that fundamentally enhance LLM problem-solving capabilities through multi-path exploration, systematic search, ensemble methods, and structural scaffolding. Based on peer-reviewed research from 2022-2024.
  19: 
  20: ---
  21: 
  22: ## üìã Table of Contents
  23: 
  24: 1. [[#Overview & Comparison]]
  25: 2. [[#Tree of Thoughts (ToT)]]
  26: 3. [[#Graph of Thoughts (GoT)]]
  27: 4. [[#Self-Consistency]]
  28: 5. [[#Program of Thoughts (PoT)]]
  29: 6. [[#Skeleton of Thoughts (SoT)]]
  30: 7. [[#Technique Selection Matrix]]
  31: 8. [[#Integration Patterns]]
  32: 9. [[#Research References]]
  33: 
  34: ---
  35: 
  36: ## Overview & Comparison
  37: 
  38: [**Reasoning-Architecture**:: Framework that structures how an LLM explores solution spaces, manages intermediate states, and arrives at final answers - ranging from linear (Chain of Thought) to tree-structured (ToT) to graph-based (GoT) to ensemble-based (Self-Consistency).]
  39: 
  40: ### **Evolution of Reasoning Approaches**
  41: 
  42: ```mermaid
  43: graph LR
  44:     A[Standard Prompting<br/>Single-pass] --> B[Chain of Thought<br/>Linear reasoning]
  45:     B --> C[Self-Consistency<br/>Multiple paths, voting]
  46:     B --> D[Tree of Thoughts<br/>Search + backtrack]
  47:     D --> E[Graph of Thoughts<br/>Non-linear connections]
  48:     B --> F[Program of Thoughts<br/>Code-based reasoning]
  49: ```
  50: 
  51: ### **Comparison Matrix**
  52: 
  53: | Technique | Search Strategy | State Management | Best For | Complexity | Token Cost |
  54: |-----------|----------------|------------------|----------|------------|------------|
  55: | **Chain of Thought** | None (linear) | Implicit | Simple reasoning | Low | Low |
  56: | **Self-Consistency** | Sample multiple | None | Reliability boost | Low-Med | Medium |
  57: | **Tree of Thoughts** | BFS/DFS | Explicit tree | Complex planning | High | High |
  58: | **Graph of Thoughts** | Custom | Explicit graph | Interconnected problems | Very High | Very High |
  59: | **Program of Thoughts** | None (linear) | Code state | Mathematical tasks | Medium | Low-Med |
  60: | **Skeleton of Thoughts** | None (structured) | Template-based | Structured analysis | Low-Med | Medium |
  61: 
  62: ---
  63: 
  64: ## Tree of Thoughts (ToT)
  65: 
  66: [**Tree-of-Thoughts**:: Deliberate problem-solving framework where LLMs explore multiple reasoning branches, systematically search through solution space using algorithms like BFS/DFS, evaluate intermediate states, and backtrack when needed.]
  67: 
  68: ### üéØ Core Concept
  69: 
  70: Traditional prompting generates solutions linearly - once the model commits to a reasoning path, it cannot easily backtrack. **[ToT-Innovation**:: Enables LLMs to explore like humans do when solving complex problems: try an approach, evaluate progress, backtrack if stuck, explore alternatives.]**
  71: 
  72: Tree of Thoughts decomposes problem-solving into:
  73: 1. **Thought Generation**: Create intermediate reasoning steps (branches)
  74: 2. **State Evaluation**: Score quality/promise of each thought
  75: 3. **Search Algorithm**: Systematically explore thought tree (BFS/DFS)
  76: 4. **Backtracking**: Abandon unpromising paths, explore alternatives
  77: 
  78: ### üî¨ How It Works
  79: 
  80: **[ToT-Four-Components**:: (1) Thought Decomposition - how to break problem into intermediate steps, (2) Thought Generator - LLM prompted to generate candidate next steps, (3) State Evaluator - LLM or heuristic to score thought quality, (4) Search Algorithm - BFS/DFS to navigate tree.]**
  81: 
  82: #### Component 1: Thought Decomposition
  83: 
  84: Define what constitutes a "thought" (intermediate reasoning step):
  85: 
  86: ```python
  87: # Example: Game of 24
  88: # Thought = One equation combining numbers
  89: 
  90: INPUT: Numbers [4, 5, 6, 10]
  91: GOAL: Reach 24 using +, -, *, /
  92: 
  93: THOUGHT_1: "6 * 4 = 24" (Direct solution!)
  94: THOUGHT_2: "10 - 6 = 4" (Intermediate, not solution yet)
  95: THOUGHT_3: "5 + 4 = 9" (Intermediate)
  96: ```
  97: 
  98: #### Component 2: Thought Generator
  99: 
 100: **Prompt template for generating next thoughts**:
 101: 
 102: ```markdown
 103: # THOUGHT GENERATION PROMPT
 104: 
 105: Current State:
 106: {current_numbers}
 107: 
 108: Steps taken so far:
 109: {previous_thoughts}
 110: 
 111: Generate {k} possible next steps. Each step should:
 112: 1. Combine two numbers from current state
 113: 2. Use one operation: +, -, *, /
 114: 3. Result in a new number
 115: 
 116: Possible next steps:
 117: 1. [First candidate thought]
 118: 2. [Second candidate thought]
 119: 3. [Third candidate thought]
 120: ...
 121: ```
 122: 
 123: **Implementation**:
 124: 
 125: ```python
 126: def generate_thoughts(state, num_candidates=3):
 127:     """Generate k candidate next thoughts from current state."""
 128:     
 129:     prompt = f"""
 130: Current numbers: {state['numbers']}
 131: Goal: Reach 24
 132: Steps so far: {state['history']}
 133: 
 134: Generate {num_candidates} different next steps.
 135: Each step: combine two numbers with +, -, *, or /.
 136: 
 137: Format:
 138: 1. [operation] => [result]
 139: 2. [operation] => [result]
 140: 3. [operation] => [result]
 141: """
 142:     
 143:     response = llm.generate(prompt, n=1, temperature=0.7)
 144:     thoughts = parse_thoughts(response)
 145:     return thoughts
 146: ```
 147: 
 148: #### Component 3: State Evaluator
 149: 
 150: **[State-Evaluation**:: Assess how promising a partial solution is - can use LLM judgment ("rate this approach 1-10"), heuristic functions (distance to goal), or domain-specific rules.]**
 151: 
 152: **Value Prompt Template**:
 153: 
 154: ```markdown
 155: # STATE EVALUATION PROMPT
 156: 
 157: Goal: Reach 24 using [4, 5, 6, 10]
 158: 
 159: Current state after operations:
 160: {thought_sequence}
 161: Current numbers available: {current_numbers}
 162: 
 163: Evaluate this state:
 164: - Is it IMPOSSIBLE? (no way to reach 24 from here)
 165: - Is it LIKELY? (clear path visible)
 166: - Is it MAYBE? (possible but uncertain)
 167: - Is it SOLVED? (reached 24)
 168: 
 169: Provide:
 170: 1. Assessment: [IMPOSSIBLE/MAYBE/LIKELY/SOLVED]
 171: 2. Confidence: [0-10]
 172: 3. Reasoning: [brief explanation]
 173: ```
 174: 
 175: **Implementation**:
 176: 
 177: ```python
 178: def evaluate_state(state):
 179:     """Score how promising current state is."""
 180:     
 181:     # Check if solved
 182:     if 24 in state['numbers']:
 183:         return {'value': 10, 'status': 'SOLVED'}
 184:     
 185:     # LLM-based evaluation for intermediate states
 186:     prompt = f"""
 187: Evaluate this partial solution:
 188: Numbers left: {state['numbers']}
 189: Goal: Reach 24
 190: 
 191: Rate promise of this state (1-10):
 192: - 1-3: Dead end, impossible
 193: - 4-6: Uncertain, might work
 194: - 7-9: Promising, likely solvable
 195: - 10: Solved
 196: 
 197: Score: """
 198:     
 199:     response = llm.generate(prompt, temperature=0.0)
 200:     score = extract_score(response)
 201:     
 202:     return {'value': score, 'status': 'IN_PROGRESS'}
 203: ```
 204: 
 205: #### Component 4: Search Algorithm
 206: 
 207: **Breadth-First Search (BFS)**:
 208: - Explores all thoughts at depth *d* before moving to depth *d+1*
 209: - Finds shortest solution path
 210: - Higher memory/token cost
 211: 
 212: **Depth-First Search (DFS)**:
 213: - Explores one branch fully before backtracking
 214: - Lower memory/token cost
 215: - May not find optimal solution
 216: 
 217: **BFS Implementation**:
 218: 
 219: ```python
 220: from collections import deque
 221: 
 222: def tree_of_thoughts_bfs(initial_state, max_depth=5, branching_factor=3):
 223:     """
 224:     BFS implementation of Tree of Thoughts.
 225:     
 226:     Args:
 227:         initial_state: Starting problem state
 228:         max_depth: Maximum search depth
 229:         branching_factor: Thoughts generated per state
 230:     
 231:     Returns:
 232:         Solution path if found, else None
 233:     """
 234:     queue = deque([(initial_state, [])])  # (state, path)
 235:     
 236:     while queue:
 237:         current_state, path = queue.popleft()
 238:         
 239:         # Check if solved
 240:         evaluation = evaluate_state(current_state)
 241:         if evaluation['status'] == 'SOLVED':
 242:             return path + [current_state]
 243:         
 244:         # Prune dead ends
 245:         if evaluation['value'] < 3:  # Threshold for pruning
 246:             continue
 247:         
 248:         # Don't exceed depth limit
 249:         if len(path) >= max_depth:
 250:             continue
 251:         
 252:         # Generate and evaluate next thoughts
 253:         next_thoughts = generate_thoughts(current_state, branching_factor)
 254:         
 255:         for thought in next_thoughts:
 256:             new_state = apply_thought(current_state, thought)
 257:             queue.append((new_state, path + [thought]))
 258:     
 259:     return None  # No solution found
 260: ```
 261: 
 262: **DFS Implementation**:
 263: 
 264: ```python
 265: def tree_of_thoughts_dfs(state, path=[], max_depth=5, branching_factor=3):
 266:     """
 267:     DFS implementation of Tree of Thoughts (recursive).
 268:     """
 269:     # Base cases
 270:     evaluation = evaluate_state(state)
 271:     
 272:     if evaluation['status'] == 'SOLVED':
 273:         return path + [state]
 274:     
 275:     if len(path) >= max_depth or evaluation['value'] < 3:
 276:         return None
 277:     
 278:     # Generate next thoughts
 279:     thoughts = generate_thoughts(state, branching_factor)
 280:     
 281:     # Sort by evaluation score (best-first)
 282:     thoughts_with_scores = []
 283:     for thought in thoughts:
 284:         temp_state = apply_thought(state, thought)
 285:         score = evaluate_state(temp_state)['value']
 286:         thoughts_with_scores.append((score, thought))
 287:     
 288:     thoughts_with_scores.sort(reverse=True)  # Best first
 289:     
 290:     # Explore each branch
 291:     for score, thought in thoughts_with_scores:
 292:         new_state = apply_thought(state, thought)
 293:         result = tree_of_thoughts_dfs(
 294:             new_state, 
 295:             path + [thought], 
 296:             max_depth, 
 297:             branching_factor
 298:         )
 299:         if result:
 300:             return result
 301:     
 302:     return None  # No solution in this branch
 303: ```
 304: 
 305: ### üí° When to Use Tree of Thoughts
 306: 
 307: **[ToT-Ideal-Use-Cases**:: (1) Planning tasks with multiple valid approaches, (2) Creative problems requiring exploration, (3) Tasks where early mistakes lead to dead ends, (4) Problems with clear intermediate goals, (5) Situations where optimal solution matters.]**
 308: 
 309: **‚úÖ Excellent For:**
 310: - **Game of 24**: Mathematical puzzle requiring search
 311: - **Creative Writing**: Exploring different story angles
 312: - **Travel Planning**: Optimizing multi-city routes
 313: - **Code Generation**: Trying different architectural approaches
 314: - **Strategic Planning**: Business decisions with multiple paths
 315: 
 316: **‚ùå Overkill For:**
 317: - Simple factual questions ("What is the capital of France?")
 318: - Tasks with single clear path (straightforward calculations)
 319: - Time-critical applications (too slow)
 320: - Token-budget-constrained scenarios (very expensive)
 321: 
 322: ### üìù Complete Working Example: Game of 24
 323: 
 324: **Problem**: Using [4, 5, 6, 10], reach 24 with +, -, *, /
 325: 
 326: **ToT Solution with BFS**:
 327: 
 328: ```python
 329: # Initial state
 330: state_0 = {
 331:     'numbers': [4, 5, 6, 10],
 332:     'operations': [],
 333:     'goal': 24
 334: }
 335: 
 336: # Iteration 1: Generate 3 thoughts from initial state
 337: thoughts_1 = generate_thoughts(state_0, k=3)
 338: # Outputs:
 339: # Thought 1.1: "10 - 6 = 4" ‚Üí new state [4, 4, 5]
 340: # Thought 1.2: "6 * 4 = 24" ‚Üí SOLVED! ‚úì
 341: # Thought 1.3: "5 + 4 = 9" ‚Üí new state [6, 9, 10]
 342: 
 343: # Evaluation:
 344: # Thought 1.1: Score 6/10 (neutral)
 345: # Thought 1.2: Score 10/10 (SOLVED!)
 346: # Thought 1.3: Score 5/10 (possible)
 347: 
 348: # BFS found solution at depth 1: "6 * 4 = 24"
 349: ```
 350: 
 351: **More Complex Example** (no immediate solution):
 352: 
 353: ```
 354: Initial: [2, 3, 7, 9]
 355: Goal: 24
 356: 
 357: BFS Exploration:
 358: 
 359: Depth 1:
 360: ‚îú‚îÄ 9 * 3 = 27 [2, 7, 27] ‚Üí Score: 7/10
 361: ‚îú‚îÄ 9 - 7 = 2  [2, 2, 3]  ‚Üí Score: 3/10 (prune)
 362: ‚îî‚îÄ 7 + 2 = 9  [3, 9, 9]  ‚Üí Score: 4/10
 363: 
 364: Depth 2 (from best path):
 365: ‚îú‚îÄ 27 - 7 = 20 [2, 20] ‚Üí Score: 8/10
 366: ‚îú‚îÄ 27 / 3 = 9  [2, 7, 9] ‚Üí Score: 5/10
 367: ‚îî‚îÄ 27 + 2 = 29 [7, 29] ‚Üí Score: 4/10
 368: 
 369: Depth 3 (from best path):
 370: ‚îú‚îÄ 20 + 2 = 22 [22] ‚Üí Score: 6/10
 371: ‚îú‚îÄ 20 - 2 = 18 [18] ‚Üí Score: 5/10
 372: ‚îî‚îÄ 20 * 2 = 40 [40] ‚Üí Score: 2/10 (prune)
 373: 
 374: BACKTRACK to Depth 1, try next branch...
 375: Eventually finds: (9 - 7) * (3 + 2) = 2 * 12 = 24
 376: ```
 377: 
 378: ### üîß Production-Ready ToT Template
 379: 
 380: **Complete copyable implementation**:
 381: 
 382: ```python
 383: class TreeOfThoughts:
 384:     """
 385:     Production implementation of Tree of Thoughts prompting.
 386:     
 387:     Usage:
 388:         tot = TreeOfThoughts(llm_client)
 389:         solution = tot.solve(problem_state, search='bfs')
 390:     """
 391:     
 392:     def __init__(self, llm, branching_factor=3, max_depth=5):
 393:         self.llm = llm
 394:         self.branching_factor = branching_factor
 395:         self.max_depth = max_depth
 396:         
 397:     def generate_thoughts(self, state, k):
 398:         """Generate k candidate next steps."""
 399:         prompt = self._build_generation_prompt(state, k)
 400:         response = self.llm.complete(prompt, temperature=0.7)
 401:         return self._parse_thoughts(response)
 402:     
 403:     def evaluate_state(self, state):
 404:         """Score how promising current state is (0-10)."""
 405:         if self._is_goal(state):
 406:             return {'score': 10, 'status': 'solved'}
 407:         
 408:         prompt = self._build_evaluation_prompt(state)
 409:         response = self.llm.complete(prompt, temperature=0.0)
 410:         score = self._extract_score(response)
 411:         
 412:         return {'score': score, 'status': 'in_progress'}
 413:     
 414:     def solve(self, initial_state, search='bfs'):
 415:         """
 416:         Solve problem using ToT.
 417:         
 418:         Args:
 419:             initial_state: Problem starting point
 420:             search: 'bfs' or 'dfs'
 421:         
 422:         Returns:
 423:             Solution path or None
 424:         """
 425:         if search == 'bfs':
 426:             return self._solve_bfs(initial_state)
 427:         else:
 428:             return self._solve_dfs(initial_state)
 429:     
 430:     def _solve_bfs(self, initial_state):
 431:         """BFS implementation."""
 432:         from collections import deque
 433:         
 434:         queue = deque([(initial_state, [])])
 435:         visited = set()
 436:         
 437:         while queue:
 438:             state, path = queue.popleft()
 439:             
 440:             state_hash = self._hash_state(state)
 441:             if state_hash in visited:
 442:                 continue
 443:             visited.add(state_hash)
 444:             
 445:             eval_result = self.evaluate_state(state)
 446:             
 447:             if eval_result['status'] == 'solved':
 448:                 return path + [state]
 449:             
 450:             if eval_result['score'] < 3 or len(path) >= self.max_depth:
 451:                 continue
 452:             
 453:             thoughts = self.generate_thoughts(state, self.branching_factor)
 454:             
 455:             for thought in thoughts:
 456:                 new_state = self._apply_thought(state, thought)
 457:                 queue.append((new_state, path + [thought]))
 458:         
 459:         return None
 460:     
 461:     def _solve_dfs(self, state, path=[], visited=None):
 462:         """DFS implementation (recursive)."""
 463:         if visited is None:
 464:             visited = set()
 465:         
 466:         state_hash = self._hash_state(state)
 467:         if state_hash in visited:
 468:             return None
 469:         visited.add(state_hash)
 470:         
 471:         eval_result = self.evaluate_state(state)
 472:         
 473:         if eval_result['status'] == 'solved':
 474:             return path + [state]
 475:         
 476:         if eval_result['score'] < 3 or len(path) >= self.max_depth:
 477:             return None
 478:         
 479:         thoughts = self.generate_thoughts(state, self.branching_factor)
 480:         
 481:         # Sort by promise (best-first)
 482:         thoughts_scored = [
 483:             (self.evaluate_state(self._apply_thought(state, t))['score'], t)
 484:             for t in thoughts
 485:         ]
 486:         thoughts_scored.sort(reverse=True)
 487:         
 488:         for score, thought in thoughts_scored:
 489:             new_state = self._apply_thought(state, thought)
 490:             result = self._solve_dfs(new_state, path + [thought], visited)
 491:             if result:
 492:                 return result
 493:         
 494:         return None
 495:     
 496:     # Helper methods (implement based on problem domain)
 497:     def _build_generation_prompt(self, state, k):
 498:         """Construct prompt for thought generation."""
 499:         raise NotImplementedError
 500:     
 501:     def _build_evaluation_prompt(self, state):
 502:         """Construct prompt for state evaluation."""
 503:         raise NotImplementedError
 504:     
 505:     def _parse_thoughts(self, response):
 506:         """Extract thoughts from LLM response."""
 507:         raise NotImplementedError
 508:     
 509:     def _extract_score(self, response):
 510:         """Extract numeric score from evaluation."""
 511:         raise NotImplementedError
 512:     
 513:     def _is_goal(self, state):
 514:         """Check if state satisfies goal."""
 515:         raise NotImplementedError
 516:     
 517:     def _apply_thought(self, state, thought):
 518:         """Generate new state from current + thought."""
 519:         raise NotImplementedError
 520:     
 521:     def _hash_state(self, state):
 522:         """Create hashable representation (for visited set)."""
 523:         raise NotImplementedError
 524: ```
 525: 
 526: ### üìä Performance Benchmarks
 527: 
 528: [**ToT-Performance-Data**:: Game of 24 task - ToT achieves 74% success vs. 7.3% for standard prompting (10x improvement). Creative Writing task - ToT achieves 20% coherence vs. 12% baseline. Crossword task - ToT outperforms by 60%+.]**
 529: 
 530: **From Yao et al. 2023**:
 531: 
 532: | Task | Method | Success Rate | Improvement |
 533: |------|--------|--------------|-------------|
 534: | **Game of 24** | Standard Prompting | 7.3% | - |
 535: | **Game of 24** | CoT Prompting | 4.0% | -3.3pp |
 536: | **Game of 24** | ToT (BFS, b=5, T=3) | **74.0%** | **+66.7pp** |
 537: |  |  |  |  |
 538: | **Creative Writing** | Standard | 12% coherent | - |
 539: | **Creative Writing** | ToT | **20% coherent** | **+8pp** |
 540: |  |  |  |  |
 541: | **5x5 Crossword** | Standard | <20% | - |
 542: | **5x5 Crossword** | ToT | **78%** | **+58pp** |
 543: 
 544: ### ‚ö†Ô∏è Limitations & Considerations
 545: 
 546: **[ToT-Limitations**:: (1) High token cost - generates multiple thoughts per step, (2) Slow - systematic search takes time, (3) Requires good evaluation function, (4) Not all problems benefit from search, (5) Can get stuck in local optima.]**
 547: 
 548: 1. **Token Cost**: Branching factor of 3-5 and depth of 3-5 means 27-3125 LLM calls
 549: 2. **Latency**: Sequential evaluation creates bottlenecks
 550: 3. **Evaluation Quality**: Weak evaluator ‚Üí poor search decisions
 551: 4. **Problem Structure**: Must have decomposable intermediate states
 552: 5. **Local Optima**: Like all search, can miss global optimum
 553: 
 554: **Mitigation Strategies**:
 555: - Use **pruning aggressively** (threshold evaluation scores)
 556: - Implement **beam search** (limit branches explored per level)
 557: - Cache **state evaluations** (avoid re-evaluating same states)
 558: - Use **heuristics** instead of LLM evaluation when possible
 559: - Combine with **Self-Consistency** at final answer stage
 560: 
 561: ---
 562: 
 563: ## Graph of Thoughts (GoT)
 564: 
 565: [**Graph-of-Thoughts**:: Extends ToT by allowing arbitrary connections between reasoning steps (not just tree hierarchy), enabling non-linear thought processes where concepts can interconnect bidirectionally and thoughts can build on multiple predecessors.]**
 566: 
 567: ### üéØ Core Concept
 568: 
 569: While ToT structures thoughts hierarchically (parent ‚Üí child), **GoT recognizes that human reasoning often involves non-linear connections**: a thought at depth 3 might inform a thought at depth 2, or two parallel branches might merge.
 570: 
 571: **[GoT-vs-ToT-Distinction**:: ToT enforces tree structure (each thought has one parent). GoT allows graph structure (thoughts can have multiple parents, children can influence parents, parallel branches can merge). Think Wikipedia's interconnected articles vs. a table of contents.]**
 572: 
 573: ```
 574: Tree of Thoughts:          Graph of Thoughts:
 575:       ROOT                      ROOT
 576:       /  \                     /  |  \
 577:      A    B                   A   B   C
 578:     / \    \                  |\ /|\ /|
 579:    C   D    E                 D E F G H
 580:                               |X|X|X|X|
 581:                                Final Answer
 582: ```
 583: 
 584: ### üî¨ How It Works
 585: 
 586: **GoT Architecture** (from Besta et al. 2024):
 587: 
 588: 1. **Nodes**: Individual thoughts/reasoning steps
 589: 2. **Edges**: Dependencies and relationships between thoughts
 590: 3. **Operations**:
 591:    - **Generate**: Create new thought node
 592:    - **Aggregate**: Merge multiple thoughts into one
 593:    - **Refine**: Improve existing thought based on others
 594:    - **Validate**: Check thought against criteria
 595: 
 596: **[GoT-Operations**:: Four fundamental operations - Generate creates new nodes, Aggregate merges nodes, Refine improves nodes, Validate checks node quality. These enable complex workflows like "generate 3 approaches ‚Üí validate each ‚Üí aggregate best parts ‚Üí refine combined approach".]**
 597: 
 598: ### üí° When to Use Graph of Thoughts
 599: 
 600: **‚úÖ Ideal For:**
 601: - **Multi-faceted problems** requiring integration of diverse perspectives
 602: - **Creative synthesis** where ideas build on each other non-linearly
 603: - **Comparative analysis** (compare A vs B, then synthesize insights)
 604: - **Iterative refinement** where later thoughts improve earlier ones
 605: - **Document understanding** with cross-referenced concepts
 606: 
 607: **‚ùå Overkill For:**
 608: - Simple linear reasoning tasks
 609: - Problems with clear hierarchical structure (use ToT instead)
 610: - Resource-constrained environments (GoT is even more expensive than ToT)
 611: 
 612: ### üìù Complete Example: Comparative Analysis
 613: 
 614: **Problem**: Compare and synthesize insights from 3 research approaches
 615: 
 616: **GoT Workflow**:
 617: 
 618: ```python
 619: # Phase 1: Generate independent analyses (parallel nodes)
 620: thought_A = generate("Analyze Approach A: [Neural Networks]")
 621: thought_B = generate("Analyze Approach B: [Symbolic AI]")
 622: thought_C = generate("Analyze Approach C: [Hybrid Systems]")
 623: 
 624: # Phase 2: Pairwise comparisons (cross-connections)
 625: comparison_AB = aggregate(thought_A, thought_B, 
 626:                           operation="compare_strengths_weaknesses")
 627: comparison_BC = aggregate(thought_B, thought_C, 
 628:                           operation="compare_strengths_weaknesses")
 629: comparison_AC = aggregate(thought_A, thought_C, 
 630:                           operation="compare_strengths_weaknesses")
 631: 
 632: # Phase 3: Refine original analyses based on comparisons (backward edges!)
 633: thought_A_refined = refine(thought_A, 
 634:                            context=[comparison_AB, comparison_AC])
 635: thought_B_refined = refine(thought_B, 
 636:                            context=[comparison_AB, comparison_BC])
 637: thought_C_refined = refine(thought_C, 
 638:                            context=[comparison_BC, comparison_AC])
 639: 
 640: # Phase 4: Synthesize all refined insights
 641: synthesis = aggregate(thought_A_refined, thought_B_refined, thought_C_refined,
 642:                       operation="synthesize_unified_perspective")
 643: 
 644: # Phase 5: Validate synthesis against original papers
 645: validation = validate(synthesis, 
 646:                       criteria=["accuracy", "completeness", "novelty"])
 647: 
 648: # Phase 6: Final refinement based on validation
 649: final_output = refine(synthesis, validation_feedback=validation)
 650: ```
 651: 
 652: **Prompt Template for Aggregate Operation**:
 653: 
 654: ```markdown
 655: # AGGREGATE THOUGHTS PROMPT
 656: 
 657: You are synthesizing multiple reasoning steps into a unified insight.
 658: 
 659: Thought 1:
 660: {thought_1_content}
 661: 
 662: Thought 2:
 663: {thought_2_content}
 664: 
 665: Operation: {operation_type}
 666: (Examples: "compare", "merge", "find_common_ground", "synthesize")
 667: 
 668: Generate a new thought that:
 669: 1. Identifies key points from each input thought
 670: 2. Finds relationships/connections between them
 671: 3. Produces integrated insight (not just concatenation)
 672: 
 673: Aggregated Thought:
 674: ```
 675: 
 676: ### üîß GoT Implementation Pattern
 677: 
 678: ```python
 679: class GraphOfThoughts:
 680:     """
 681:     Graph of Thoughts implementation.
 682:     
 683:     Nodes are thoughts, edges are dependencies/relationships.
 684:     Supports: generate, aggregate, refine, validate operations.
 685:     """
 686:     
 687:     def __init__(self, llm):
 688:         self.llm = llm
 689:         self.graph = {}  # node_id ‚Üí {'content': str, 'dependencies': list}
 690:         self.node_counter = 0
 691:     
 692:     def generate(self, prompt, dependencies=None):
 693:         """Create new thought node."""
 694:         node_id = f"node_{self.node_counter}"
 695:         self.node_counter += 1
 696:         
 697:         # If dependencies exist, include context
 698:         context = ""
 699:         if dependencies:
 700:             context = "Based on previous thoughts:\n"
 701:             for dep_id in dependencies:
 702:                 context += f"- {self.graph[dep_id]['content']}\n"
 703:         
 704:         full_prompt = context + "\n" + prompt
 705:         response = self.llm.complete(full_prompt)
 706:         
 707:         self.graph[node_id] = {
 708:             'content': response,
 709:             'dependencies': dependencies or [],
 710:             'operation': 'generate'
 711:         }
 712:         
 713:         return node_id
 714:     
 715:     def aggregate(self, node_ids, operation="merge"):
 716:         """Merge multiple thoughts into one."""
 717:         new_id = f"node_{self.node_counter}"
 718:         self.node_counter += 1
 719:         
 720:         # Gather content from input nodes
 721:         thoughts = [self.graph[nid]['content'] for nid in node_ids]
 722:         
 723:         prompt = f"""
 724: Aggregate these {len(thoughts)} thoughts using operation: {operation}
 725: 
 726: Thoughts to aggregate:
 727: {self._format_thoughts(thoughts)}
 728: 
 729: Produce a unified thought that synthesizes the key insights.
 730: """
 731:         
 732:         response = self.llm.complete(prompt)
 733:         
 734:         self.graph[new_id] = {
 735:             'content': response,
 736:             'dependencies': node_ids,
 737:             'operation': f'aggregate_{operation}'
 738:         }
 739:         
 740:         return new_id
 741:     
 742:     def refine(self, node_id, context_nodes=None, feedback=None):
 743:         """Improve a thought based on additional context or feedback."""
 744:         new_id = f"node_{self.node_counter}"
 745:         self.node_counter += 1
 746:         
 747:         original_content = self.graph[node_id]['content']
 748:         
 749:         additional_context = ""
 750:         if context_nodes:
 751:             additional_context = "Additional context:\n"
 752:             for ctx_id in context_nodes:
 753:                 additional_context += f"- {self.graph[ctx_id]['content']}\n"
 754:         
 755:         if feedback:
 756:             additional_context += f"\nFeedback to address:\n{feedback}\n"
 757:         
 758:         prompt = f"""
 759: Original thought:
 760: {original_content}
 761: 
 762: {additional_context}
 763: 
 764: Refine the original thought incorporating the additional context.
 765: Improved thought:
 766: """
 767:         
 768:         response = self.llm.complete(prompt)
 769:         
 770:         self.graph[new_id] = {
 771:             'content': response,
 772:             'dependencies': [node_id] + (context_nodes or []),
 773:             'operation': 'refine'
 774:         }
 775:         
 776:         return new_id
 777:     
 778:     def validate(self, node_id, criteria):
 779:         """Evaluate thought against criteria."""
 780:         content = self.graph[node_id]['content']
 781:         
 782:         prompt = f"""
 783: Evaluate this thought against criteria:
 784: {content}
 785: 
 786: Criteria:
 787: {chr(10).join(f'- {c}' for c in criteria)}
 788: 
 789: For each criterion, provide:
 790: 1. Score (1-10)
 791: 2. Explanation
 792: 3. Suggestions for improvement
 793: 
 794: Validation Results:
 795: """
 796:         
 797:         response = self.llm.complete(prompt)
 798:         return response
 799:     
 800:     def _format_thoughts(self, thoughts):
 801:         """Format multiple thoughts for display."""
 802:         return "\n\n".join(f"{i+1}. {t}" for i, t in enumerate(thoughts))
 803:     
 804:     def visualize(self):
 805:         """Generate Mermaid diagram of thought graph."""
 806:         lines = ["graph TD"]
 807:         for node_id, data in self.graph.items():
 808:             label = data['content'][:30] + "..." if len(data['content']) > 30 else data['content']
 809:             lines.append(f'    {node_id}["{label}"]')
 810:             
 811:             for dep in data['dependencies']:
 812:                 lines.append(f'    {dep} --> {node_id}')
 813:         
 814:         return "\n".join(lines)
 815: ```
 816: 
 817: ### ‚ö†Ô∏è GoT Limitations
 818: 
 819: 1. **Extreme Complexity**: Managing graph state is harder than tree state
 820: 2. **Even Higher Cost**: More operations ‚Üí more LLM calls than ToT
 821: 3. **Cycle Risk**: Graph structure can create circular dependencies
 822: 4. **Difficult Visualization**: Hard to inspect/debug reasoning process
 823: 
 824: **When to Use GoT vs ToT**:
 825: - **Use ToT** if problem has hierarchical structure, clear parent-child relationships
 826: - **Use GoT** if insights genuinely need to cross-influence, merge, or build bidirectionally
 827: 
 828: ---
 829: 
 830: ## Self-Consistency
 831: 
 832: [**Self-Consistency**:: Ensemble method that generates diverse reasoning paths for the same query (typically 5-40 samples), then selects the most frequent final answer via majority voting to improve reliability and reduce errors.]**
 833: 
 834: ### üéØ Core Concept
 835: 
 836: **The Problem**: Even with Chain of Thought, a single reasoning path can lead to errors. A small mistake early in reasoning cascades into wrong answer.
 837: 
 838: **[Self-Consistency-Insight**:: Humans solve hard problems by trying multiple approaches - if different methods yield same answer, confidence increases. Self-Consistency brings this to LLMs by sampling diverse reasoning paths and using consensus as confidence signal.]**
 839: 
 840: ### üî¨ How It Works
 841: 
 842: **Three-Step Process** (Wang et al. 2022):
 843: 
 844: 1. **Sample Diverse Paths**: Use high temperature (0.7-1.0) to generate N different reasoning chains
 845: 2. **Extract Answers**: Parse final answer from each reasoning path
 846: 3. **Majority Vote**: Select most frequent answer
 847: 
 848: ```python
 849: def self_consistency(prompt, num_samples=5):
 850:     """
 851:     Self-Consistency implementation.
 852:     
 853:     Args:
 854:         prompt: Task prompt (preferably with CoT)
 855:         num_samples: Number of reasoning paths to generate
 856:     
 857:     Returns:
 858:         Most consistent answer + confidence score
 859:     """
 860:     reasoning_paths = []
 861:     answers = []
 862:     
 863:     # Step 1: Generate diverse reasoning paths
 864:     for i in range(num_samples):
 865:         response = llm.complete(
 866:             prompt,
 867:             temperature=0.7,  # Higher temp for diversity
 868:             max_tokens=512
 869:         )
 870:         reasoning_paths.append(response)
 871:         answer = extract_final_answer(response)
 872:         answers.append(answer)
 873:     
 874:     # Step 2: Majority vote
 875:     from collections import Counter
 876:     vote_counts = Counter(answers)
 877:     most_common_answer, count = vote_counts.most_common(1)[0]
 878:     
 879:     # Step 3: Calculate confidence
 880:     confidence = count / num_samples
 881:     
 882:     return {
 883:         'answer': most_common_answer,
 884:         'confidence': confidence,
 885:         'vote_distribution': dict(vote_counts),
 886:         'all_paths': reasoning_paths
 887:     }
 888: ```
 889: 
 890: ### üí° When to Use Self-Consistency
 891: 
 892: **[Self-Consistency-Use-Cases**:: (1) High-stakes decisions requiring reliability, (2) Arithmetic/mathematical reasoning prone to calculation errors, (3) Multi-step commonsense reasoning, (4) When single-path CoT is insufficient, (5) Whenever you can afford 5-10x token cost.]**
 893: 
 894: **‚úÖ Excellent For:**
 895: - **Mathematical reasoning** (GSM8K, SVAMP, AQuA benchmarks)
 896: - **Commonsense reasoning** (StrategyQA, ARC benchmarks)
 897: - **High-reliability applications** (medical, legal, financial decisions)
 898: - **Verification of complex reasoning** (validate ToT/GoT outputs)
 899: 
 900: **‚ùå Not Worth It For:**
 901: - **Simple factual questions** (no reasoning to vary)
 902: - **Open-ended creative tasks** (diversity is feature, not bug)
 903: - **Real-time applications** (5-10x slower)
 904: - **Tight token budgets** (5-10x more expensive)
 905: 
 906: ### üìù Complete Example: Math Problem
 907: 
 908: **Problem**: "A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?"
 909: 
 910: **Standard CoT** (single path - may err):
 911: 
 912: ```
 913: Reasoning:
 914: - Total balls: 16
 915: - Half are golf balls: 16 / 2 = 8 golf balls
 916: - Half of golf balls are blue: 8 / 2 = 4
 917: 
 918: Answer: 4 blue golf balls ‚úì (Correct)
 919: ```
 920: 
 921: **But sometimes CoT makes mistakes**:
 922: 
 923: ```
 924: Reasoning:
 925: - Total balls: 16
 926: - Half are golf balls: 8
 927: - Blue golf balls: 8 (MISTAKE - misread "half of golf balls")
 928: 
 929: Answer: 8 ‚ùå (Wrong)
 930: ```
 931: 
 932: **Self-Consistency** (5 paths):
 933: 
 934: ```python
 935: # Path 1:
 936: "16 balls total. Half = 8 are golf balls. Half of those = 4 are blue. Answer: 4"
 937: 
 938: # Path 2:
 939: "Total: 16. Golf balls: 16/2 = 8. Blue golf: 8/2 = 4. Answer: 4"
 940: 
 941: # Path 3:
 942: "16 balls, 50% are golf (8 balls). Of those 8, 50% blue = 4. Answer: 4"
 943: 
 944: # Path 4:
 945: "16 balls. Half golf = 8. Half of 8 = 4 blue golf balls. Answer: 4"
 946: 
 947: # Path 5:
 948: "Start: 16. Golf: 16 √∑ 2 = 8. Blue: 8 √∑ 2 = 4. Answer: 4"
 949: 
 950: # Majority vote: 4 appears 5/5 times ‚Üí Confidence: 100%
 951: ```
 952: 
 953: Even if one path makes an error:
 954: 
 955: ```python
 956: # Path 1-4: All correctly conclude "4"
 957: # Path 5: "Blue golf balls = 8" (error)
 958: 
 959: # Majority vote: 4 appears 4/5 times ‚Üí Confidence: 80%
 960: # Still selects correct answer despite one error!
 961: ```
 962: 
 963: ### üìä Performance Benchmarks
 964: 
 965: **From Wang et al. 2022**:
 966: 
 967: | Task (Dataset) | CoT Baseline | Self-Consistency | Improvement |
 968: |----------------|--------------|------------------|-------------|
 969: | **GSM8K (Math)** | 46.9% | 74.4% | **+27.5pp** |
 970: | **SVAMP (Math)** | 68.9% | 79.9% | **+11.0pp** |
 971: | **AQuA (Math)** | 33.8% | 46.0% | **+12.2pp** |
 972: | **StrategyQA (Reasoning)** | 66.4% | 72.5% | **+6.1pp** |
 973: | **ARC-challenge (Science)** | 79.4% | 83.7% | **+4.3pp** |
 974: 
 975: **[Self-Consistency-Performance-Pattern**:: Improvements largest on mathematical/arithmetic tasks (10-27pp), moderate on commonsense (4-10pp). Gains increase with model scale - larger models benefit more from self-consistency.]**
 976: 
 977: ### üîß Production Template with Adaptive Sampling
 978: 
 979: ```python
 980: class AdaptiveSelfConsistency:
 981:     """
 982:     Self-Consistency with adaptive sampling.
 983:     
 984:     Starts with minimum samples, adds more if low confidence.
 985:     """
 986:     
 987:     def __init__(self, llm, min_samples=3, max_samples=10, confidence_threshold=0.7):
 988:         self.llm = llm
 989:         self.min_samples = min_samples
 990:         self.max_samples = max_samples
 991:         self.confidence_threshold = confidence_threshold
 992:     
 993:     def solve(self, prompt, cot_template=None):
 994:         """
 995:         Adaptive self-consistency.
 996:         
 997:         Returns early if high confidence achieved,
 998:         continues sampling if uncertain.
 999:         """
1000:         from collections import Counter
1001:         
1002:         # Apply CoT template if provided
1003:         if cot_template:
1004:             prompt = cot_template.format(query=prompt)
1005:         else:
1006:             prompt = f"{prompt}\n\nLet's solve this step by step:"
1007:         
1008:         answers = []
1009:         reasoning_paths = []
1010:         
1011:         # Initial sampling
1012:         for i in range(self.min_samples):
1013:             response = self.llm.complete(prompt, temperature=0.7)
1014:             reasoning_paths.append(response)
1015:             answer = self._extract_answer(response)
1016:             answers.append(answer)
1017:         
1018:         # Check if confident
1019:         vote_counts = Counter(answers)
1020:         most_common, count = vote_counts.most_common(1)[0]
1021:         confidence = count / len(answers)
1022:         
1023:         # If confident, return early
1024:         if confidence >= self.confidence_threshold:
1025:             return self._format_result(most_common, confidence, 
1026:                                       vote_counts, reasoning_paths)
1027:         
1028:         # Otherwise, continue sampling
1029:         while len(answers) < self.max_samples:
1030:             response = self.llm.complete(prompt, temperature=0.7)
1031:             reasoning_paths.append(response)
1032:             answer = self._extract_answer(response)
1033:             answers.append(answer)
1034:             
1035:             # Recompute confidence
1036:             vote_counts = Counter(answers)
1037:             most_common, count = vote_counts.most_common(1)[0]
1038:             confidence = count / len(answers)
1039:             
1040:             # Break if confident
1041:             if confidence >= self.confidence_threshold:
1042:                 break
1043:         
1044:         return self._format_result(most_common, confidence, 
1045:                                    vote_counts, reasoning_paths)
1046:     
1047:     def _extract_answer(self, response):
1048:         """Extract final answer from reasoning text."""
1049:         # Common patterns
1050:         patterns = [
1051:             r"answer is:?\s*([^\n]+)",
1052:             r"final answer:?\s*([^\n]+)",
1053:             r"therefore,?\s*([^\n]+)",
1054:             r"so,?\s*([^\n]+)"
1055:         ]
1056:         
1057:         import re
1058:         for pattern in patterns:
1059:             match = re.search(pattern, response, re.IGNORECASE)
1060:             if match:
1061:                 return match.group(1).strip()
1062:         
1063:         # Fallback: last line
1064:         return response.strip().split('\n')[-1]
1065:     
1066:     def _format_result(self, answer, confidence, votes, paths):
1067:         """Format output with metadata."""
1068:         return {
1069:             'answer': answer,
1070:             'confidence': confidence,
1071:             'vote_distribution': dict(votes),
1072:             'num_samples': len(paths),
1073:             'reasoning_paths': paths
1074:         }
1075: ```
1076: 
1077: ### ‚öôÔ∏è Tuning Self-Consistency
1078: 
1079: **[SC-Hyperparameters**:: (1) Temperature - controls diversity (0.7-1.0 recommended), (2) Num samples - more samples = higher reliability but slower (5-40 typical), (3) Confidence threshold - when to stop adaptive sampling (0.6-0.8).]**
1080: 
1081: **Temperature Selection**:
1082: - **0.3-0.5**: Low diversity, may not catch errors (not recommended)
1083: - **0.7-0.8**: Good balance (recommended for most tasks)
1084: - **0.9-1.0**: High diversity, may generate nonsense (use cautiously)
1085: 
1086: **Sample Count**:
1087: ```
1088: Minimum effective: 3 samples
1089: Typical production: 5-10 samples
1090: High-stakes: 20-40 samples
1091: ```
1092: 
1093: **Cost vs. Reliability Trade-off**:
1094: ```python
1095: # Cheap but less reliable
1096: quick_sc = self_consistency(prompt, num_samples=3)
1097: 
1098: # Balanced
1099: standard_sc = self_consistency(prompt, num_samples=5)
1100: 
1101: # Expensive but highly reliable
1102: thorough_sc = self_consistency(prompt, num_samples=20)
1103: ```
1104: 
1105: ### üîó Integration with Other Techniques
1106: 
1107: **Self-Consistency + Tree of Thoughts**:
1108: 
1109: ```python
1110: def tot_with_self_consistency(problem, branching_factor=3, sc_samples=5):
1111:     """
1112:     Use ToT to find solution, validate with Self-Consistency.
1113:     """
1114:     # Step 1: ToT exploration
1115:     tot = TreeOfThoughts(llm, branching_factor=branching_factor)
1116:     solution_path = tot.solve(problem)
1117:     
1118:     if not solution_path:
1119:         return None
1120:     
1121:     # Step 2: Validate final answer with SC
1122:     final_state = solution_path[-1]
1123:     answer_prompt = f"Given this solution path:\n{format_path(solution_path)}\nWhat is the final answer?"
1124:     
1125:     sc_result = self_consistency(answer_prompt, num_samples=sc_samples)
1126:     
1127:     return {
1128:         'solution_path': solution_path,
1129:         'final_answer': sc_result['answer'],
1130:         'confidence': sc_result['confidence']
1131:     }
1132: ```
1133: 
1134: ---
1135: 
1136: ## Program of Thoughts (PoT)
1137: 
1138: [**Program-of-Thoughts**:: Instead of expressing reasoning in natural language, generate executable code (Python) that performs calculations, with LLM writing the program and interpreter providing accurate results.]**
1139: 
1140: ### üéØ Core Concept
1141: 
1142: **[PoT-Key-Insight**:: Natural language is imprecise for mathematics. "Multiply 7.3 by 892.4" might be computed wrong in NL reasoning, but `7.3 * 892.4` in Python is always correct. PoT delegates calculation to code interpreter while LLM handles problem understanding and program construction.]**
1143: 
1144: **Standard CoT** (error-prone):
1145: ```
1146: Question: What is 1234 * 5678?
1147: Reasoning:
1148: 1234
1149: √ó5678
1150: -----
1151: 9872 (1234 √ó 8)
1152: 86380 (1234 √ó 70)
1153: ... [complex mental math]
1154: Answer: 7006652 ‚úì (if lucky)
1155: ```
1156: 
1157: **Program of Thoughts**:
1158: ```python
1159: # Question: What is 1234 * 5678?
1160: result = 1234 * 5678
1161: print(result)
1162: # Output: 7006652 ‚úì (always correct)
1163: ```
1164: 
1165: ### üî¨ How It Works
1166: 
1167: **Two Components**:
1168: 1. **LLM**: Generates Python code expressing the reasoning
1169: 2. **Interpreter**: Executes code, returns result
1170: 
1171: **[PoT-Architecture**:: LLM acts as programmer (understanding problem, decomposing into steps, writing code). Python interpreter acts as calculator (performing exact arithmetic, data manipulation). Final answer comes from code execution, not LLM generation.]**
1172: 
1173: ### üìù Complete Example: Multi-Step Math Problem
1174: 
1175: **Problem**: "A store has 1250 apples. They sell 40% on Monday, 30% of what remains on Tuesday. How many apples are left?"
1176: 
1177: **Standard CoT** (prone to calculation errors):
1178: ```
1179: Step 1: Apples sold Monday = 1250 √ó 0.4 = 500
1180: Step 2: Remaining after Monday = 1250 - 500 = 750
1181: Step 3: Apples sold Tuesday = 750 √ó 0.3 = 225
1182: Step 4: Final remaining = 750 - 225 = 525
1183: 
1184: Answer: 525 apples
1185: ```
1186: 
1187: **Program of Thoughts**:
1188: ```python
1189: # Initial apples
1190: total_apples = 1250
1191: 
1192: # Monday: sell 40%
1193: sold_monday = total_apples * 0.4
1194: remaining_monday = total_apples - sold_monday
1195: 
1196: # Tuesday: sell 30% of remaining
1197: sold_tuesday = remaining_monday * 0.3
1198: remaining_tuesday = remaining_monday - sold_tuesday
1199: 
1200: print(f"Final answer: {remaining_tuesday} apples")
1201: # Output: Final answer: 525.0 apples ‚úì
1202: ```
1203: 
1204: ### üîß PoT Implementation
1205: 
1206: ```python
1207: class ProgramOfThoughts:
1208:     """
1209:     Program of Thoughts prompting.
1210:     
1211:     LLM generates Python code, interpreter executes it.
1212:     """
1213:     
1214:     def __init__(self, llm):
1215:         self.llm = llm
1216:     
1217:     def solve(self, question, max_code_length=500):
1218:         """
1219:         Generate and execute program to solve question.
1220:         
1221:         Returns:
1222:             {'answer': final_result, 'code': generated_code, 'output': execution_output}
1223:         """
1224:         # Step 1: Generate code
1225:         code_prompt = f"""
1226: Convert this problem into Python code that solves it.
1227: 
1228: Problem: {question}
1229: 
1230: Write Python code that:
1231: 1. Defines variables for given quantities
1232: 2. Performs calculations step by step
1233: 3. Prints the final answer
1234: 
1235: Python code:
1236: ```python
1237: """
1238:         
1239:         code = self.llm.complete(code_prompt, temperature=0.0)
1240:         code = self._extract_code(code)
1241:         
1242:         # Safety check
1243:         if len(code) > max_code_length:
1244:             return {'error': 'Generated code too long (possible infinite loop)'}
1245:         
1246:         # Step 2: Execute code
1247:         execution_result = self._execute_code(code)
1248:         
1249:         return {
1250:             'code': code,
1251:             'output': execution_result['output'],
1252:             'answer': self._extract_answer(execution_result['output']),
1253:             'error': execution_result.get('error')
1254:         }
1255:     
1256:     def _extract_code(self, response):
1257:         """Extract Python code from LLM response."""
1258:         import re
1259:         
1260:         # Try to find code block
1261:         match = re.search(r'```python\n(.*?)\n```', response, re.DOTALL)
1262:         if match:
1263:             return match.group(1)
1264:         
1265:         # Fallback: treat entire response as code
1266:         return response.strip()
1267:     
1268:     def _execute_code(self, code):
1269:         """
1270:         Safely execute Python code.
1271:         
1272:         Uses restricted environment for safety.
1273:         """
1274:         import io
1275:         import sys
1276:         from contextlib import redirect_stdout
1277:         
1278:         # Capture output
1279:         output_buffer = io.StringIO()
1280:         
1281:         try:
1282:             # Execute in restricted namespace (no dangerous imports)
1283:             namespace = {'__builtins__': __builtins__}
1284:             
1285:             with redirect_stdout(output_buffer):
1286:                 exec(code, namespace)
1287:             
1288:             output = output_buffer.getvalue()
1289:             return {'output': output, 'error': None}
1290:         
1291:         except Exception as e:
1292:             return {'output': None, 'error': str(e)}
1293:     
1294:     def _extract_answer(self, output):
1295:         """Extract final numerical answer from output."""
1296:         if not output:
1297:             return None
1298:         
1299:         # Look for numbers in output
1300:         import re
1301:         numbers = re.findall(r'-?\d+\.?\d*', output)
1302:         
1303:         if numbers:
1304:             return float(numbers[-1])  # Last number is likely the answer
1305:         
1306:         return output.strip()
1307: ```
1308: 
1309: ### üí° When to Use PoT
1310: 
1311: **[PoT-Ideal-Tasks**:: (1) Multi-step arithmetic, (2) Percentage calculations, (3) Data aggregation/statistics, (4) Algorithmic problems, (5) Any task where precise calculation matters more than natural language explanation.]**
1312: 
1313: **‚úÖ Excellent For:**
1314: - **Mathematical word problems** (GSM8K, SVAMP, ASDiv benchmarks)
1315: - **Financial calculations** (interest, amortization, ROI)
1316: - **Statistical analysis** (mean, median, variance)
1317: - **Unit conversions** (currency, measurements)
1318: - **Algorithmic puzzles** (combinatorics, optimization)
1319: 
1320: **‚ùå Not Suitable For:**
1321: - **Commonsense reasoning** (no code equivalent)
1322: - **Creative writing** (not a computational task)
1323: - **Subjective questions** (no right answer to compute)
1324: - **When code execution unavailable** (interpreter required)
1325: 
1326: ### üìä Performance Benchmarks
1327: 
1328: **From Chen et al. 2022**:
1329: 
1330: | Task | CoT Accuracy | PoT Accuracy | Improvement |
1331: |------|--------------|--------------|-------------|
1332: | **GSM8K (Grade School Math)** | 46.9% | 59.8% | **+12.9pp** |
1333: | **SVAMP (Math Word Problems)** | 68.9% | 79.0% | **+10.1pp** |
1334: | **ASDiv (Diverse Math)** | 73.9% | 82.6% | **+8.7pp** |
1335: | **TabMWP (Tabular Math)** | 57.4% | 67.2% | **+9.8pp** |
1336: 
1337: **[PoT-Performance-Advantage**:: PoT consistently outperforms CoT on arithmetic-heavy tasks by 8-13 percentage points. Benefit comes from delegating calculation to Python rather than error-prone natural language arithmetic.]**
1338: 
1339: ### üîó Integration: PoT + Self-Consistency
1340: 
1341: ```python
1342: def pot_with_self_consistency(question, num_samples=5):
1343:     """
1344:     Generate multiple programs, execute all, majority vote on answers.
1345:     """
1346:     pot = ProgramOfThoughts(llm)
1347:     
1348:     answers = []
1349:     programs = []
1350:     
1351:     for i in range(num_samples):
1352:         result = pot.solve(question)
1353:         
1354:         if result.get('error'):
1355:             continue  # Skip failed executions
1356:         
1357:         programs.append(result['code'])
1358:         answers.append(result['answer'])
1359:     
1360:     # Majority vote on numerical answers
1361:     from collections import Counter
1362:     vote_counts = Counter(answers)
1363:     
1364:     if not vote_counts:
1365:         return {'error': 'All code executions failed'}
1366:     
1367:     most_common_answer, count = vote_counts.most_common(1)[0]
1368:     
1369:     return {
1370:         'answer': most_common_answer,
1371:         'confidence': count / len(answers),
1372:         'programs': programs,
1373:         'vote_distribution': dict(vote_counts)
1374:     }
1375: ```
1376: 
1377: ### ‚ö†Ô∏è Safety Considerations
1378: 
1379: **[PoT-Security**:: Executing LLM-generated code is inherently risky - model could generate malicious code (file I/O, network access, infinite loops). Always use sandboxed execution environment with strict resource limits.]**
1380: 
1381: **Mitigation Strategies**:
1382: 
1383: ```python
1384: import resource
1385: import signal
1386: 
1387: def execute_code_safely(code, timeout=5):
1388:     """
1389:     Execute code with safety restrictions.
1390:     
1391:     - Timeout after 5 seconds
1392:     - Memory limit: 256MB
1393:     - No file I/O, network access
1394:     """
1395:     # Set resource limits
1396:     resource.setrlimit(resource.RLIMIT_AS, (256 * 1024 * 1024, 256 * 1024 * 1024))
1397:     
1398:     # Set timeout
1399:     signal.signal(signal.SIGALRM, timeout_handler)
1400:     signal.alarm(timeout)
1401:     
1402:     # Restricted namespace (no dangerous modules)
1403:     safe_namespace = {
1404:         '__builtins__': {
1405:             'print': print,
1406:             'range': range,
1407:             'len': len,
1408:             'sum': sum,
1409:             'max': max,
1410:             'min': min,
1411:             'abs': abs,
1412:             # ... safe built-ins only
1413:         }
1414:     }
1415:     
1416:     try:
1417:         exec(code, safe_namespace)
1418:         signal.alarm(0)  # Cancel alarm
1419:         return {'success': True}
1420:     except Exception as e:
1421:         return {'error': str(e)}
1422: 
1423: def timeout_handler(signum, frame):
1424:     raise TimeoutError("Code execution exceeded time limit")
1425: ```
1426: 
1427: **Production Alternative**: Use cloud sandboxes (AWS Lambda, Google Cloud Functions) to isolate code execution.
1428: 
1429: ---
1430: 
1431: ## Skeleton of Thoughts (SoT)
1432: 
1433: [**Skeleton-of-Thoughts**:: Establishes structural framework/outline before elaboration, ensuring comprehensive coverage by first creating "skeleton" then "fleshing out" each component systematically.]**
1434: 
1435: ### üéØ Core Concept
1436: 
1437: **[SoT-Metaphor**:: Like an essay outline - first create structure (Introduction, Point 1, Point 2, Conclusion), then expand each section. Ensures no key aspects forgotten and logical flow.]**
1438: 
1439: **Problem**: When generating long-form content, LLMs may:
1440: - Forget to cover important aspects
1441: - Lose logical flow mid-generation
1442: - Repeat themselves
1443: - End abruptly without conclusion
1444: 
1445: **Solution**: Generate skeleton first, then expand systematically.
1446: 
1447: ### üî¨ How It Works
1448: 
1449: **Two-Stage Process**:
1450: 
1451: **Stage 1: Skeleton Generation**
1452: ```
1453: Prompt: Create an outline/structure for [task]
1454: 
1455: Output: 
1456: 1. Introduction
1457:    - Hook
1458:    - Context
1459:    - Thesis
1460: 2. Main Analysis
1461:    - Point A
1462:    - Point B
1463:    - Point C
1464: 3. Conclusion
1465:    - Summary
1466:    - Implications
1467: ```
1468: 
1469: **Stage 2: Flesh Out Skeleton**
1470: ```
1471: For each skeleton point:
1472:   Prompt: Expand "[Point]" in detail
1473:   
1474:   Output: [Detailed paragraph for that point]
1475: ```
1476: 
1477: ### üìù Complete Example: Essay Writing
1478: 
1479: **Task**: Write an analysis of renewable energy adoption challenges
1480: 
1481: **Stage 1 - Generate Skeleton**:
1482: 
1483: ```markdown
1484: # Skeleton Prompt:
1485: Create a structured outline for an essay analyzing challenges in renewable energy adoption.
1486: Include: introduction, 3-4 main challenges, conclusion
1487: 
1488: # Generated Skeleton:
1489: 1. Introduction
1490:    - Growing climate concerns
1491:    - Promise of renewable energy
1492:    - Thesis: Despite benefits, adoption faces economic, technical, and political barriers
1493: 
1494: 2. Challenge 1: Economic Barriers
1495:    - High upfront costs
1496:    - Subsidy dependence
1497:    - Competition with fossil fuels
1498: 
1499: 3. Challenge 2: Technical Limitations
1500:    - Intermittency (solar/wind)
1501:    - Storage challenges
1502:    - Grid infrastructure needs
1503: 
1504: 4. Challenge 3: Political/Regulatory
1505:    - Policy inconsistency
1506:    - Fossil fuel lobbying
1507:    - International coordination difficulties
1508: 
1509: 5. Conclusion
1510:    - Recap challenges
1511:    - Path forward: innovation + policy
1512:    - Cautious optimism
1513: ```
1514: 
1515: **Stage 2 - Flesh Out Each Point**:
1516: 
1517: ```markdown
1518: # Expansion Prompt for Point 1:
1519: Expand this outline point into 2-3 detailed paragraphs:
1520: 
1521: "Introduction
1522: - Growing climate concerns
1523: - Promise of renewable energy
1524: - Thesis: Despite benefits, adoption faces economic, technical, and political barriers"
1525: 
1526: # Generated Expansion:
1527: The escalating climate crisis has thrust renewable energy into the global spotlight...
1528: [2-3 paragraphs expanding introduction]
1529: 
1530: # Repeat for each skeleton point...
1531: ```
1532: 
1533: ### üîß Implementation
1534: 
1535: ```python
1536: class SkeletonOfThoughts:
1537:     """
1538:     Two-stage generation: skeleton then expansion.
1539:     """
1540:     
1541:     def __init__(self, llm):
1542:         self.llm = llm
1543:     
1544:     def generate(self, task, detail_level="medium"):
1545:         """
1546:         Generate content using skeleton-first approach.
1547:         
1548:         Args:
1549:             task: Description of content to generate
1550:             detail_level: "brief", "medium", "detailed"
1551:         
1552:         Returns:
1553:             Complete expanded content
1554:         """
1555:         # Stage 1: Generate skeleton
1556:         skeleton = self._generate_skeleton(task)
1557:         
1558:         # Stage 2: Expand each skeleton point
1559:         expanded_sections = []
1560:         for point in skeleton:
1561:             expansion = self._expand_point(point, detail_level)
1562:             expanded_sections.append(expansion)
1563:         
1564:         # Combine into final output
1565:         final_output = self._combine_sections(expanded_sections)
1566:         
1567:         return {
1568:             'skeleton': skeleton,
1569:             'expanded': final_output
1570:         }
1571:     
1572:     def _generate_skeleton(self, task):
1573:         """Generate structural outline."""
1574:         prompt = f"""
1575: Create a structured outline for: {task}
1576: 
1577: Requirements:
1578: - Include introduction and conclusion
1579: - Identify 3-5 main points/sections
1580: - Each section should have 2-4 sub-points
1581: - Use clear hierarchical structure
1582: 
1583: Outline:
1584: """
1585:         
1586:         response = self.llm.complete(prompt, temperature=0.3)
1587:         skeleton = self._parse_skeleton(response)
1588:         return skeleton
1589:     
1590:     def _expand_point(self, point, detail_level):
1591:         """Expand a single skeleton point."""
1592:         expansion_targets = {
1593:             'brief': "1 paragraph",
1594:             'medium': "2-3 paragraphs",
1595:             'detailed': "3-5 paragraphs with examples"
1596:         }
1597:         
1598:         prompt = f"""
1599: Expand this outline point in detail:
1600: 
1601: {point}
1602: 
1603: Target length: {expansion_targets[detail_level]}
1604: 
1605: Make the expansion:
1606: - Comprehensive (cover all sub-points)
1607: - Well-structured (clear progression)
1608: - Informative (specific details, not vague)
1609: 
1610: Expansion:
1611: """
1612:         
1613:         response = self.llm.complete(prompt, temperature=0.7)
1614:         return response
1615:     
1616:     def _parse_skeleton(self, outline_text):
1617:         """Parse outline into structured list."""
1618:         # Simple parsing - can be made more sophisticated
1619:         lines = outline_text.strip().split('\n')
1620:         skeleton = []
1621:         
1622:         for line in lines:
1623:             if line.strip() and not line.strip().startswith('#'):
1624:                 skeleton.append(line.strip())
1625:         
1626:         return skeleton
1627:     
1628:     def _combine_sections(self, sections):
1629:         """Combine expanded sections into coherent whole."""
1630:         return "\n\n".join(sections)
1631: ```
1632: 
1633: ### üí° When to Use SoT
1634: 
1635: **‚úÖ Ideal For:**
1636: - **Long-form content** (essays, articles, reports)
1637: - **Structured analysis** (business plans, research papers)
1638: - **Multi-faceted topics** (ensuring comprehensive coverage)
1639: - **Complex arguments** (maintaining logical flow)
1640: 
1641: **‚ùå Not Useful For:**
1642: - **Short responses** (overhead not worth it)
1643: - **Highly creative writing** (structure may constrain creativity)
1644: - **Real-time responses** (two-stage generation is slower)
1645: 
1646: ### üìä Benefits
1647: 
1648: **[SoT-Advantages**:: (1) Ensures comprehensive coverage - skeleton prevents forgetting key points, (2) Maintains logical flow - structure guides coherent progression, (3) Enables parallelization - can expand multiple skeleton points simultaneously, (4) Improves planning - forces upfront thinking about scope.]**
1649: 
1650: ---
1651: 
1652: ## Technique Selection Matrix
1653: 
1654: ### Quick Decision Guide
1655: 
1656: ```
1657: START: What's your primary goal?
1658: 
1659: ‚îå‚îÄ RELIABILITY/ACCURACY
1660: ‚îÇ  ‚îú‚îÄ Simple task ‚Üí Self-Consistency (5 samples)
1661: ‚îÇ  ‚îú‚îÄ Complex reasoning ‚Üí ToT + Self-Consistency
1662: ‚îÇ  ‚îî‚îÄ Mathematical ‚Üí Program of Thoughts
1663: ‚îÇ
1664: ‚îú‚îÄ EXPLORATION/CREATIVITY
1665: ‚îÇ  ‚îú‚îÄ Hierarchical problem ‚Üí Tree of Thoughts
1666: ‚îÇ  ‚îú‚îÄ Interconnected concepts ‚Üí Graph of Thoughts
1667: ‚îÇ  ‚îî‚îÄ Multiple perspectives ‚Üí Self-Consistency (high diversity)
1668: ‚îÇ
1669: ‚îú‚îÄ STRUCTURED CONTENT
1670: ‚îÇ  ‚îú‚îÄ Long-form ‚Üí Skeleton of Thoughts
1671: ‚îÇ  ‚îú‚îÄ Multi-step calculation ‚Üí Program of Thoughts
1672: ‚îÇ  ‚îî‚îÄ Comparative analysis ‚Üí Graph of Thoughts
1673: ‚îÇ
1674: ‚îî‚îÄ EFFICIENCY/SPEED
1675:    ‚îú‚îÄ Moderate reliability boost ‚Üí Self-Consistency (3 samples)
1676:    ‚îú‚îÄ Mathematical precision ‚Üí Program of Thoughts
1677:    ‚îî‚îÄ Standard cases ‚Üí Chain of Thought (not covered here, but baseline)
1678: ```
1679: 
1680: ### Combination Strategies
1681: 
1682: | Primary | Add | Benefit | Use Case |
1683: |---------|-----|---------|----------|
1684: | **ToT** | Self-Consistency | Validate ToT solution | High-stakes planning |
1685: | **PoT** | Self-Consistency | Multiple programs vote | Critical calculations |
1686: | **SoT** | Self-Consistency | Multiple skeleton variants | Important documents |
1687: | **ToT** | PoT | Use code for ToT state evaluation | Game of 24 |
1688: | **GoT** | Self-Consistency | Validate graph synthesis | Multi-source analysis |
1689: 
1690: ---
1691: 
1692: ## Integration Patterns
1693: 
1694: ### Pattern 1: ToT for Exploration + SC for Validation
1695: 
1696: ```python
1697: def tot_sc_pipeline(problem, tot_depth=4, sc_samples=5):
1698:     """
1699:     Use ToT to explore solution space deeply,
1700:     then Self-Consistency to validate final answer.
1701:     """
1702:     # Stage 1: ToT exploration
1703:     tot = TreeOfThoughts(llm)
1704:     solution_candidates = tot.solve(problem, max_depth=tot_depth)
1705:     
1706:     if not solution_candidates:
1707:         return {'error': 'No solution found via ToT'}
1708:     
1709:     # Stage 2: Extract answer from ToT path
1710:     tot_answer = extract_answer_from_path(solution_candidates)
1711:     
1712:     # Stage 3: Validate with SC
1713:     validation_prompt = f"""
1714: Problem: {problem}
1715: Proposed solution: {tot_answer}
1716: 
1717: Verify this solution is correct. If incorrect, provide correct answer.
1718: 
1719: Answer:
1720: """
1721:     
1722:     sc_result = self_consistency(validation_prompt, num_samples=sc_samples)
1723:     
1724:     return {
1725:         'tot_solution': tot_answer,
1726:         'validated_answer': sc_result['answer'],
1727:         'confidence': sc_result['confidence'],
1728:         'agreement': tot_answer == sc_result['answer']
1729:     }
1730: ```
1731: 
1732: ### Pattern 2: SoT + PoT for Structured Analysis
1733: 
1734: ```python
1735: def sot_pot_report(data, analysis_task):
1736:     """
1737:     Use SoT for structure, PoT for calculations.
1738:     
1739:     Example: Financial report generation
1740:     """
1741:     sot = SkeletonOfThoughts(llm)
1742:     pot = ProgramOfThoughts(llm)
1743:     
1744:     # Stage 1: Generate report skeleton
1745:     skeleton_prompt = f"Create outline for {analysis_task} report analyzing: {data}"
1746:     skeleton = sot._generate_skeleton(skeleton_prompt)
1747:     
1748:     # Stage 2: For each section, determine if computation needed
1749:     expanded_sections = []
1750:     
1751:     for section in skeleton:
1752:         if requires_calculation(section):
1753:             # Use PoT for numerical sections
1754:             code_result = pot.solve(f"Calculate {section} from data: {data}")
1755:             expanded_sections.append({
1756:                 'section': section,
1757:                 'content': format_numerical_results(code_result),
1758:                 'method': 'PoT'
1759:             })
1760:         else:
1761:             # Use standard expansion for narrative sections
1762:             expansion = sot._expand_point(section, 'medium')
1763:             expanded_sections.append({
1764:                 'section': section,
1765:                 'content': expansion,
1766:                 'method': 'Narrative'
1767:             })
1768:     
1769:     return sot._combine_sections([s['content'] for s in expanded_sections])
1770: ```
1771: 
1772: ---
1773: 
1774: ## Research References
1775: 
1776: ### Tree of Thoughts
1777: - **[Yao et al. 2023](https://arxiv.org/abs/2305.10601)** - "Tree of Thoughts: Deliberate Problem Solving with Large Language Models" - NeurIPS 2024
1778: - **[Long 2023](https://arxiv.org/abs/2305.08291)** - "Large Language Model Guided Tree-of-Thought"
1779: 
1780: ### Graph of Thoughts
1781: - **[Besta et al. 2024](https://arxiv.org/abs/2308.09687)** - "Graph of Thoughts: Solving Elaborate Problems with Large Language Models" - AAAI 2024
1782: 
1783: ### Self-Consistency
1784: - **[Wang et al. 2022](https://arxiv.org/abs/2203.11171)** - "Self-Consistency Improves Chain of Thought Reasoning in Language Models" - ICLR 2023
1785: 
1786: ### Program of Thoughts
1787: - **[Chen et al. 2022](https://arxiv.org/abs/2211.12588)** - "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"
1788: 
1789: ### Skeleton of Thoughts
1790: - **[Ning et al. 2023](https://arxiv.org/abs/2307.15337)** - "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"
1791: 
1792: ### Foundational Chain of Thought
1793: - **[Wei et al. 2022](https://arxiv.org/abs/2201.11903)** - "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" - NeurIPS 2022
1794: - **[Kojima et al. 2022](https://arxiv.org/abs/2205.11916)** - "Large Language Models are Zero-Shot Reasoners" - NeurIPS 2022
1795: 
1796: ---
1797: 
1798: ## üîó Related Topics for PKB Expansion
1799: 
1800: 1. **[[agentic-reasoning-frameworks]]**
1801:    - **Connection**: ReAct, Reflexion extend reasoning with tool use and learning
1802:    - **Depth Potential**: Agent architectures combining reasoning + action + memory
1803:    - **Knowledge Graph Role**: Bridges reasoning techniques to autonomous systems
1804:    - **Priority**: High - natural progression from reasoning to agency
1805: 
1806: 2. **[[evaluation-metrics-for-reasoning]]**
1807:    - **Connection**: How to measure quality of ToT, SC, PoT outputs
1808:    - **Depth Potential**: Automated scoring, human evaluation, benchmark datasets
1809:    - **Knowledge Graph Role**: Quality assurance methodology for reasoning systems
1810:    - **Priority**: Medium - essential for production deployment
1811: 
1812: 3. **[[computational-efficiency-reasoning-techniques]]**
1813:    - **Connection**: Token optimization, caching, parallelization strategies
1814:    - **Depth Potential**: Making ToT/GoT practical at scale, cost-benefit analysis
1815:    - **Knowledge Graph Role**: Production engineering considerations
1816:    - **Priority**: High - critical for real-world use
1817: 
1818: 4. **[[reasoning-model-capabilities]]**
1819:    - **Connection**: Which techniques work best with different model families
1820:    - **Depth Potential**: Model-specific optimization (GPT-4 vs Claude vs Gemini)
1821:    - **Knowledge Graph Role**: Model selection guide for reasoning tasks
1822:    - **Priority**: Medium - helps choose right tool for job
1823: 
1824: 5. **[[combining-symbolic-neural-reasoning]]**
1825:    - **Connection**: PoT bridges symbolic (code) and neural (LLM) reasoning
1826:    - **Depth Potential**: Neuro-symbolic AI, formal verification with LLMs
1827:    - **Knowledge Graph Role**: Theoretical foundations of hybrid reasoning
1828:    - **Priority**: Low - advanced topic for later exploration
1829: 
1830: 6. **[[reasoning-task-taxonomy]]**
1831:    - **Connection**: Classification of reasoning types and matching techniques
1832:    - **Depth Potential**: When to use which technique based on task characteristics
1833:    - **Knowledge Graph Role**: Decision framework for technique selection
1834:    - **Priority**: High - practical navigation tool
1835: 
1836: ---
1837: 
1838: *This guide synthesizes research from 2022-2024 on advanced reasoning techniques. For implementation support, see Quick Reference Cards. For integration patterns, see [[06-integration-patterns-guide]].*
</file>

<file path="__LOCAL-REPO/__exemplar/__import/__master-exemplar/02-agentic-frameworks-guide.md">
   1: ---
   2: tags: #prompt-engineering #agentic-ai #react #reflexion #autonomous-agents #tool-use #reference
   3: aliases: [Agentic AI, ReAct Guide, Agent Frameworks, Tool-Using Agents]
   4: status: evergreen
   5: certainty: verified
   6: priority: high
   7: created: 2025-12-25
   8: modified: 2025-12-25
   9: type: reference
  10: version: 1.0.0
  11: source: claude-sonnet-4.5
  12: category: agentic-frameworks
  13: ---
  14: 
  15: # Agentic Frameworks Guide
  16: 
  17: > [!abstract] Purpose
  18: > Comprehensive guide to frameworks enabling autonomous agent behavior in LLMs through tool integration, iterative learning, and structured action cycles. Covers ReAct, Reflexion, ART, and ReWOO based on 2022-2023 research.
  19: 
  20: ---
  21: 
  22: ## üìã Table of Contents
  23: 
  24: 1. [[#Overview & Agent Paradigm]]
  25: 2. [[#ReAct Framework]]
  26: 3. [[#Reflexion Framework]]
  27: 4. [[#ART (Automatic Reasoning & Tool-use)]]
  28: 5. [[#ReWOO (Reasoning Without Observation)]]
  29: 6. [[#Technique Comparison Matrix]]
  30: 7. [[#Integration Patterns]]
  31: 8. [[#Research References]]
  32: 
  33: ---
  34: 
  35: ## Overview & Agent Paradigm
  36: 
  37: [**Agentic-Framework**:: System architecture enabling LLMs to function as autonomous agents through structured interaction patterns with external tools, environments, and self-evaluation mechanisms, transforming passive text generators into active problem solvers.]
  38: 
  39: ### **What Makes an Agent?**
  40: 
  41: **[Agent-Definition**:: An autonomous entity that perceives environment through observations, reasons about actions to take, executes those actions via tools/APIs, and learns from outcomes - contrasted with traditional LLMs that simply generate text without environment interaction.]**
  42: 
  43: **Traditional LLM**:
  44: ```
  45: Input ‚Üí LLM ‚Üí Output
  46: (Single pass, no interaction)
  47: ```
  48: 
  49: **Agentic LLM**:
  50: ```
  51: Input ‚Üí Think ‚Üí Act ‚Üí Observe ‚Üí Think ‚Üí Act ‚Üí ... ‚Üí Final Answer
  52:        ‚Üë_____‚Üì      ‚Üë_____‚Üì     ‚Üë______‚Üì
  53:       (Reasoning) (Tool Use) (Feedback)
  54: ```
  55: 
  56: ### **Core Components of Agentic Systems**
  57: 
  58: 1. **Perception**: Receiving observations from environment/tools
  59: 2. **Reasoning**: Deciding what action to take next
  60: 3. **Action**: Executing operations via APIs/tools
  61: 4. **Memory**: Retaining context across interactions
  62: 5. **Learning**: Improving from past experiences (advanced)
  63: 
  64: ### **Evolution of Agentic Capabilities**
  65: 
  66: ```mermaid
  67: graph LR
  68:     A[Chain of Thought<br/>Reasoning only] --> B[ReAct<br/>Reasoning + Actions]
  69:     B --> C[Reflexion<br/>+ Self-Correction]
  70:     C --> D[ART<br/>+ Task Libraries]
  71:     B --> E[ReWOO<br/>+ Planning/Execution Split]
  72: ```
  73: 
  74: ### **Comparison Matrix**
  75: 
  76: | Framework | Learning | Memory | Tool Use | Planning | Best For | Complexity |
  77: |-----------|----------|--------|----------|----------|----------|------------|
  78: | **ReAct** | ‚ùå No | Session only | ‚úÖ Yes | Implicit | General tool use | Medium |
  79: | **Reflexion** | ‚úÖ Yes | Episodic | ‚úÖ Yes | Implicit | Improving agents | High |
  80: | **ART** | ‚ùå No | Task library | ‚úÖ Yes | Explicit | Multi-tool workflows | High |
  81: | **ReWOO** | ‚ùå No | None | ‚úÖ Yes | Explicit | Token efficiency | Medium |
  82: 
  83: ---
  84: 
  85: ## ReAct Framework
  86: 
  87: [**ReAct**:: "Reasoning and Acting" - framework synergizing verbal reasoning traces with action execution in interleaved manner, enabling LLMs to generate reasoning steps (Thought), execute actions (Act), and process feedback (Observe) in iterative cycles.]
  88: 
  89: ### üéØ Core Concept
  90: 
  91: **[ReAct-Paradigm-Shift**:: Traditional approaches separate reasoning (CoT) from acting (tool use). ReAct unifies them: model alternates between thinking about what to do and actually doing it, using observations to inform next thoughts in dynamic feedback loop.]**
  92: 
  93: **Problem**: Pure reasoning (CoT) cannot access external information. Pure action (tool calling) lacks interpretability and planning.
  94: 
  95: **Solution**: Interleave reasoning and acting in structured loop.
  96: 
  97: ### üî¨ How It Works
  98: 
  99: **The Thought-Action-Observation Cycle**:
 100: 
 101: ```
 102: THOUGHT: [Reasoning about current situation and next step]
 103: ACTION: [Tool to call with specific parameters]
 104: OBSERVATION: [Result from tool execution]
 105: THOUGHT: [Reasoning about observation]
 106: ACTION: [Next tool call based on new information]
 107: OBSERVATION: [New result]
 108: ...
 109: THOUGHT: [Final reasoning]
 110: ANSWER: [Final response to user]
 111: ```
 112: 
 113: **[ReAct-Loop-Structure**:: Each iteration contains three components - (1) Thought: internal monologue about what's known and what's needed, (2) Action: structured command to execute via tool/API, (3) Observation: feedback from environment that informs next iteration.]**
 114: 
 115: ### üìù Complete Example: Question Answering with Search
 116: 
 117: **Question**: "Who was the president of the United States when the iPhone was first released?"
 118: 
 119: **ReAct Execution**:
 120: 
 121: ```
 122: THOUGHT 1: I need to find when the iPhone was first released, then determine who was president at that time.
 123: 
 124: ACTION 1: Search["when was iPhone first released"]
 125: 
 126: OBSERVATION 1: The first iPhone was released on June 29, 2007.
 127: 
 128: THOUGHT 2: Now I know the iPhone was released on June 29, 2007. I need to find who was the U.S. president in 2007.
 129: 
 130: ACTION 2: Search["US president in 2007"]
 131: 
 132: OBSERVATION 2: George W. Bush was the 43rd president of the United States, serving from 2001 to 2009.
 133: 
 134: THOUGHT 3: George W. Bush was president from 2001-2009, and the iPhone was released on June 29, 2007, which falls within his presidency. I can now provide the final answer.
 135: 
 136: ANSWER: George W. Bush was the president of the United States when the iPhone was first released in June 2007.
 137: ```
 138: 
 139: ### üîß Production Implementation
 140: 
 141: ```python
 142: class ReActAgent:
 143:     """
 144:     ReAct Framework implementation.
 145:     
 146:     Enables LLM to reason and act in interleaved manner,
 147:     using tools to gather information and accomplish tasks.
 148:     """
 149:     
 150:     def __init__(self, llm, tools, max_iterations=10):
 151:         """
 152:         Initialize ReAct agent.
 153:         
 154:         Args:
 155:             llm: Language model client
 156:             tools: Dict of available tools {name: function}
 157:             max_iterations: Maximum thought-action cycles
 158:         """
 159:         self.llm = llm
 160:         self.tools = tools
 161:         self.max_iterations = max_iterations
 162:         self.trajectory = []  # Store full execution trace
 163:     
 164:     def run(self, task):
 165:         """
 166:         Execute task using ReAct loop.
 167:         
 168:         Args:
 169:             task: User's question or objective
 170:         
 171:         Returns:
 172:             Final answer with execution trace
 173:         """
 174:         self.trajectory = []
 175:         
 176:         # System prompt establishing ReAct pattern
 177:         system_prompt = self._build_system_prompt()
 178:         
 179:         # Initialize context with task
 180:         context = f"Question: {task}\n\n"
 181:         
 182:         for iteration in range(self.max_iterations):
 183:             # Generate thought and action
 184:             response = self.llm.complete(
 185:                 system_prompt + context,
 186:                 temperature=0.0
 187:             )
 188:             
 189:             # Parse response
 190:             thought, action, action_input = self._parse_response(response)
 191:             
 192:             if thought:
 193:                 self.trajectory.append(('THOUGHT', thought))
 194:                 context += f"THOUGHT {iteration + 1}: {thought}\n"
 195:             
 196:             # Check if final answer reached
 197:             if action == 'FINISH':
 198:                 self.trajectory.append(('ANSWER', action_input))
 199:                 return {
 200:                     'answer': action_input,
 201:                     'trajectory': self.trajectory,
 202:                     'iterations': iteration + 1
 203:                 }
 204:             
 205:             # Execute action
 206:             if action in self.tools:
 207:                 observation = self._execute_tool(action, action_input)
 208:                 self.trajectory.append(('ACTION', f"{action}[{action_input}]"))
 209:                 self.trajectory.append(('OBSERVATION', observation))
 210:                 
 211:                 context += f"ACTION {iteration + 1}: {action}[{action_input}]\n"
 212:                 context += f"OBSERVATION {iteration + 1}: {observation}\n\n"
 213:             else:
 214:                 # Invalid action
 215:                 observation = f"Error: Tool '{action}' not available. Available tools: {list(self.tools.keys())}"
 216:                 context += f"OBSERVATION {iteration + 1}: {observation}\n\n"
 217:         
 218:         # Max iterations reached without answer
 219:         return {
 220:             'answer': "Could not reach conclusion within iteration limit",
 221:             'trajectory': self.trajectory,
 222:             'iterations': self.max_iterations
 223:         }
 224:     
 225:     def _build_system_prompt(self):
 226:         """Construct system prompt defining ReAct pattern."""
 227:         tool_descriptions = "\n".join(
 228:             f"- {name}: {tool.__doc__ or 'No description'}"
 229:             for name, tool in self.tools.items()
 230:         )
 231:         
 232:         return f"""You are a helpful assistant that can use tools to answer questions.
 233: 
 234: Available tools:
 235: {tool_descriptions}
 236: 
 237: Follow this format for EVERY step:
 238: 
 239: THOUGHT: [Your reasoning about what to do next]
 240: ACTION: [Tool name from available tools, or FINISH if ready to answer]
 241: ACTION INPUT: [Input for the tool, or final answer if ACTION is FINISH]
 242: 
 243: You will receive:
 244: OBSERVATION: [Result from tool execution]
 245: 
 246: Then continue with next THOUGHT-ACTION-OBSERVATION cycle.
 247: 
 248: When you have enough information to answer the original question:
 249: THOUGHT: [Final reasoning]
 250: ACTION: FINISH
 251: ACTION INPUT: [Your final answer]
 252: 
 253: Begin!
 254: 
 255: """
 256:     
 257:     def _parse_response(self, response):
 258:         """Extract thought, action, and action input from LLM response."""
 259:         import re
 260:         
 261:         # Extract THOUGHT
 262:         thought_match = re.search(r'THOUGHT:?\s*(.+?)(?=ACTION:|$)', response, re.DOTALL | re.IGNORECASE)
 263:         thought = thought_match.group(1).strip() if thought_match else None
 264:         
 265:         # Extract ACTION
 266:         action_match = re.search(r'ACTION:?\s*(\w+)', response, re.IGNORECASE)
 267:         action = action_match.group(1).strip() if action_match else None
 268:         
 269:         # Extract ACTION INPUT
 270:         input_match = re.search(r'ACTION INPUT:?\s*(.+?)(?=OBSERVATION:|$)', response, re.DOTALL | re.IGNORECASE)
 271:         action_input = input_match.group(1).strip() if input_match else None
 272:         
 273:         return thought, action, action_input
 274:     
 275:     def _execute_tool(self, tool_name, tool_input):
 276:         """Execute tool and return observation."""
 277:         try:
 278:             result = self.tools[tool_name](tool_input)
 279:             return str(result)
 280:         except Exception as e:
 281:             return f"Error executing {tool_name}: {str(e)}"
 282: ```
 283: 
 284: ### üí° Example Tools Integration
 285: 
 286: ```python
 287: # Define tools the agent can use
 288: def web_search(query):
 289:     """Search the web for information."""
 290:     # In production, integrate with actual search API
 291:     # Here's a mock example
 292:     search_results = {
 293:         "when was iPhone first released": "The first iPhone was released on June 29, 2007.",
 294:         "US president in 2007": "George W. Bush was president from 2001-2009.",
 295:         # ... more results
 296:     }
 297:     return search_results.get(query, "No results found.")
 298: 
 299: def calculator(expression):
 300:     """Evaluate mathematical expressions."""
 301:     try:
 302:         # Safe eval with restricted namespace
 303:         result = eval(expression, {"__builtins__": {}}, {})
 304:         return f"Result: {result}"
 305:     except Exception as e:
 306:         return f"Error: {str(e)}"
 307: 
 308: def wikipedia_lookup(entity):
 309:     """Look up entity on Wikipedia."""
 310:     # Mock implementation
 311:     wiki_data = {
 312:         "George W. Bush": "43rd President of the United States (2001-2009)",
 313:         "iPhone": "Smartphone designed by Apple Inc., first released June 29, 2007",
 314:     }
 315:     return wiki_data.get(entity, "Entity not found in Wikipedia.")
 316: 
 317: # Create agent with tools
 318: tools = {
 319:     'Search': web_search,
 320:     'Calculator': calculator,
 321:     'Wikipedia': wikipedia_lookup
 322: }
 323: 
 324: agent = ReActAgent(llm=your_llm_client, tools=tools)
 325: 
 326: # Run task
 327: result = agent.run("What is 15% of the number of days between iPhone release and today?")
 328: ```
 329: 
 330: ### üìä Performance Benchmarks
 331: 
 332: **From Yao et al. 2022 (ICLR 2023)**:
 333: 
 334: | Task | Baseline | ReAct | Improvement |
 335: |------|----------|-------|-------------|
 336: | **HotpotQA** (Multi-hop QA) | 27.4% | 35.1% | **+7.7pp** |
 337: | **FEVER** (Fact Verification) | 56.3% | 60.9% | **+4.6pp** |
 338: | **AlfWorld** (Interactive Planning) | 34% | 71% | **+37pp** |
 339: | **WebShop** (Web Navigation) | 28.7% | 50.0% | **+21.3pp** |
 340: 
 341: **[ReAct-Performance-Pattern**:: Largest gains on tasks requiring external information access (web search, APIs) and interactive environments (games, simulators). Moderate gains on pure reasoning tasks where tools add limited value.]**
 342: 
 343: ### üí° When to Use ReAct
 344: 
 345: **‚úÖ Excellent For:**
 346: - **Information lookup** (search engines, databases, APIs)
 347: - **Multi-step research** (gathering facts from multiple sources)
 348: - **Interactive environments** (games, simulations, robotics)
 349: - **Tool orchestration** (file systems, calculators, code execution)
 350: - **Dynamic tasks** where information needs emerge during execution
 351: 
 352: **‚ùå Not Suitable For:**
 353: - **Pure reasoning** (no external information needed ‚Üí use CoT instead)
 354: - **Real-time constraints** (tool calls add latency)
 355: - **No tool access** (framework requires executable actions)
 356: - **Simple queries** (overhead not worth it)
 357: 
 358: ### ‚öôÔ∏è ReAct Prompt Engineering Tips
 359: 
 360: **[ReAct-Prompt-Best-Practices**:: (1) Explicit format specification reduces parsing errors, (2) Tool descriptions must be clear and unambiguous, (3) Few-shot examples dramatically improve action selection, (4) Error handling in observations helps agent recover, (5) Iteration limits prevent infinite loops.]**
 361: 
 362: **Improved System Prompt with Examples**:
 363: 
 364: ```markdown
 365: You solve tasks by alternating between thinking and acting.
 366: 
 367: FORMAT:
 368: THOUGHT: [Reasoning about current state]
 369: ACTION: [Tool name]
 370: ACTION INPUT: [Tool parameter]
 371: [You receive OBSERVATION: [Tool output]]
 372: ... repeat until solved ...
 373: THOUGHT: [Final reasoning]
 374: ACTION: FINISH
 375: ACTION INPUT: [Final answer]
 376: 
 377: AVAILABLE TOOLS:
 378: - Search[query]: Web search
 379: - Wikipedia[entity]: Look up entity
 380: - Calculator[expression]: Evaluate math
 381: 
 382: EXAMPLE:
 383: Question: What is the age difference between Barack Obama and Donald Trump?
 384: 
 385: THOUGHT: I need to find the birth years of both people.
 386: ACTION: Wikipedia
 387: ACTION INPUT: Barack Obama
 388: 
 389: OBSERVATION: Barack Obama, born August 4, 1961, 44th President...
 390: 
 391: THOUGHT: Obama was born in 1961. Now I need Trump's birth year.
 392: ACTION: Wikipedia  
 393: ACTION INPUT: Donald Trump
 394: 
 395: OBSERVATION: Donald Trump, born June 14, 1946, 45th President...
 396: 
 397: THOUGHT: Trump born 1946, Obama born 1961. Difference is 1961-1946=15 years.
 398: ACTION: FINISH
 399: ACTION INPUT: The age difference is 15 years, with Donald Trump being older.
 400: 
 401: Now solve this:
 402: Question: {user_question}
 403: ```
 404: 
 405: ### üîó ReAct Variations
 406: 
 407: **ReAct + Chain of Thought**:
 408: ```python
 409: # Enhanced thought quality with CoT
 410: def react_with_cot(agent, task):
 411:     """ReAct where thoughts use chain-of-thought reasoning."""
 412:     # Modify system prompt to encourage step-by-step thinking
 413:     enhanced_prompt = """
 414: THOUGHT: [Break down your reasoning step by step:
 415: 1. What do I know?
 416: 2. What do I need to find out?
 417: 3. What tool should I use?]
 418: ACTION: [Tool]
 419: ACTION INPUT: [Input]
 420: """
 421:     # Rest of implementation...
 422: ```
 423: 
 424: **ReAct + Self-Consistency**:
 425: ```python
 426: def react_with_sc(agent, task, num_paths=3):
 427:     """Run ReAct multiple times, vote on final answers."""
 428:     results = []
 429:     
 430:     for i in range(num_paths):
 431:         result = agent.run(task)
 432:         results.append(result['answer'])
 433:     
 434:     # Majority vote
 435:     from collections import Counter
 436:     votes = Counter(results)
 437:     best_answer = votes.most_common(1)[0][0]
 438:     
 439:     return best_answer
 440: ```
 441: 
 442: ---
 443: 
 444: ## Reflexion Framework
 445: 
 446: [**Reflexion**:: Advanced agentic framework with self-reflection and episodic memory, enabling agents to learn from mistakes across multiple trials through verbal self-evaluation and experience storage.]
 447: 
 448: ### üéØ Core Concept
 449: 
 450: **[Reflexion-Innovation**:: ReAct executes one trajectory per task with no learning. Reflexion adds (1) Evaluator to assess trajectory quality, (2) Self-Reflection to generate improvement insights, (3) Episodic Memory to store past attempts and learnings, enabling iterative improvement across trials.]**
 451: 
 452: **ReAct Limitation**: Each task execution is independent - agent doesn't learn from past failures.
 453: 
 454: **Reflexion Solution**: After each trial, agent reflects on failures, stores insights in memory, uses them in subsequent attempts.
 455: 
 456: ### üî¨ Architecture
 457: 
 458: **Four Core Components** (Shinn et al. 2023):
 459: 
 460: 1. **Actor**: ReAct-style agent executing tasks
 461: 2. **Evaluator**: Scores trajectory quality (success/failure)
 462: 3. **Self-Reflection**: Generates verbal analysis of failures
 463: 4. **Memory**: Stores reflections for future trials
 464: 
 465: ```mermaid
 466: graph TD
 467:     A[Task] --> B[Actor: Execute Trial]
 468:     B --> C[Evaluator: Check Success]
 469:     C -->|Success| D[Return Answer]
 470:     C -->|Failure| E[Self-Reflection: Analyze]
 471:     E --> F[Memory: Store Reflection]
 472:     F --> B
 473:     B -.->|Reads from| F
 474: ```
 475: 
 476: **[Reflexion-Loop**:: Trial ‚Üí Evaluate ‚Üí Reflect ‚Üí Remember ‚Üí Retry with improved strategy. Process continues for max_trials or until success, with each trial informed by reflections from previous failures.]**
 477: 
 478: ### üìù Complete Example: Coding Task
 479: 
 480: **Task**: "Write a function to find the longest palindromic substring"
 481: 
 482: **Trial 1**:
 483: ```python
 484: # Actor generates code
 485: def longest_palindrome(s):
 486:     return s[::-1]  # Wrong approach - just reverses string
 487: 
 488: # Evaluator tests code
 489: test_cases = [("babad", "bab"), ("cbbd", "bb")]
 490: result = evaluate(longest_palindrome, test_cases)
 491: # Result: FAILED (0/2 tests passed)
 492: 
 493: # Self-Reflection generates
 494: reflection_1 = """
 495: REFLECTION: My solution simply reversed the entire string, which is incorrect.
 496: I confused "palindrome" with "reverse". A palindrome reads the same forwards
 497: and backwards, but the task asks for the longest SUBSTRING that is a palindrome,
 498: not the whole string reversed.
 499: 
 500: KEY INSIGHT: I need to:
 501: 1. Check all substrings
 502: 2. For each substring, test if it's a palindrome
 503: 3. Track the longest one found
 504: 
 505: MISTAKE: Fundamental misunderstanding of problem requirements.
 506: """
 507: 
 508: # Memory stores reflection_1
 509: ```
 510: 
 511: **Trial 2** (with reflection from Trial 1):
 512: ```python
 513: # Actor reads reflection_1 from memory, tries again
 514: def longest_palindrome(s):
 515:     longest = ""
 516:     for i in range(len(s)):
 517:         for j in range(i, len(s)):
 518:             substr = s[i:j+1]
 519:             if substr == substr[::-1] and len(substr) > len(longest):
 520:                 longest = substr
 521:     return longest
 522: 
 523: # Evaluator tests
 524: result = evaluate(longest_palindrome, test_cases)
 525: # Result: FAILED (1/2 tests passed - timeout on long strings)
 526: 
 527: # Self-Reflection
 528: reflection_2 = """
 529: REFLECTION: My solution is correct in logic but inefficient (O(n¬≥)).
 530: It checks every substring and reverses each to test palindrome property.
 531: For long strings, this times out.
 532: 
 533: KEY INSIGHT: I need a more efficient approach:
 534: - Expand around centers (O(n¬≤)) would be better
 535: - There are 2n-1 possible centers (each char + between each pair)
 536: 
 537: MISTAKE: Correct algorithm but poor time complexity.
 538: """
 539: 
 540: # Memory stores reflection_2
 541: ```
 542: 
 543: **Trial 3** (with reflections 1 & 2):
 544: ```python
 545: # Actor incorporates both reflections
 546: def longest_palindrome(s):
 547:     def expand_around_center(left, right):
 548:         while left >= 0 and right < len(s) and s[left] == s[right]:
 549:             left -= 1
 550:             right += 1
 551:         return s[left+1:right]
 552:     
 553:     longest = ""
 554:     for i in range(len(s)):
 555:         # Odd length palindromes (center is single char)
 556:         odd_palindrome = expand_around_center(i, i)
 557:         # Even length palindromes (center is between chars)
 558:         even_palindrome = expand_around_center(i, i+1)
 559:         
 560:         longest = max(longest, odd_palindrome, even_palindrome, key=len)
 561:     
 562:     return longest
 563: 
 564: # Evaluator tests
 565: result = evaluate(longest_palindrome, test_cases)
 566: # Result: SUCCESS (2/2 tests passed)
 567: 
 568: # No reflection needed - task complete
 569: ```
 570: 
 571: ### üîß Implementation
 572: 
 573: ```python
 574: class ReflexionAgent:
 575:     """
 576:     Reflexion framework: Actor + Evaluator + Self-Reflection + Memory.
 577:     
 578:     Learns from failures across multiple trials.
 579:     """
 580:     
 581:     def __init__(self, llm, tools, evaluator_fn, max_trials=3):
 582:         """
 583:         Args:
 584:             llm: Language model
 585:             tools: Available tools (like ReAct)
 586:             evaluator_fn: Function to evaluate trajectory ‚Üí score/success
 587:             max_trials: Maximum attempts per task
 588:         """
 589:         self.llm = llm
 590:         self.tools = tools
 591:         self.evaluator = evaluator_fn
 592:         self.max_trials = max_trials
 593:         self.memory = []  # Episodic memory of reflections
 594:     
 595:     def solve(self, task):
 596:         """
 597:         Solve task with iterative self-improvement.
 598:         
 599:         Returns:
 600:             Best solution found across all trials
 601:         """
 602:         best_solution = None
 603:         best_score = -float('inf')
 604:         
 605:         for trial in range(self.max_trials):
 606:             print(f"\n=== Trial {trial + 1}/{self.max_trials} ===")
 607:             
 608:             # Actor: Execute task (ReAct-style)
 609:             trajectory = self._execute_trial(task)
 610:             
 611:             # Evaluator: Score the trajectory
 612:             eval_result = self.evaluator(trajectory)
 613:             score = eval_result['score']
 614:             success = eval_result['success']
 615:             
 616:             print(f"Score: {score}, Success: {success}")
 617:             
 618:             # Track best solution
 619:             if score > best_score:
 620:                 best_score = score
 621:                 best_solution = trajectory
 622:             
 623:             # If successful, return
 624:             if success:
 625:                 print("‚úì Task completed successfully")
 626:                 return {
 627:                     'solution': trajectory,
 628:                     'trial': trial + 1,
 629:                     'reflections': self.memory
 630:                 }
 631:             
 632:             # Self-Reflection: Analyze failure
 633:             if trial < self.max_trials - 1:  # Don't reflect on last trial
 634:                 reflection = self._generate_reflection(task, trajectory, eval_result)
 635:                 self.memory.append(reflection)
 636:                 print(f"Reflection generated: {reflection[:100]}...")
 637:         
 638:         # Max trials reached without success
 639:         print("‚úó Max trials reached")
 640:         return {
 641:             'solution': best_solution,
 642:             'trial': self.max_trials,
 643:             'reflections': self.memory,
 644:             'success': False
 645:         }
 646:     
 647:     def _execute_trial(self, task):
 648:         """
 649:         Execute one trial attempt using ReAct-style loop.
 650:         
 651:         Incorporates past reflections from memory.
 652:         """
 653:         # Build context with memory
 654:         memory_context = ""
 655:         if self.memory:
 656:             memory_context = "\nPAST ATTEMPTS AND REFLECTIONS:\n"
 657:             for i, reflection in enumerate(self.memory):
 658:                 memory_context += f"\nTrial {i+1} Reflection:\n{reflection}\n"
 659:         
 660:         system_prompt = f"""You are solving: {task}
 661: 
 662: {memory_context}
 663: 
 664: Use the THOUGHT-ACTION-OBSERVATION format.
 665: Learn from past reflections to avoid previous mistakes.
 666: """
 667:         
 668:         # Standard ReAct loop (simplified here)
 669:         context = system_prompt
 670:         trajectory = []
 671:         
 672:         for step in range(10):  # Max 10 steps per trial
 673:             response = self.llm.complete(context, temperature=0.0)
 674:             
 675:             # Parse and execute (similar to ReAct)
 676:             thought, action, action_input = self._parse(response)
 677:             
 678:             if action == 'FINISH':
 679:                 trajectory.append({
 680:                     'thought': thought,
 681:                     'action': action,
 682:                     'result': action_input
 683:                 })
 684:                 break
 685:             
 686:             # Execute tool
 687:             observation = self.tools[action](action_input) if action in self.tools else "Invalid tool"
 688:             
 689:             trajectory.append({
 690:                 'thought': thought,
 691:                 'action': action,
 692:                 'action_input': action_input,
 693:                 'observation': observation
 694:             })
 695:             
 696:             context += f"\nTHOUGHT: {thought}\nACTION: {action}[{action_input}]\nOBSERVATION: {observation}\n"
 697:         
 698:         return trajectory
 699:     
 700:     def _generate_reflection(self, task, trajectory, eval_result):
 701:         """
 702:         Generate verbal self-reflection on failed trajectory.
 703:         """
 704:         trajectory_text = self._format_trajectory(trajectory)
 705:         failure_details = eval_result.get('feedback', 'No specific feedback')
 706:         
 707:         reflection_prompt = f"""Analyze this failed attempt and provide a detailed reflection.
 708: 
 709: TASK: {task}
 710: 
 711: TRAJECTORY:
 712: {trajectory_text}
 713: 
 714: EVALUATION RESULT:
 715: - Success: {eval_result['success']}
 716: - Score: {eval_result['score']}
 717: - Feedback: {failure_details}
 718: 
 719: Provide a reflection covering:
 720: 1. What went wrong?
 721: 2. Why did this approach fail?
 722: 3. What key insight would improve the next attempt?
 723: 4. What specific mistake should be avoided?
 724: 
 725: REFLECTION:
 726: """
 727:         
 728:         reflection = self.llm.complete(reflection_prompt, temperature=0.3)
 729:         return reflection
 730:     
 731:     def _format_trajectory(self, trajectory):
 732:         """Format trajectory for display."""
 733:         lines = []
 734:         for i, step in enumerate(trajectory):
 735:             lines.append(f"Step {i+1}:")
 736:             lines.append(f"  Thought: {step.get('thought', '')}")
 737:             lines.append(f"  Action: {step.get('action', '')}[{step.get('action_input', '')}]")
 738:             if 'observation' in step:
 739:                 lines.append(f"  Observation: {step['observation']}")
 740:         return "\n".join(lines)
 741:     
 742:     def _parse(self, response):
 743:         """Parse LLM response (same as ReAct)."""
 744:         # Implementation same as ReAct._parse_response
 745:         pass
 746: ```
 747: 
 748: ### üí° When to Use Reflexion
 749: 
 750: **[Reflexion-Ideal-Use-Cases**:: (1) Complex coding tasks requiring iteration, (2) Games/puzzles where trial-and-error learning helps, (3) Tasks with clear success criteria and evaluable outcomes, (4) Scenarios where learning from failures provides compounding value, (5) Multi-trial workflows acceptable.]**
 751: 
 752: **‚úÖ Excellent For:**
 753: - **Code generation** with test-driven evaluation
 754: - **Interactive games** (text adventures, puzzles)
 755: - **Optimization tasks** (find best parameters)
 756: - **Creative tasks** with refinement cycles
 757: - **Any task where self-critique helps**
 758: 
 759: **‚ùå Not Suitable For:**
 760: - **One-shot queries** (no opportunity for retrial)
 761: - **Ambiguous success criteria** (can't evaluate objectively)
 762: - **Real-time requirements** (multiple trials too slow)
 763: - **Simple tasks** (overhead not worth it)
 764: 
 765: ### üìä Performance Benchmarks
 766: 
 767: **From Shinn et al. 2023 (NeurIPS)**:
 768: 
 769: | Task | ReAct Baseline | Reflexion | Improvement | Trials |
 770: |------|----------------|-----------|-------------|--------|
 771: | **AlfWorld** (Interactive) | 71% | 91% | **+20pp** | 3 trials |
 772: | **HotPotQA** (QA) | 35.1% | 40.2% | **+5.1pp** | 3 trials |
 773: | **HumanEval** (Coding) | 67% | 88% | **+21pp** | Up to 3 |
 774: 
 775: **[Reflexion-Learning-Curve**:: Performance improves monotonically with trials. Trial 1 ‚âà ReAct performance. Trial 2 shows moderate gains. Trial 3 achieves peak performance. Diminishing returns after 3-4 trials.]**
 776: 
 777: ### ‚öôÔ∏è Reflexion Variations
 778: 
 779: **Reflexion + External Memory**:
 780: ```python
 781: class ReflexionWithVectorMemory(ReflexionAgent):
 782:     """
 783:     Use vector database for reflection retrieval.
 784:     
 785:     Instead of using all past reflections, retrieve most relevant ones.
 786:     """
 787:     
 788:     def __init__(self, llm, tools, evaluator_fn, embedding_model):
 789:         super().__init__(llm, tools, evaluator_fn)
 790:         self.embedding_model = embedding_model
 791:         self.reflection_db = []  # (embedding, reflection) pairs
 792:     
 793:     def _get_relevant_reflections(self, task, top_k=3):
 794:         """Retrieve top-k most similar past reflections."""
 795:         if not self.reflection_db:
 796:             return []
 797:         
 798:         task_embedding = self.embedding_model.encode(task)
 799:         
 800:         # Compute similarities
 801:         similarities = []
 802:         for emb, refl in self.reflection_db:
 803:             sim = cosine_similarity(task_embedding, emb)
 804:             similarities.append((sim, refl))
 805:         
 806:         # Return top-k
 807:         similarities.sort(reverse=True)
 808:         return [refl for _, refl in similarities[:top_k]]
 809:     
 810:     def _execute_trial(self, task):
 811:         """Use only relevant past reflections."""
 812:         relevant_refs = self._get_relevant_reflections(task)
 813:         
 814:         memory_context = ""
 815:         if relevant_refs:
 816:             memory_context = "\nRELEVANT PAST REFLECTIONS:\n"
 817:             for i, ref in enumerate(relevant_refs):
 818:                 memory_context += f"\n{i+1}. {ref}\n"
 819:         
 820:         # Rest same as base class, but with filtered context
 821:         # ...
 822: ```
 823: 
 824: ---
 825: 
 826: ## ART (Automatic Reasoning & Tool-use)
 827: 
 828: [**ART**:: "Automatic multi-step Reasoning and Tool-use" - framework with decomposable task library and tool library, enabling zero-shot generalization to new tasks via automatic selection of relevant demonstrations and tools.]
 829: 
 830: ### üéØ Core Concept
 831: 
 832: **[ART-Architecture**:: Maintains (1) Task Library - few-shot demonstrations of multi-step reasoning for different task types, (2) Tool Library - executable functions with descriptions, (3) Automatic Selection - given new task, retrieves similar demonstrations and relevant tools, constructs prompt automatically.]**
 833: 
 834: **Problem**: ReAct/Reflexion require manual prompt engineering for each task type. Tools must be specified upfront.
 835: 
 836: **Solution**: Build libraries that enable zero-shot generalization via automatic retrieval.
 837: 
 838: ### üî¨ How It Works
 839: 
 840: **Three-Stage Process** (Paranjape et al. 2023):
 841: 
 842: **Stage 1: Task Decomposition**
 843: ```python
 844: # Given new task, find similar task in library
 845: new_task = "Solve this math word problem: ..."
 846: 
 847: # Retrieve similar demonstrations
 848: similar_tasks = task_library.search(new_task, k=2)
 849: # Returns: [arithmetic_word_problem_demo, multi_step_math_demo]
 850: ```
 851: 
 852: **Stage 2: Tool Selection**
 853: ```python
 854: # Based on task type, select relevant tools
 855: relevant_tools = tool_library.select_for_task(similar_tasks)
 856: # Returns: [Calculator, UnitConverter, SearchEngine]
 857: ```
 858: 
 859: **Stage 3: Prompt Construction**
 860: ```python
 861: # Automatically construct prompt with demonstrations + tools
 862: prompt = construct_prompt(
 863:     demonstrations=similar_tasks,
 864:     tools=relevant_tools,
 865:     new_task=new_task
 866: )
 867: 
 868: # Execute with ReAct-style loop
 869: result = execute(prompt)
 870: ```
 871: 
 872: ### üìù Task Library Structure
 873: 
 874: ```python
 875: task_library = {
 876:     'arithmetic_word_problem': {
 877:         'demonstration': """
 878: Question: A bakery makes 12 cakes per hour. How many cakes in 3.5 hours?
 879: 
 880: THOUGHT: I need to multiply 12 cakes/hour by 3.5 hours.
 881: ACTION: Calculator[12 * 3.5]
 882: OBSERVATION: 42
 883: 
 884: THOUGHT: The bakery makes 42 cakes in 3.5 hours.
 885: ACTION: FINISH[42 cakes]
 886: """,
 887:         'required_tools': ['Calculator'],
 888:         'task_type': 'math'
 889:     },
 890:     
 891:     'multi_hop_qa': {
 892:         'demonstration': """
 893: Question: What is the elevation of the highest peak in the country where the Eiffel Tower is located?
 894: 
 895: THOUGHT: First, I need to find which country has the Eiffel Tower.
 896: ACTION: Search[Where is Eiffel Tower located]
 897: OBSERVATION: The Eiffel Tower is in Paris, France.
 898: 
 899: THOUGHT: Now I need to find France's highest peak.
 900: ACTION: Search[highest peak in France]
 901: OBSERVATION: Mont Blanc is France's highest peak.
 902: 
 903: THOUGHT: Finally, I need the elevation of Mont Blanc.
 904: ACTION: Search[Mont Blanc elevation]
 905: OBSERVATION: Mont Blanc has an elevation of 4,808 meters.
 906: 
 907: THOUGHT: I have all the information needed.
 908: ACTION: FINISH[4,808 meters]
 909: """,
 910:         'required_tools': ['Search'],
 911:         'task_type': 'multi_hop_reasoning'
 912:     },
 913:     
 914:     # ... more task types
 915: }
 916: ```
 917: 
 918: ### üîß Implementation
 919: 
 920: ```python
 921: class ARTFramework:
 922:     """
 923:     ART: Automatic Reasoning and Tool-use.
 924:     
 925:     Combines task library (demonstrations) with tool library
 926:     for zero-shot generalization to new tasks.
 927:     """
 928:     
 929:     def __init__(self, llm, task_library, tool_library, embedding_model):
 930:         """
 931:         Args:
 932:             llm: Language model
 933:             task_library: Dict of {task_type: demonstration}
 934:             tool_library: Dict of {tool_name: tool_function}
 935:             embedding_model: For semantic similarity search
 936:         """
 937:         self.llm = llm
 938:         self.task_library = task_library
 939:         self.tool_library = tool_library
 940:         self.embedder = embedding_model
 941:         
 942:         # Pre-compute embeddings for task library
 943:         self.task_embeddings = {}
 944:         for task_type, data in task_library.items():
 945:             demo_text = data['demonstration']
 946:             self.task_embeddings[task_type] = self.embedder.encode(demo_text)
 947:     
 948:     def solve(self, new_task, k_demonstrations=2):
 949:         """
 950:         Solve new task using automatic retrieval.
 951:         
 952:         Args:
 953:             new_task: User's question
 954:             k_demonstrations: Number of similar demonstrations to use
 955:         
 956:         Returns:
 957:             Solution using automatically constructed prompt
 958:         """
 959:         # Step 1: Retrieve similar task demonstrations
 960:         similar_tasks = self._retrieve_similar_tasks(new_task, k=k_demonstrations)
 961:         
 962:         # Step 2: Determine required tools
 963:         required_tools = self._get_required_tools(similar_tasks)
 964:         
 965:         # Step 3: Construct prompt automatically
 966:         prompt = self._construct_prompt(
 967:             demonstrations=similar_tasks,
 968:             tools=required_tools,
 969:             new_task=new_task
 970:         )
 971:         
 972:         # Step 4: Execute with ReAct loop
 973:         result = self._execute_react_loop(prompt, required_tools)
 974:         
 975:         return result
 976:     
 977:     def _retrieve_similar_tasks(self, query, k=2):
 978:         """Find k most similar task demonstrations."""
 979:         query_embedding = self.embedder.encode(query)
 980:         
 981:         similarities = []
 982:         for task_type, task_emb in self.task_embeddings.items():
 983:             sim = cosine_similarity(query_embedding, task_emb)
 984:             similarities.append((sim, task_type))
 985:         
 986:         similarities.sort(reverse=True)
 987:         
 988:         # Return top-k task data
 989:         top_tasks = []
 990:         for _, task_type in similarities[:k]:
 991:             top_tasks.append(self.task_library[task_type])
 992:         
 993:         return top_tasks
 994:     
 995:     def _get_required_tools(self, task_demonstrations):
 996:         """Extract union of required tools from demonstrations."""
 997:         all_tools = set()
 998:         for demo_data in task_demonstrations:
 999:             all_tools.update(demo_data['required_tools'])
1000:         
1001:         # Return actual tool functions
1002:         return {
1003:             tool_name: self.tool_library[tool_name]
1004:             for tool_name in all_tools
1005:             if tool_name in self.tool_library
1006:         }
1007:     
1008:     def _construct_prompt(self, demonstrations, tools, new_task):
1009:         """Build prompt from retrieved components."""
1010:         # Tool descriptions
1011:         tool_desc = "\n".join(
1012:             f"- {name}: {func.__doc__}" 
1013:             for name, func in tools.items()
1014:         )
1015:         
1016:         # Demonstration examples
1017:         demo_text = "\n\n".join(
1018:             demo['demonstration'] 
1019:             for demo in demonstrations
1020:         )
1021:         
1022:         prompt = f"""You can use these tools:
1023: {tool_desc}
1024: 
1025: Here are examples of similar tasks:
1026: 
1027: {demo_text}
1028: 
1029: Now solve this task:
1030: {new_task}
1031: 
1032: Follow the THOUGHT-ACTION-OBSERVATION format shown in examples.
1033: """
1034:         
1035:         return prompt
1036:     
1037:     def _execute_react_loop(self, prompt, tools):
1038:         """Execute ReAct-style loop with prompt and tools."""
1039:         # Standard ReAct execution (same as before)
1040:         context = prompt
1041:         max_steps = 10
1042:         
1043:         for step in range(max_steps):
1044:             response = self.llm.complete(context, temperature=0.0)
1045:             
1046:             thought, action, action_input = self._parse(response)
1047:             
1048:             if action == 'FINISH':
1049:                 return action_input
1050:             
1051:             if action in tools:
1052:                 observation = tools[action](action_input)
1053:             else:
1054:                 observation = f"Tool {action} not available"
1055:             
1056:             context += f"\nTHOUGHT: {thought}\nACTION: {action}[{action_input}]\nOBSERVATION: {observation}\n"
1057:         
1058:         return "Max steps reached"
1059:     
1060:     def _parse(self, response):
1061:         """Parse response (same as ReAct)."""
1062:         # Implementation identical to ReAct
1063:         pass
1064: ```
1065: 
1066: ### üí° When to Use ART
1067: 
1068: **‚úÖ Excellent For:**
1069: - **Zero-shot task adaptation** (new task types without manual prompting)
1070: - **Large tool libraries** (automatically select relevant subset)
1071: - **Production systems** (reuse demonstrations across users)
1072: - **Scaling to many tasks** (add to library, don't reprogram)
1073: 
1074: **‚ùå Not Suitable For:**
1075: - **Completely novel tasks** (no similar demonstrations exist)
1076: - **Simple applications** (overhead of library management)
1077: - **Limited task diversity** (manual ReAct more direct)
1078: 
1079: ### üìä Performance
1080: 
1081: **From Paranjape et al. 2023**:
1082: 
1083: | Task Type | Manual ReAct | ART (Auto) | Comparison |
1084: |-----------|--------------|------------|------------|
1085: | **BigBench Tasks** | 68% | 78% | +10pp via better demonstrations |
1086: | **Tool Selection Accuracy** | Manual | 92% auto | Near-human performance |
1087: 
1088: ---
1089: 
1090: ## ReWOO (Reasoning Without Observation)
1091: 
1092: [**ReWOO**:: "Reasoning WithOut Observation" - decouples planning from execution through three modules (Planner, Worker, Solver), reducing token usage by generating complete plan upfront then executing all tools in parallel before final solving.]**
1093: 
1094: ### üéØ Core Concept
1095: 
1096: **[ReWOO-Efficiency-Innovation**:: ReAct interleaves reasoning and tool calls, requiring LLM invocation after each observation (high token cost). ReWOO separates into phases - (1) Plan all actions upfront, (2) Execute tools in parallel, (3) Solve with all results available - reducing LLM calls from O(n) to O(1) for n tools.]**
1097: 
1098: **ReAct Pattern** (Sequential, High Token Cost):
1099: ```
1100: LLM ‚Üí Tool1 ‚Üí LLM ‚Üí Tool2 ‚Üí LLM ‚Üí Tool3 ‚Üí LLM ‚Üí Answer
1101:  ‚Üë_______‚Üì     ‚Üë_____‚Üì      ‚Üë_____‚Üì      ‚Üë
1102: (4 LLM calls, sequential execution)
1103: ```
1104: 
1105: **ReWOO Pattern** (Parallel, Low Token Cost):
1106: ```
1107: LLM (Plan) ‚Üí [Tool1, Tool2, Tool3] (Parallel) ‚Üí LLM (Solve) ‚Üí Answer
1108:     ‚Üë                                                ‚Üë
1109: (2 LLM calls, parallel execution)
1110: ```
1111: 
1112: ### üî¨ Three-Module Architecture
1113: 
1114: **Module 1: Planner** - Generates complete plan with variable placeholders
1115: **Module 2: Worker** - Executes all tool calls (can be parallel)
1116: **Module 3: Solver** - Generates final answer given all evidence
1117: 
1118: ### üìù Complete Example
1119: 
1120: **Question**: "What is the hometown of the 2023 Nobel Prize in Literature winner?"
1121: 
1122: **Phase 1: Planner**
1123: ```
1124: PLANNER OUTPUT:
1125: #E1 = Search[2023 Nobel Prize Literature winner]
1126: #E2 = LookUp[#E1, hometown]
1127: Answer: #E2
1128: ```
1129: 
1130: **Phase 2: Worker** (Parallel Execution)
1131: ```
1132: #E1 = Search[2023 Nobel Prize Literature winner]
1133:      ‚Üí "Jon Fosse from Norway won 2023 Nobel Literature"
1134: 
1135: #E2 = LookUp["Jon Fosse from Norway won 2023 Nobel Literature", hometown]
1136:      ‚Üí "Jon Fosse's hometown is Haugesund, Norway"
1137: ```
1138: 
1139: **Phase 3: Solver**
1140: ```
1141: SOLVER INPUT:
1142: Question: What is the hometown of the 2023 Nobel Prize in Literature winner?
1143: Evidence:
1144: #E1: Jon Fosse from Norway won 2023 Nobel Literature
1145: #E2: Jon Fosse's hometown is Haugesund, Norway
1146: 
1147: SOLVER OUTPUT:
1148: The hometown of the 2023 Nobel Prize in Literature winner (Jon Fosse) is Haugesund, Norway.
1149: ```
1150: 
1151: ### üîß Implementation
1152: 
1153: ```python
1154: class ReWOOFramework:
1155:     """
1156:     ReWOO: Reasoning Without Observation.
1157:     
1158:     Three modules: Planner ‚Üí Worker ‚Üí Solver
1159:     More token-efficient than ReAct for multi-tool tasks.
1160:     """
1161:     
1162:     def __init__(self, llm, tools):
1163:         self.llm = llm
1164:         self.tools = tools
1165:     
1166:     def solve(self, question):
1167:         """
1168:         Execute ReWOO pipeline.
1169:         
1170:         Returns:
1171:             Final answer with execution details
1172:         """
1173:         # Phase 1: Planning
1174:         plan = self._plan(question)
1175:         
1176:         # Phase 2: Worker execution
1177:         evidence = self._execute_plan(plan)
1178:         
1179:         # Phase 3: Solving
1180:         answer = self._solve(question, evidence)
1181:         
1182:         return {
1183:             'answer': answer,
1184:             'plan': plan,
1185:             'evidence': evidence
1186:         }
1187:     
1188:     def _plan(self, question):
1189:         """
1190:         Generate complete plan with variable placeholders.
1191:         
1192:         Returns:
1193:             List of (variable, tool, input) tuples
1194:         """
1195:         tool_desc = "\n".join(f"- {name}" for name in self.tools.keys())
1196:         
1197:         planner_prompt = f"""Generate a plan to answer the question.
1198: 
1199: Available tools:
1200: {tool_desc}
1201: 
1202: Use variables #E1, #E2, etc. to reference evidence from previous steps.
1203: 
1204: Format:
1205: #E1 = ToolName[input]
1206: #E2 = ToolName[#E1]  # Can reference previous evidence
1207: ...
1208: Answer: #EN
1209: 
1210: Question: {question}
1211: 
1212: Plan:
1213: """
1214:         
1215:         response = self.llm.complete(planner_prompt, temperature=0.0)
1216:         plan = self._parse_plan(response)
1217:         
1218:         return plan
1219:     
1220:     def _parse_plan(self, plan_text):
1221:         """Parse plan into structured steps."""
1222:         import re
1223:         
1224:         steps = []
1225:         lines = plan_text.strip().split('\n')
1226:         
1227:         for line in lines:
1228:             # Match pattern: #E1 = Tool[input]
1229:             match = re.match(r'#E(\d+)\s*=\s*(\w+)\[(.*?)\]', line)
1230:             if match:
1231:                 var_num = int(match.group(1))
1232:                 tool = match.group(2)
1233:                 tool_input = match.group(3)
1234:                 steps.append({
1235:                     'variable': f'#E{var_num}',
1236:                     'tool': tool,
1237:                     'input': tool_input
1238:                 })
1239:         
1240:         return steps
1241:     
1242:     def _execute_plan(self, plan):
1243:         """
1244:         Execute all plan steps (can be parallelized).
1245:         
1246:         Returns:
1247:             Dict mapping variables to evidence
1248:         """
1249:         evidence = {}
1250:         
1251:         for step in plan:
1252:             var = step['variable']
1253:             tool_name = step['tool']
1254:             tool_input = step['input']
1255:             
1256:             # Substitute previous evidence variables
1257:             for prev_var, prev_evidence in evidence.items():
1258:                 tool_input = tool_input.replace(prev_var, str(prev_evidence))
1259:             
1260:             # Execute tool
1261:             if tool_name in self.tools:
1262:                 result = self.tools[tool_name](tool_input)
1263:                 evidence[var] = result
1264:             else:
1265:                 evidence[var] = f"Tool {tool_name} not found"
1266:         
1267:         return evidence
1268:     
1269:     def _solve(self, question, evidence):
1270:         """
1271:         Generate final answer given question and all evidence.
1272:         """
1273:         evidence_text = "\n".join(
1274:             f"{var}: {value}"
1275:             for var, value in evidence.items()
1276:         )
1277:         
1278:         solver_prompt = f"""Answer the question using the provided evidence.
1279: 
1280: Question: {question}
1281: 
1282: Evidence:
1283: {evidence_text}
1284: 
1285: Answer:
1286: """
1287:         
1288:         answer = self.llm.complete(solver_prompt, temperature=0.0)
1289:         return answer
1290: ```
1291: 
1292: ### üí° When to Use ReWOO
1293: 
1294: **[ReWOO-vs-ReAct-Tradeoff**:: ReWOO is faster and cheaper when plan is deterministic and parallelizable. ReAct is better when observations must inform next actions (dynamic planning). Choose based on task dependency structure.]**
1295: 
1296: **‚úÖ Use ReWOO For:**
1297: - **Multi-step lookup** (independent information retrieval)
1298: - **Token-budget constraints** (ReWOO uses fewer tokens)
1299: - **Parallelizable tools** (can execute simultaneously)
1300: - **Production cost optimization**
1301: 
1302: **‚ùå Use ReAct Instead For:**
1303: - **Dynamic planning** (next action depends on observation)
1304: - **Interactive environments** (game states, simulations)
1305: - **Error recovery** (may need to retry/adjust)
1306: 
1307: ### üìä Performance Comparison
1308: 
1309: **Token Efficiency** (from Xu et al. 2023):
1310: 
1311: | Task Type | ReAct Tokens | ReWOO Tokens | Savings |
1312: |-----------|--------------|--------------|---------|
1313: | **3-step lookup** | ~2400 | ~1200 | **50%** |
1314: | **5-step research** | ~4500 | ~1800 | **60%** |
1315: 
1316: **Accuracy** (similar to ReAct when tasks are parallelizable):
1317: 
1318: | Task | ReAct | ReWOO | Note |
1319: |------|-------|-------|------|
1320: | **Multi-hop QA** | 35% | 34% | Comparable |
1321: | **HotpotQA** | 27% | 26% | Slight decrease acceptable for cost savings |
1322: 
1323: ---
1324: 
1325: ## Technique Comparison Matrix
1326: 
1327: ### **Selection Decision Tree**
1328: 
1329: ```
1330: Does task require LEARNING from mistakes?
1331: ‚îú‚îÄ YES ‚Üí Reflexion
1332: ‚îÇ  ‚îî‚îÄ Trials: 3-5 attempts
1333: ‚îÇ
1334: ‚îî‚îÄ NO ‚Üí Continue below
1335: 
1336: Does task involve MANY sequential tool calls?
1337: ‚îú‚îÄ YES, and observations inform next actions
1338: ‚îÇ  ‚îî‚îÄ ReAct (dynamic planning)
1339: ‚îÇ
1340: ‚îî‚îÄ YES, but tools can run in parallel
1341:    ‚îî‚îÄ ReWOO (efficiency)
1342: 
1343: Is this a RECURRING task type?
1344: ‚îú‚îÄ YES, with library of demonstrations
1345: ‚îÇ  ‚îî‚îÄ ART (zero-shot generalization)
1346: ‚îÇ
1347: ‚îî‚îÄ NO ‚Üí ReAct (general purpose)
1348: ```
1349: 
1350: ### **Feature Comparison**
1351: 
1352: | Feature | ReAct | Reflexion | ART | ReWOO |
1353: |---------|-------|-----------|-----|-------|
1354: | **Learning** | ‚ùå | ‚úÖ Episodic | ‚ùå | ‚ùå |
1355: | **Memory** | Session | Persistent | Library | None |
1356: | **Parallel Execution** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
1357: | **Token Efficiency** | Medium | Low | Medium | High |
1358: | **Dynamic Planning** | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå |
1359: | **Self-Improvement** | ‚ùå | ‚úÖ | ‚ùå | ‚ùå |
1360: | **Zero-Shot Adaptation** | ‚ùå | ‚ùå | ‚úÖ | ‚ùå |
1361: | **Implementation Complexity** | Medium | High | High | Medium |
1362: 
1363: ---
1364: 
1365: ## Integration Patterns
1366: 
1367: ### Pattern 1: ReAct + Reflexion Hybrid
1368: 
1369: ```python
1370: def react_reflexion_hybrid(task, max_trials=3):
1371:     """
1372:     Use ReAct for execution, Reflexion for learning.
1373:     
1374:     Best of both: ReAct's dynamic planning + Reflexion's learning
1375:     """
1376:     reflexion = ReflexionAgent(llm, tools, evaluator)
1377:     
1378:     # Each trial uses ReAct execution
1379:     result = reflexion.solve(task)
1380:     
1381:     return result
1382: ```
1383: 
1384: ### Pattern 2: ART + ReWOO for Efficiency
1385: 
1386: ```python
1387: def art_rewoo_pipeline(new_task):
1388:     """
1389:     Use ART for zero-shot demonstration retrieval,
1390:     ReWOO for efficient execution.
1391:     """
1392:     # ART: Select demonstrations and tools
1393:     art = ARTFramework(llm, task_library, tool_library, embedder)
1394:     similar_demos = art._retrieve_similar_tasks(new_task)
1395:     tools = art._get_required_tools(similar_demos)
1396:     
1397:     # ReWOO: Efficient execution
1398:     rewoo = ReWOOFramework(llm, tools)
1399:     result = rewoo.solve(new_task)
1400:     
1401:     return result
1402: ```
1403: 
1404: ---
1405: 
1406: ## Research References
1407: 
1408: ### Core Papers
1409: 
1410: - **[Yao et al. 2022](https://arxiv.org/abs/2210.03629)** - "ReAct: Synergizing Reasoning and Acting in Language Models" - ICLR 2023
1411: - **[Shinn et al. 2023](https://arxiv.org/abs/2303.11366)** - "Reflexion: Language Agents with Verbal Reinforcement Learning" - NeurIPS 2023
1412: - **[Paranjape et al. 2023](https://arxiv.org/abs/2303.09014)** - "ART: Automatic Multi-step Reasoning and Tool-use for Large Language Models"
1413: - **[Xu et al. 2023](https://arxiv.org/abs/2305.18323)** - "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models"
1414: 
1415: ---
1416: 
1417: ## üîó Related Topics for PKB Expansion
1418: 
1419: 1. **[[tool-library-design-for-agents]]**
1420:    - **Connection**: Designing effective tool APIs for agent frameworks
1421:    - **Depth Potential**: Tool interface patterns, error handling, composition
1422:    - **Knowledge Graph Role**: Practical implementation guide
1423:    - **Priority**: High - essential for production agents
1424: 
1425: 2. **[[agent-evaluation-metrics]]**
1426:    - **Connection**: How to measure agent performance objectively
1427:    - **Depth Potential**: Success rates, tool selection accuracy, efficiency metrics
1428:    - **Knowledge Graph Role**: Quality assurance methodology
1429:    - **Priority**: High - needed for iterative improvement
1430: 
1431: 3. **[[agent-safety-sandboxing]]**
1432:    - **Connection**: Preventing agents from harmful actions
1433:    - **Depth Potential**: Code execution safety, API rate limiting, action validation
1434:    - **Knowledge Graph Role**: Production deployment requirements
1435:    - **Priority**: Critical - safety cannot be optional
1436: 
1437: 4. **[[multi-agent-collaboration]]**
1438:    - **Connection**: Multiple agents working together on complex tasks
1439:    - **Depth Potential**: Communication protocols, task delegation, consensus
1440:    - **Knowledge Graph Role**: Advanced agent architectures
1441:    - **Priority**: Medium - frontier topic
1442: 
1443: ---
1444: 
1445: *This guide covers agentic frameworks enabling autonomous behavior. For reasoning techniques, see [[01-reasoning-techniques-guide]]. For meta-optimization, see [[03-meta-optimization-guide]].*
</file>

<file path="__LOCAL-REPO/__exemplar/__import/__master-exemplar/03-meta-optimization-guide.md">
   1: ---
   2: tags: #prompt-engineering #meta-optimization #ape #opro #prompt-breeding #automatic-improvement #reference
   3: aliases: [Meta-Optimization, Automatic Prompt Engineering, Prompt Optimization, Self-Improving Prompts]
   4: status: evergreen
   5: certainty: verified
   6: priority: high
   7: created: 2025-12-25
   8: modified: 2025-12-25
   9: type: reference
  10: version: 1.0.0
  11: source: claude-sonnet-4.5
  12: category: meta-optimization
  13: ---
  14: 
  15: # Meta-Optimization Guide
  16: 
  17: > [!abstract] Purpose
  18: > Comprehensive guide to techniques that automatically improve prompts without manual iteration - using LLMs to optimize prompts, evolutionary algorithms for breeding better variants, reinforcement learning for refinement, and structural abstraction for generalization. Based on cutting-edge research from 2023-2025.
  19: 
  20: ---
  21: 
  22: ## üìã Table of Contents
  23: 
  24: 1. [[#Overview & Comparison]]
  25: 2. [[#APE: Automatic Prompt Engineer]]
  26: 3. [[#OPRO: Optimization by Prompting]]
  27: 4. [[#Active-Prompt]]
  28: 5. [[#PromptBreeder]]
  29: 6. [[#RPO: Reinforced Prompt Optimization]]
  30: 7. [[#Meta-Prompting]]
  31: 8. [[#Technique Selection Guide]]
  32: 9. [[#Research References]]
  33: 
  34: ---
  35: 
  36: ## Overview & Comparison
  37: 
  38: [**Meta-Optimization**:: Automated techniques that improve prompts without manual iteration by using LLMs as optimizers, evolutionary algorithms for breeding variants, reinforcement learning for refinement, or structural abstraction for generalization - transforming prompt engineering from craft to systematic optimization.]
  39: 
  40: ### **Why Meta-Optimization Matters**
  41: 
  42: **The Problem**: Traditional prompt engineering is:
  43: - **Manual**: Requires expert time for iteration
  44: - **Inconsistent**: Quality varies by engineer skill
  45: - **Slow**: Multiple rounds of testing and refinement
  46: - **Local**: Optimizes for observed cases, may miss better solutions
  47: - **Expensive**: Human time costs more than compute
  48: 
  49: **[Meta-Optimization-Value**:: Automates the prompt improvement cycle - generate candidates automatically, evaluate systematically, select best performers, iterate rapidly. Trades human time for compute time. Enables optimization at scale impossible manually.]**
  50: 
  51: ### **Evolution of Meta-Optimization**
  52: 
  53: ```mermaid
  54: graph LR
  55:     A[Manual Iteration<br/>Human crafts prompts] --> B[APE<br/>LLM generates candidates]
  56:     B --> C[OPRO<br/>LLM optimizes iteratively]
  57:     C --> D[PromptBreeder<br/>Evolutionary self-improvement]
  58:     B --> E[Active-Prompt<br/>Uncertainty-based selection]
  59:     C --> F[RPO<br/>+ Reinforcement learning]
  60:     A --> G[Meta-Prompting<br/>Structural abstraction]
  61: ```
  62: 
  63: ### **Comparison Matrix**
  64: 
  65: | Technique | Approach | Iterations | Complexity | Best For |
  66: |-----------|----------|------------|------------|----------|
  67: | **APE** | Generate + score + select | Single round | Low | Quick optimization |
  68: | **OPRO** | Iterative LLM-as-optimizer | 5-20 rounds | Medium | Systematic improvement |
  69: | **Active-Prompt** | Uncertainty-based example selection | 1-3 rounds | Low | Few-shot optimization |
  70: | **PromptBreeder** | Evolutionary breeding | 50-100 generations | High | Maximum quality |
  71: | **RPO** | Reinforcement learning | 10-50 episodes | Very High | Fine-grained tuning |
  72: | **Meta-Prompting** | Structural templates | N/A | Low | Zero-shot transfer |
  73: 
  74: ### **Performance Summary**
  75: 
  76: | Technique | GSM8K (Math) | BBH (Reasoning) | Typical Improvement |
  77: |-----------|--------------|-----------------|---------------------|
  78: | **Manual Baseline** | 65% | 55% | - |
  79: | **APE** | 78% (+13pp) | 63% (+8pp) | +8-13pp |
  80: | **OPRO** | 82% (+17pp) | 68% (+13pp) | +10-17pp |
  81: | **PromptBreeder** | 85% (+20pp) | 71% (+16pp) | +15-20pp |
  82: | **Active-Prompt** | 73% (+8pp) | 60% (+5pp) | +5-8pp (less compute) |
  83: 
  84: ---
  85: 
  86: ## APE: Automatic Prompt Engineer
  87: 
  88: [**APE-Framework**:: Uses LLM to automatically generate diverse prompt candidates, evaluates each on training set, selects best performer - achieving human-level prompt engineering performance without manual iteration.]
  89: 
  90: ### üéØ Core Concept
  91: 
  92: **[APE-Innovation**:: Instead of human engineer iterating prompts manually, use LLM to generate many candidates automatically, score each on held-out examples, select top performer. LLM acts as prompt engineer.]**
  93: 
  94: **Traditional Process**:
  95: ```
  96: Human: Writes prompt
  97: ‚Üí Tests on examples  
  98: ‚Üí Identifies issues
  99: ‚Üí Rewrites prompt
 100: ‚Üí Repeat...
 101: (10-20 iterations, hours/days)
 102: ```
 103: 
 104: **APE Process**:
 105: ```
 106: LLM: Generates 50 prompt candidates
 107: ‚Üí Automated scoring on test set
 108: ‚Üí Select best performer
 109: (Minutes, fully automated)
 110: ```
 111: 
 112: ### üî¨ How It Works
 113: 
 114: **[APE-Three-Steps**:: (1) Generation - LLM creates diverse prompt candidates from task description and examples, (2) Evaluation - each candidate scored on validation set, (3) Selection - highest-scoring prompt returned as optimal.]**
 115: 
 116: #### Step 1: Prompt Generation
 117: 
 118: **Inputs**:
 119: - Task description: "Classify sentiment as Positive/Negative/Neutral"
 120: - Few training examples: [(input, output), ...]
 121: 
 122: **Generation Prompt**:
 123: ```markdown
 124: I need a prompt for an AI to perform this task:
 125: 
 126: Task: {task_description}
 127: 
 128: Examples of input-output pairs:
 129: {examples}
 130: 
 131: Generate {num_candidates} different prompts that would make an AI perform this task well.
 132: Each prompt should:
 133: - Clearly specify the task
 134: - Provide helpful context or instructions
 135: - Encourage accurate outputs
 136: 
 137: Prompts:
 138: 1. [First candidate]
 139: 2. [Second candidate]
 140: ...
 141: ```
 142: 
 143: **LLM Generates** (example output):
 144: ```
 145: 1. "Analyze the sentiment of the following text and classify it as Positive, Negative, or Neutral. Consider both explicit and implicit sentiment cues."
 146: 
 147: 2. "You are a sentiment analysis expert. Classify the emotional tone of this text into one of three categories: Positive (optimistic, happy), Negative (critical, sad), or Neutral (factual, balanced)."
 148: 
 149: 3. "Determine whether the following statement expresses a positive opinion, negative opinion, or neutral stance. Respond with a single word: Positive, Negative, or Neutral."
 150: 
 151: ... (47 more candidates)
 152: ```
 153: 
 154: #### Step 2: Evaluation
 155: 
 156: Each generated prompt is scored on validation set:
 157: 
 158: ```python
 159: def evaluate_prompt(prompt, validation_set):
 160:     """
 161:     Score prompt on validation examples.
 162:     
 163:     Returns accuracy on validation set.
 164:     """
 165:     correct = 0
 166:     
 167:     for example in validation_set:
 168:         # Format with prompt
 169:         full_prompt = prompt + "\n\n" + example['input']
 170:         
 171:         # Get model prediction
 172:         prediction = llm.complete(full_prompt)
 173:         
 174:         # Check if correct
 175:         if prediction.strip().lower() == example['expected'].strip().lower():
 176:             correct += 1
 177:     
 178:     return correct / len(validation_set)
 179: 
 180: 
 181: # Score all candidates
 182: scores = []
 183: for candidate in generated_prompts:
 184:     score = evaluate_prompt(candidate, validation_set)
 185:     scores.append((score, candidate))
 186: ```
 187: 
 188: #### Step 3: Selection
 189: 
 190: ```python
 191: # Sort by score, select best
 192: scores.sort(reverse=True)
 193: best_prompt, best_score = scores[0]
 194: 
 195: print(f"Best Prompt (Accuracy: {best_score:.1%}):")
 196: print(best_prompt)
 197: ```
 198: 
 199: ### üìù Complete Example: Math Word Problems
 200: 
 201: **Task**: Solve grade-school math problems
 202: 
 203: **Training Examples**:
 204: ```
 205: Input: "If John has 5 apples and gives 2 to Mary, how many does he have?"
 206: Output: "3"
 207: 
 208: Input: "A car travels 60 mph for 2 hours. How far does it go?"
 209: Output: "120 miles"
 210: ```
 211: 
 212: **APE Process**:
 213: 
 214: ```python
 215: # Step 1: Generate candidates
 216: generation_prompt = """
 217: Generate 20 different prompts for solving math word problems.
 218: 
 219: Examples:
 220: - Input: "If John has 5 apples and gives 2 to Mary, how many does he have?"
 221:   Output: "3"
 222: 
 223: - Input: "A car travels 60 mph for 2 hours. How far does it go?"
 224:   Output: "120 miles"
 225: 
 226: Each prompt should help an AI solve similar problems accurately.
 227: 
 228: Prompts:
 229: """
 230: 
 231: candidates = llm.complete(generation_prompt, n=1, max_tokens=2000)
 232: 
 233: # Parse candidates
 234: prompts = parse_numbered_list(candidates)  # Extract 1-20
 235: 
 236: # Step 2: Evaluate
 237: validation_set = [
 238:     {'input': 'Sarah has 12 cookies and eats 3. How many left?', 'expected': '9'},
 239:     {'input': 'A train travels 90 mph for 3 hours. Distance?', 'expected': '270 miles'},
 240:     # ... more validation examples
 241: ]
 242: 
 243: results = []
 244: for prompt in prompts:
 245:     accuracy = evaluate_prompt(prompt, validation_set)
 246:     results.append({'prompt': prompt, 'accuracy': accuracy})
 247: 
 248: # Step 3: Select best
 249: best = max(results, key=lambda x: x['accuracy'])
 250: 
 251: print(f"Optimal Prompt ({best['accuracy']:.1%} accuracy):")
 252: print(best['prompt'])
 253: ```
 254: 
 255: **Output Example**:
 256: ```
 257: Optimal Prompt (87% accuracy):
 258: "Solve this math problem step by step. First identify the quantities, then determine the operation needed, calculate the answer, and include units if applicable."
 259: ```
 260: 
 261: ### üîß Production APE Implementation
 262: 
 263: ```python
 264: class AutomaticPromptEngineer:
 265:     """
 266:     APE framework for automatic prompt optimization.
 267:     """
 268:     
 269:     def __init__(self, llm, num_candidates=20):
 270:         self.llm = llm
 271:         self.num_candidates = num_candidates
 272:     
 273:     def optimize(self, task_description, train_examples, validation_examples):
 274:         """
 275:         Automatically engineer optimal prompt.
 276:         
 277:         Args:
 278:             task_description: What the task is
 279:             train_examples: Examples for generation (input-output pairs)
 280:             validation_examples: Examples for evaluation
 281:         
 282:         Returns:
 283:             {
 284:                 'best_prompt': optimized_prompt,
 285:                 'accuracy': score_on_validation,
 286:                 'all_candidates': list_of_all_tested_prompts
 287:             }
 288:         """
 289:         # Step 1: Generate candidates
 290:         print(f"Generating {self.num_candidates} prompt candidates...")
 291:         candidates = self._generate_prompts(task_description, train_examples)
 292:         
 293:         # Step 2: Evaluate each candidate
 294:         print(f"Evaluating {len(candidates)} candidates...")
 295:         results = []
 296:         for i, candidate in enumerate(candidates):
 297:             accuracy = self._evaluate_prompt(candidate, validation_examples)
 298:             results.append({
 299:                 'prompt': candidate,
 300:                 'accuracy': accuracy,
 301:                 'rank': None  # Will be filled after sorting
 302:             })
 303:             print(f"  Candidate {i+1}/{len(candidates)}: {accuracy:.1%}")
 304:         
 305:         # Step 3: Select best
 306:         results.sort(key=lambda x: x['accuracy'], reverse=True)
 307:         for i, result in enumerate(results):
 308:             result['rank'] = i + 1
 309:         
 310:         best = results[0]
 311:         print(f"\n‚úÖ Best prompt found (Rank 1, {best['accuracy']:.1%} accuracy)")
 312:         
 313:         return {
 314:             'best_prompt': best['prompt'],
 315:             'accuracy': best['accuracy'],
 316:             'all_candidates': results
 317:         }
 318:     
 319:     def _generate_prompts(self, task_description, examples):
 320:         """Generate diverse prompt candidates."""
 321:         
 322:         # Format examples
 323:         examples_text = "\n\n".join([
 324:             f"Input: {ex['input']}\nOutput: {ex['output']}"
 325:             for ex in examples[:5]  # Use first 5 for generation
 326:         ])
 327:         
 328:         generation_prompt = f"""
 329: Generate {self.num_candidates} different prompts for this task:
 330: 
 331: Task: {task_description}
 332: 
 333: Example input-output pairs:
 334: {examples_text}
 335: 
 336: Create diverse prompts that would help an AI perform this task accurately.
 337: Vary the approach: some should be concise, others detailed; some should emphasize reasoning, others output format; etc.
 338: 
 339: List {self.num_candidates} prompts, numbered:
 340: 
 341: 1."""
 342:         
 343:         response = self.llm.complete(
 344:             generation_prompt,
 345:             temperature=0.9,  # High temp for diversity
 346:             max_tokens=2000
 347:         )
 348:         
 349:         # Parse numbered list
 350:         candidates = self._parse_numbered_list(response)
 351:         
 352:         return candidates[:self.num_candidates]  # Ensure we have exactly num_candidates
 353:     
 354:     def _evaluate_prompt(self, prompt, validation_examples):
 355:         """Score prompt on validation set."""
 356:         
 357:         correct = 0
 358:         total = len(validation_examples)
 359:         
 360:         for example in validation_examples:
 361:             # Construct full prompt
 362:             full_prompt = f"{prompt}\n\nInput: {example['input']}\nOutput:"
 363:             
 364:             # Get prediction
 365:             prediction = self.llm.complete(
 366:                 full_prompt,
 367:                 temperature=0.0,  # Deterministic for eval
 368:                 max_tokens=100
 369:             ).strip()
 370:             
 371:             # Check correctness
 372:             if self._is_correct(prediction, example['expected']):
 373:                 correct += 1
 374:         
 375:         return correct / total
 376:     
 377:     def _is_correct(self, prediction, expected):
 378:         """Check if prediction matches expected output."""
 379:         # Simple exact match (can be made more sophisticated)
 380:         pred_clean = prediction.strip().lower()
 381:         exp_clean = expected.strip().lower()
 382:         
 383:         return pred_clean == exp_clean or pred_clean in exp_clean
 384:     
 385:     def _parse_numbered_list(self, text):
 386:         """Extract numbered items from LLM response."""
 387:         import re
 388:         
 389:         # Match patterns like "1. Some text" or "1) Some text"
 390:         pattern = r'\d+[\.)]\s*(.+?)(?=\n\d+[\.)]|\Z)'
 391:         matches = re.findall(pattern, text, re.DOTALL)
 392:         
 393:         return [match.strip() for match in matches]
 394: 
 395: 
 396: # Usage
 397: ape = AutomaticPromptEngineer(llm, num_candidates=20)
 398: 
 399: result = ape.optimize(
 400:     task_description="Classify text sentiment as Positive, Negative, or Neutral",
 401:     train_examples=[
 402:         {'input': 'I love this product!', 'output': 'Positive'},
 403:         {'input': 'Terrible experience.', 'output': 'Negative'},
 404:         {'input': 'It works as expected.', 'output': 'Neutral'}
 405:     ],
 406:     validation_examples=[
 407:         {'input': 'Best purchase ever!', 'expected': 'Positive'},
 408:         {'input': 'Waste of money.', 'expected': 'Negative'},
 409:         # ... 20+ more for robust evaluation
 410:     ]
 411: )
 412: 
 413: print(f"\nOptimal Prompt:\n{result['best_prompt']}")
 414: ```
 415: 
 416: ### üí° When to Use APE
 417: 
 418: **[APE-Use-Cases**:: (1) New task requiring prompt from scratch, (2) Have labeled examples but no good prompt yet, (3) Manual iteration not yielding improvements, (4) Need to optimize multiple prompts quickly, (5) Want baseline before more sophisticated optimization.]**
 419: 
 420: **‚úÖ Excellent For:**
 421: - **Rapid prototyping** (get good prompt quickly)
 422: - **Benchmark establishment** (what's achievable?)
 423: - **Task with many examples** (data-rich scenarios)
 424: - **Replacing manual prompt engineering** (automation value high)
 425: 
 426: **‚ùå Not Worth It For:**
 427: - **Trivial tasks** (manual prompting sufficient)
 428: - **Few examples** (<10 validation examples - unreliable evaluation)
 429: - **Highly complex tasks** (APE may not explore sophisticated enough strategies)
 430: 
 431: ### üìä Performance Benchmarks
 432: 
 433: **From Zhou et al. 2023**:
 434: 
 435: | Task | Manual Baseline | APE | Improvement |
 436: |------|----------------|-----|-------------|
 437: | **Instruction Induction** | 64.2% | 77.8% | **+13.6pp** |
 438: | **BBH (Reasoning)** | 55.1% | 62.9% | **+7.8pp** |
 439: | **TruthfulQA** | 48.3% | 56.1% | **+7.8pp** |
 440: 
 441: **[APE-Human-Level**:: On many tasks, APE-generated prompts match or exceed human expert prompts. Largest gains where task is well-specified but optimal phrasing unclear.]**
 442: 
 443: ### ‚ö†Ô∏è Limitations
 444: 
 445: 1. **Requires good evaluation set**: Bad eval ‚Üí bad optimization
 446: 2. **Single-round**: Doesn't iteratively improve (see OPRO for iterative)
 447: 3. **Candidate diversity limited**: LLM may generate similar variations
 448: 4. **Compute cost**: Evaluating 20-50 candidates on validation set expensive
 449: 5. **Local optimum**: Finds good prompt in explored space, may miss great prompts
 450: 
 451: ---
 452: 
 453: ## OPRO: Optimization by Prompting
 454: 
 455: [**OPRO**:: Iterative optimization framework where LLM acts as optimizer, maintaining history of (prompt, score) pairs and proposing improved prompts based on what worked previously - enabling systematic convergence to high-quality prompts over multiple rounds.]
 456: 
 457: ### üéØ Core Concept
 458: 
 459: **[OPRO-Innovation**:: Treats prompt optimization as iterative search where LLM is the optimizer. Each iteration: (1) LLM sees previous prompts and scores, (2) proposes new improved prompt, (3) new prompt evaluated, (4) result added to history. LLM learns from trajectory what improves performance.]**
 460: 
 461: **APE vs OPRO**:
 462: ```
 463: APE (One Round):
 464: Generate 50 candidates ‚Üí Evaluate all ‚Üí Select best
 465: (Breadth-first search)
 466: 
 467: OPRO (Multiple Rounds):
 468: Round 1: Generate 5 candidates ‚Üí Evaluate ‚Üí Keep best
 469: Round 2: See Round 1 results ‚Üí Generate improved 5 ‚Üí Evaluate
 470: Round 3: See Rounds 1-2 results ‚Üí Generate better 5 ‚Üí Evaluate
 471: ...
 472: Round N: Converge to optimum
 473: (Gradient descent-like search)
 474: ```
 475: 
 476: ### üî¨ How It Works
 477: 
 478: **[OPRO-Meta-Prompt**:: Fixed prompt instructing LLM to act as optimizer: "Below are prompts and their scores. Your task is to propose new prompts that will score higher. Propose N new prompts that improve upon previous attempts."]**
 479: 
 480: **Iteration Structure**:
 481: 
 482: ```
 483: Meta-Prompt (Fixed):
 484: "You are an optimization algorithm. Below are instruction prompts and their accuracies on a task.
 485: 
 486: <trajectory>
 487: Prompt: "Solve this problem"
 488: Score: 0.65
 489: 
 490: Prompt: "Think step by step and solve"  
 491: Score: 0.71
 492: 
 493: Prompt: "Analyze carefully then solve"
 494: Score: 0.69
 495: </trajectory>
 496: 
 497: Based on this trajectory, propose 3 new prompts that will achieve higher scores.
 498: 
 499: New prompts:"
 500: 
 501: LLM Output:
 502: 1. "Break the problem into steps, solve each step, then combine for final answer"
 503: 2. "First understand what's being asked, then methodically solve"
 504: 3. "Think step by step. Show your work. Verify your answer."
 505: 
 506: [Evaluate these 3 new prompts]
 507: [Add best to trajectory]
 508: [Repeat for next iteration]
 509: ```
 510: 
 511: ### üìù Complete Example: Math Problem Optimization
 512: 
 513: **Task**: Optimize prompt for GSM8K math problems
 514: 
 515: **Starting Prompt**: "Solve this problem."
 516: 
 517: **OPRO Trajectory**:
 518: 
 519: ```
 520: Iteration 0:
 521: Prompt: "Solve this problem."
 522: Score: 0.58
 523: 
 524: Iteration 1:
 525: Meta-prompt generates:
 526: 1. "Solve step by step."
 527: 2. "Think carefully and solve."
 528: 3. "Show your work."
 529: 
 530: Best: "Solve step by step." ‚Üí Score: 0.67 (+0.09)
 531: 
 532: Iteration 2:
 533: Meta-prompt sees history, generates:
 534: 1. "Break into steps: identify knowns, determine operation, calculate, verify."
 535: 2. "Solve step by step. Show each calculation."
 536: 3. "Think step by step. Check your answer."
 537: 
 538: Best: "Solve step by step. Show each calculation." ‚Üí Score: 0.73 (+0.06)
 539: 
 540: Iteration 3:
 541: Meta-prompt generates:
 542: 1. "Let's solve step by step: 1) Identify quantities 2) Determine operation 3) Calculate 4) Verify units match"
 543: 2. "Solve systematically: extract data, set up equation, compute, state final answer with units"
 544: 3. "Step-by-step solution with verification: ..."
 545: 
 546: Best: "Let's solve step by step: 1) Identify quantities 2) Determine operation 3) Calculate 4) Verify units match" ‚Üí Score: 0.79 (+0.06)
 547: 
 548: ... continues until convergence or max iterations ...
 549: 
 550: Final (Iteration 8):
 551: Prompt: "Let's work through this step-by-step:
 552: 1) Read carefully and identify all given quantities
 553: 2) Determine what operation(s) are needed
 554: 3) Perform calculations, showing work
 555: 4) State the answer clearly with appropriate units
 556: 5) Double-check the answer makes sense"
 557: 
 558: Score: 0.82 (+0.24 from start)
 559: ```
 560: 
 561: ### üîß OPRO Implementation
 562: 
 563: ```python
 564: class OPROOptimizer:
 565:     """
 566:     Optimization by Prompting framework.
 567:     
 568:     Iteratively improves prompts using LLM as optimizer.
 569:     """
 570:     
 571:     def __init__(self, llm, task_description, validation_set):
 572:         self.llm = llm
 573:         self.task_description = task_description
 574:         self.validation_set = validation_set
 575:         self.trajectory = []  # History of (prompt, score) pairs
 576:     
 577:     def optimize(self, initial_prompt, num_iterations=8, candidates_per_iter=3):
 578:         """
 579:         Optimize prompt over multiple iterations.
 580:         
 581:         Args:
 582:             initial_prompt: Starting point
 583:             num_iterations: How many optimization rounds
 584:             candidates_per_iter: New prompts to try each iteration
 585:         
 586:         Returns:
 587:             {
 588:                 'best_prompt': final_optimized_prompt,
 589:                 'best_score': accuracy_on_validation,
 590:                 'trajectory': full_optimization_history
 591:             }
 592:         """
 593:         # Evaluate initial prompt
 594:         initial_score = self._evaluate_prompt(initial_prompt)
 595:         self.trajectory.append({
 596:             'iteration': 0,
 597:             'prompt': initial_prompt,
 598:             'score': initial_score
 599:         })
 600:         
 601:         print(f"Iteration 0 (Initial): {initial_score:.1%}")
 602:         
 603:         # Optimization loop
 604:         for iteration in range(1, num_iterations + 1):
 605:             print(f"\nüîÑ Iteration {iteration}")
 606:             
 607:             # Generate new candidate prompts based on trajectory
 608:             candidates = self._generate_improved_prompts(candidates_per_iter)
 609:             
 610:             # Evaluate each candidate
 611:             best_candidate = None
 612:             best_score = -1
 613:             
 614:             for i, candidate in enumerate(candidates):
 615:                 score = self._evaluate_prompt(candidate)
 616:                 print(f"  Candidate {i+1}: {score:.1%}")
 617:                 
 618:                 if score > best_score:
 619:                     best_score = score
 620:                     best_candidate = candidate
 621:             
 622:             # Add best to trajectory
 623:             self.trajectory.append({
 624:                 'iteration': iteration,
 625:                 'prompt': best_candidate,
 626:                 'score': best_score
 627:             })
 628:             
 629:             print(f"  ‚úÖ Best: {best_score:.1%} (Œî = {best_score - self.trajectory[-2]['score']:+.1%})")
 630:             
 631:             # Early stopping if no improvement
 632:             if best_score <= self.trajectory[-2]['score']:
 633:                 print(f"  ‚ö†Ô∏è  No improvement - converged")
 634:                 break
 635:         
 636:         # Return best overall
 637:         best_overall = max(self.trajectory, key=lambda x: x['score'])
 638:         
 639:         return {
 640:             'best_prompt': best_overall['prompt'],
 641:             'best_score': best_overall['score'],
 642:             'improvement': best_overall['score'] - initial_score,
 643:             'trajectory': self.trajectory
 644:         }
 645:     
 646:     def _generate_improved_prompts(self, num_prompts):
 647:         """
 648:         Use LLM as optimizer to generate improved prompts.
 649:         """
 650:         # Format trajectory for meta-prompt
 651:         trajectory_text = self._format_trajectory()
 652:         
 653:         meta_prompt = f"""
 654: You are an optimization algorithm. Your goal is to propose instruction prompts that will maximize performance on this task:
 655: 
 656: Task: {self.task_description}
 657: 
 658: Below is the optimization trajectory showing previous prompts and their accuracies:
 659: 
 660: {trajectory_text}
 661: 
 662: Analyze the trajectory:
 663: - Which prompt elements correlate with higher scores?
 664: - What improvements can be made?
 665: - What new approaches haven't been tried?
 666: 
 667: Propose {num_prompts} new instruction prompts that will score higher than all previous attempts.
 668: 
 669: New prompts:
 670: 1."""
 671:         
 672:         response = self.llm.complete(
 673:             meta_prompt,
 674:             temperature=0.7,  # Moderate temp for diverse but focused proposals
 675:             max_tokens=800
 676:         )
 677:         
 678:         # Parse candidates
 679:         candidates = self._parse_numbered_list(response)
 680:         
 681:         return candidates[:num_prompts]
 682:     
 683:     def _format_trajectory(self):
 684:         """Format optimization history for meta-prompt."""
 685:         lines = []
 686:         for entry in self.trajectory:
 687:             lines.append(
 688:                 f"Iteration {entry['iteration']}:\n"
 689:                 f"Prompt: \"{entry['prompt']}\"\n"
 690:                 f"Score: {entry['score']:.3f}\n"
 691:             )
 692:         return "\n".join(lines)
 693:     
 694:     def _evaluate_prompt(self, prompt):
 695:         """Evaluate prompt on validation set."""
 696:         correct = 0
 697:         
 698:         for example in self.validation_set:
 699:             full_prompt = f"{prompt}\n\n{example['input']}"
 700:             prediction = self.llm.complete(full_prompt, temperature=0.0).strip()
 701:             
 702:             if self._is_correct(prediction, example['expected']):
 703:                 correct += 1
 704:         
 705:         return correct / len(self.validation_set)
 706:     
 707:     def _is_correct(self, prediction, expected):
 708:         """Check if prediction matches expected."""
 709:         return prediction.lower().strip() == expected.lower().strip()
 710:     
 711:     def _parse_numbered_list(self, text):
 712:         """Extract numbered prompts from LLM response."""
 713:         import re
 714:         pattern = r'\d+[\.)]\s*(.+?)(?=\n\d+[\.)]|\Z)'
 715:         matches = re.findall(pattern, text, re.DOTALL)
 716:         return [m.strip().strip('"\'') for m in matches]
 717: 
 718: 
 719: # Usage
 720: opro = OPROOptimizer(
 721:     llm=llm,
 722:     task_description="Solve grade-school math word problems",
 723:     validation_set=math_validation_examples
 724: )
 725: 
 726: result = opro.optimize(
 727:     initial_prompt="Solve this problem.",
 728:     num_iterations=10,
 729:     candidates_per_iter=3
 730: )
 731: 
 732: print(f"\nüéØ Final Best Prompt ({result['best_score']:.1%}):")
 733: print(result['best_prompt'])
 734: print(f"\nüìà Total Improvement: {result['improvement']:+.1%}")
 735: ```
 736: 
 737: ### üí° When to Use OPRO
 738: 
 739: **[OPRO-Use-Cases**:: (1) Have computational budget for iterations (5-20 rounds), (2) Task where incremental improvements valuable, (3) Want systematic exploration vs. random sampling, (4) APE plateau'd but think better exists, (5) Can afford meta-LLM calls (uses LLM to optimize prompts for task-LLM).]**
 740: 
 741: **‚úÖ Excellent For:**
 742: - **High-value tasks** (improvement worth iteration cost)
 743: - **Systematic optimization** (understand what works via trajectory)
 744: - **Benchmark competition** (squeeze out last percentages)
 745: - **Research** (study optimization dynamics)
 746: 
 747: **‚ùå Not Worth It For:**
 748: - **Tight compute budgets** (8+ LLM calls per iteration)
 749: - **Good-enough sufficient** (APE may be enough)
 750: - **Very few validation examples** (noisy scores ‚Üí poor optimization signal)
 751: 
 752: ### üìä Performance Benchmarks
 753: 
 754: **From Yang et al. 2023**:
 755: 
 756: | Task | Manual | APE | OPRO | OPRO Gain |
 757: |------|--------|-----|------|-----------|
 758: | **GSM8K** | 65% | 78% | **82%** | **+4pp over APE** |
 759: | **BBH** | 55% | 63% | **68%** | **+5pp over APE** |
 760: | **MMLU** | 71% | 74% | **77%** | **+3pp over APE** |
 761: 
 762: **[OPRO-Convergence**:: Typically converges in 5-10 iterations. Diminishing returns after iteration 8. Early iterations yield largest gains (40-60% of total improvement in first 3 iterations).]**
 763: 
 764: ### üîó Integration: OPRO + Self-Consistency
 765: 
 766: ```python
 767: def opro_with_sc_evaluation(task_description, validation_set, initial_prompt):
 768:     """
 769:     Use Self-Consistency for more robust prompt evaluation during OPRO.
 770:     
 771:     Each prompt evaluated with SC (5 samples), reducing noise in scores.
 772:     """
 773:     class OPROWithSC(OPROOptimizer):
 774:         def _evaluate_prompt(self, prompt):
 775:             """Override to use Self-Consistency."""
 776:             from collections import Counter
 777:             
 778:             example_scores = []
 779:             
 780:             for example in self.validation_set:
 781:                 # Self-Consistency: 5 predictions per example
 782:                 predictions = []
 783:                 for _ in range(5):
 784:                     full_prompt = f"{prompt}\n\n{example['input']}"
 785:                     pred = self.llm.complete(full_prompt, temperature=0.7).strip()
 786:                     predictions.append(pred)
 787:                 
 788:                 # Majority vote
 789:                 vote = Counter(predictions)
 790:                 majority_pred = vote.most_common(1)[0][0]
 791:                 
 792:                 # Score
 793:                 if self._is_correct(majority_pred, example['expected']):
 794:                     example_scores.append(1.0)
 795:                 else:
 796:                     example_scores.append(0.0)
 797:             
 798:             return sum(example_scores) / len(example_scores)
 799:     
 800:     opro_sc = OPROWithSC(llm, task_description, validation_set)
 801:     return opro_sc.optimize(initial_prompt)
 802: ```
 803: 
 804: ---
 805: 
 806: ## Active-Prompt
 807: 
 808: [**Active-Prompt**:: Selects most informative few-shot examples based on uncertainty - identifies inputs where model is least confident, elicits human annotations for those, includes as examples in prompt to maximally improve performance.]
 809: 
 810: ### üéØ Core Concept
 811: 
 812: **The Problem**: Few-shot prompting requires selecting k examples from dataset. Random selection may include redundant easy examples, missing hard cases where model needs most help.
 813: 
 814: **[Active-Prompt-Innovation**:: Uses active learning principle - select examples where model is most uncertain. Uncertain examples are most informative for learning. Annotating uncertain cases improves prompt more than annotating easy cases model already handles.]**
 815: 
 816: **Process**:
 817: ```
 818: 1. Unlabeled pool of inputs
 819: 2. For each input, measure model uncertainty (multiple predictions, check variance)
 820: 3. Select k inputs with highest uncertainty
 821: 4. Get annotations for those k (human or high-quality LLM)
 822: 5. Use as few-shot examples in prompt
 823: ```
 824: 
 825: ### üî¨ How It Works
 826: 
 827: **[Active-Prompt-Uncertainty-Metrics**:: (1) Disagreement - run CoT multiple times, count how many different answers, (2) Entropy - if model outputs probabilities, measure entropy, (3) Confidence - use model's self-assessed confidence scores.]**
 828: 
 829: #### Uncertainty via Disagreement
 830: 
 831: ```python
 832: def calculate_uncertainty(input_text, num_samples=5):
 833:     """
 834:     Measure uncertainty by running CoT multiple times.
 835:     
 836:     High disagreement = high uncertainty.
 837:     """
 838:     predictions = []
 839:     
 840:     for _ in range(num_samples):
 841:         prompt = f"Let's think step by step.\n\n{input_text}"
 842:         response = llm.complete(prompt, temperature=0.7)
 843:         answer = extract_answer(response)
 844:         predictions.append(answer)
 845:     
 846:     # Calculate disagreement rate
 847:     from collections import Counter
 848:     counts = Counter(predictions)
 849:     most_common_count = counts.most_common(1)[0][1]
 850:     
 851:     # Disagreement = 1 - (most_common / total)
 852:     disagreement = 1 - (most_common_count / num_samples)
 853:     
 854:     return disagreement  # 0 = all agree, 1 = all different
 855: 
 856: 
 857: # Example
 858: uncertainty_1 = calculate_uncertainty("2 + 2 = ?")
 859: # Returns: 0.0 (all predictions agree: "4")
 860: 
 861: uncertainty_2 = calculate_uncertainty("Complex ambiguous question...")
 862: # Returns: 0.8 (predictions: ["A", "B", "A", "C", "B"])
 863: ```
 864: 
 865: #### Active Selection
 866: 
 867: ```python
 868: def active_prompt_selection(unlabeled_pool, k=5):
 869:     """
 870:     Select k most uncertain examples for annotation.
 871:     """
 872:     # Calculate uncertainty for each
 873:     scored_pool = []
 874:     for input_text in unlabeled_pool:
 875:         uncertainty = calculate_uncertainty(input_text)
 876:         scored_pool.append((uncertainty, input_text))
 877:     
 878:     # Sort by uncertainty (descending)
 879:     scored_pool.sort(reverse=True)
 880:     
 881:     # Select top k most uncertain
 882:     most_uncertain = [text for _, text in scored_pool[:k]]
 883:     
 884:     return most_uncertain
 885: 
 886: 
 887: # Usage
 888: unlabeled_inputs = [
 889:     "What is 5 + 3?",
 890:     "If a train leaves Chicago at 9am traveling 60mph...",
 891:     "Complex reasoning problem with ambiguous wording...",
 892:     # ... hundreds more
 893: ]
 894: 
 895: selected_for_annotation = active_prompt_selection(unlabeled_inputs, k=8)
 896: 
 897: # Get annotations (human or high-quality LLM)
 898: annotated_examples = []
 899: for input_text in selected_for_annotation:
 900:     # Could be human annotation or high-quality LLM
 901:     answer = get_annotation(input_text)
 902:     annotated_examples.append({'input': input_text, 'output': answer})
 903: 
 904: # Build few-shot prompt with these
 905: few_shot_prompt = build_prompt_with_examples(annotated_examples)
 906: ```
 907: 
 908: ### üìù Complete Example: Math Problem Selection
 909: 
 910: **Scenario**: Have 1000 unlabeled math problems, budget for 5 annotations
 911: 
 912: **Step 1: Measure Uncertainty**
 913: 
 914: ```python
 915: math_problems = [
 916:     "2 + 2 = ?",
 917:     "If 3 apples cost $1.50, how much do 7 apples cost?",
 918:     "A complex multi-step problem involving percentages and fractions...",
 919:     # ... 997 more
 920: ]
 921: 
 922: uncertainties = []
 923: for problem in math_problems:
 924:     u = calculate_uncertainty(problem, num_samples=5)
 925:     uncertainties.append((u, problem))
 926: 
 927: # Sort by uncertainty
 928: uncertainties.sort(reverse=True)
 929: 
 930: print("Most Uncertain Problems:")
 931: for uncertainty, problem in uncertainties[:5]:
 932:     print(f"  Uncertainty: {uncertainty:.2f} - {problem[:50]}...")
 933: ```
 934: 
 935: **Output**:
 936: ```
 937: Most Uncertain Problems:
 938:   Uncertainty: 0.80 - A complex multi-step problem involving...
 939:   Uncertainty: 0.75 - If x is 20% of y, and y is 150% of z...
 940:   Uncertainty: 0.70 - Three workers can complete a job in different...
 941:   Uncertainty: 0.65 - A mixture problem with changing concentrations...
 942:   Uncertainty: 0.60 - Probability question with conditional events...
 943: ```
 944: 
 945: **Step 2: Annotate Selected**
 946: 
 947: ```python
 948: selected_problems = [problem for _, problem in uncertainties[:5]]
 949: 
 950: annotated = []
 951: for problem in selected_problems:
 952:     # High-quality annotation (could be human or GPT-4)
 953:     answer = expert_annotator(problem)
 954:     annotated.append({'input': problem, 'output': answer})
 955: ```
 956: 
 957: **Step 3: Build Prompt**
 958: 
 959: ```python
 960: few_shot_prompt = "Solve these math problems.\n\nExamples:\n\n"
 961: 
 962: for ex in annotated:
 963:     few_shot_prompt += f"Problem: {ex['input']}\nSolution: {ex['output']}\n\n"
 964: 
 965: few_shot_prompt += "Now solve:\n\nProblem: {new_problem}\nSolution:"
 966: ```
 967: 
 968: **Result**: Few-shot prompt with 5 highly informative examples (the uncertain cases) performs better than random 5 examples.
 969: 
 970: ### üîß Active-Prompt Implementation
 971: 
 972: ```python
 973: class ActivePromptSelector:
 974:     """
 975:     Select most informative few-shot examples via uncertainty.
 976:     """
 977:     
 978:     def __init__(self, llm, uncertainty_samples=5):
 979:         self.llm = llm
 980:         self.uncertainty_samples = uncertainty_samples
 981:     
 982:     def select_examples(self, unlabeled_pool, k, annotator):
 983:         """
 984:         Select k most uncertain examples and get annotations.
 985:         
 986:         Args:
 987:             unlabeled_pool: List of input texts
 988:             k: Number of examples to select
 989:             annotator: Function that takes input and returns annotated output
 990:         
 991:         Returns:
 992:             List of {'input': ..., 'output': ...} annotated examples
 993:         """
 994:         print(f"Calculating uncertainty for {len(unlabeled_pool)} examples...")
 995:         
 996:         # Calculate uncertainty for each
 997:         scored = []
 998:         for i, input_text in enumerate(unlabeled_pool):
 999:             uncertainty = self._calculate_uncertainty(input_text)
1000:             scored.append((uncertainty, input_text))
1001:             
1002:             if (i + 1) % 50 == 0:
1003:                 print(f"  Processed {i+1}/{len(unlabeled_pool)}")
1004:         
1005:         # Select top k most uncertain
1006:         scored.sort(reverse=True)
1007:         selected_inputs = [text for _, text in scored[:k]]
1008:         
1009:         print(f"\nSelected {k} most uncertain examples")
1010:         print("Getting annotations...")
1011:         
1012:         # Get annotations
1013:         annotated_examples = []
1014:         for i, input_text in enumerate(selected_inputs):
1015:             output = annotator(input_text)
1016:             annotated_examples.append({
1017:                 'input': input_text,
1018:                 'output': output
1019:             })
1020:             print(f"  Annotated {i+1}/{k}")
1021:         
1022:         return annotated_examples
1023:     
1024:     def _calculate_uncertainty(self, input_text):
1025:         """
1026:         Measure uncertainty via disagreement in multiple predictions.
1027:         """
1028:         predictions = []
1029:         
1030:         # Generate multiple predictions
1031:         for _ in range(self.uncertainty_samples):
1032:             prompt = f"Let's think step by step.\n\n{input_text}\n\nAnswer:"
1033:             response = self.llm.complete(
1034:                 prompt,
1035:                 temperature=0.7,  # Need diversity
1036:                 max_tokens=200
1037:             )
1038:             answer = self._extract_answer(response)
1039:             predictions.append(answer)
1040:         
1041:         # Calculate disagreement
1042:         from collections import Counter
1043:         counts = Counter(predictions)
1044:         
1045:         if len(counts) == 0:
1046:             return 0.0
1047:         
1048:         most_common_count = counts.most_common(1)[0][1]
1049:         agreement = most_common_count / len(predictions)
1050:         uncertainty = 1 - agreement
1051:         
1052:         return uncertainty
1053:     
1054:     def _extract_answer(self, response):
1055:         """Extract final answer from response."""
1056:         # Simple extraction - can be made more sophisticated
1057:         lines = response.strip().split('\n')
1058:         return lines[-1].strip()
1059:     
1060:     def build_few_shot_prompt(self, annotated_examples, task_description=""):
1061:         """
1062:         Construct few-shot prompt with selected examples.
1063:         """
1064:         prompt = task_description
1065:         if task_description:
1066:             prompt += "\n\n"
1067:         
1068:         prompt += "Examples:\n\n"
1069:         
1070:         for ex in annotated_examples:
1071:             prompt += f"Input: {ex['input']}\nOutput: {ex['output']}\n\n"
1072:         
1073:         prompt += "Now solve:\n\nInput: {{new_input}}\nOutput:"
1074:         
1075:         return prompt
1076: 
1077: 
1078: # Usage
1079: selector = ActivePromptSelector(llm, uncertainty_samples=5)
1080: 
1081: # Select examples
1082: annotated = selector.select_examples(
1083:     unlabeled_pool=math_problems,
1084:     k=8,
1085:     annotator=lambda x: expert_llm.solve(x)  # High-quality annotator
1086: )
1087: 
1088: # Build prompt
1089: few_shot_prompt = selector.build_few_shot_prompt(
1090:     annotated,
1091:     task_description="Solve these grade-school math problems."
1092: )
1093: 
1094: # Use prompt
1095: result = llm.complete(few_shot_prompt.format(new_input="Sarah has 15 cookies..."))
1096: ```
1097: 
1098: ### üí° When to Use Active-Prompt
1099: 
1100: **[Active-Prompt-Use-Cases**:: (1) Large unlabeled pool but limited annotation budget, (2) Few-shot learning where example quality matters more than quantity, (3) Task has some hard cases model struggles with, (4) Want to maximize performance per annotation, (5) Can afford uncertainty calculation cost.]**
1101: 
1102: **‚úÖ Excellent For:**
1103: - **Expensive annotations** (human expert time costly)
1104: - **Unbalanced difficulty** (mix of easy and hard examples)
1105: - **Few-shot optimization** (selecting best k from large pool)
1106: - **Domain adaptation** (find edge cases in new domain)
1107: 
1108: **‚ùå Not Worth It For:**
1109: - **Cheap annotations** (if labeling is free, label everything)
1110: - **Homogeneous difficulty** (all examples equally hard/easy - no benefit)
1111: - **Very small pools** (if only have 10 examples, just use all)
1112: - **Tight compute budget** (uncertainty calculation requires multiple forward passes)
1113: 
1114: ### üìä Performance Benchmarks
1115: 
1116: **From Diao et al. 2023**:
1117: 
1118: | Selection Method | GSM8K | SVAMP | AQuA |
1119: |------------------|-------|-------|------|
1120: | **Random 8-shot** | 71.2% | 76.8% | 42.1% |
1121: | **Active-Prompt 8-shot** | **78.5%** | **82.3%** | **48.9%** |
1122: | **Improvement** | **+7.3pp** | **+5.5pp** | **+6.8pp** |
1123: 
1124: **[Active-Prompt-Efficiency**:: With same annotation budget (k examples), active selection yields +5-7pp improvement over random selection. Most gains from identifying genuinely hard cases model needs help with.]**
1125: 
1126: ---
1127: 
1128: ## PromptBreeder
1129: 
1130: [**PromptBreeder**:: Self-referential evolutionary algorithm where LLM breeds better prompts through mutation and selection - prompts that perform well survive and reproduce, generating even better offspring over many generations without human intervention.]**
1131: 
1132: ### üéØ Core Concept
1133: 
1134: **[PromptBreeder-Innovation**:: Applies evolutionary algorithms to prompt engineering. Start with population of prompts, evaluate fitness (performance), select best performers, mutate/crossover to create offspring, repeat for many generations. LLM generates mutations of prompts, creating self-improving system.]**
1135: 
1136: **Evolutionary Process**:
1137: ```
1138: Generation 0: Random initial population (10-20 prompts)
1139: ‚Üì
1140: Evaluate fitness (accuracy on validation set)
1141: ‚Üì
1142: Select best performers (top 50%)
1143: ‚Üì
1144: Breed offspring via mutation:
1145:   - LLM mutates prompt: "Make this prompt better..."
1146:   - LLM crosses prompts: "Combine these two prompts..."
1147: ‚Üì
1148: Replace worst with offspring
1149: ‚Üì
1150: Generation 1: New population
1151: ‚Üì
1152: Repeat for 50-100 generations
1153: ```
1154: 
1155: ### üî¨ How It Works
1156: 
1157: **[PromptBreeder-Components**:: (1) Task prompts - the actual prompts being optimized, (2) Mutation prompts - meta-prompts that tell LLM how to mutate task prompts, (3) Fitness function - evaluation on validation set, (4) Evolution operators - selection, mutation, crossover.]**
1158: 
1159: #### Mutation Operators
1160: 
1161: **Mutation Prompt Templates**:
1162: ```markdown
1163: # Mutation 1: Direct Improvement
1164: "Here is a prompt: '{prompt}'
1165: 
1166: Make this prompt better for the task: {task_description}
1167: 
1168: Improved prompt:"
1169: 
1170: # Mutation 2: Add Constraint
1171: "Here is a prompt: '{prompt}'
1172: 
1173: Add a helpful constraint or instruction to make it better.
1174: 
1175: Enhanced prompt:"
1176: 
1177: # Mutation 3: Simplify
1178: "Here is a prompt: '{prompt}'
1179: 
1180: Simplify this prompt while preserving its effectiveness.
1181: 
1182: Simplified prompt:"
1183: 
1184: # Mutation 4: Crossover
1185: "Here are two prompts:
1186: Prompt A: '{prompt_a}'
1187: Prompt B: '{prompt_b}'
1188: 
1189: Combine the best elements of both into a new prompt.
1190: 
1191: Combined prompt:"
1192: ```
1193: 
1194: #### Complete Algorithm
1195: 
1196: ```python
1197: class PromptBreeder:
1198:     """
1199:     Evolutionary algorithm for prompt optimization.
1200:     """
1201:     
1202:     def __init__(self, llm, task_description, validation_set,
1203:                  population_size=20, num_generations=50):
1204:         self.llm = llm
1205:         self.task_description = task_description
1206:         self.validation_set = validation_set
1207:         self.population_size = population_size
1208:         self.num_generations = num_generations
1209:         
1210:         # Mutation prompts (meta-level)
1211:         self.mutation_templates = self._create_mutation_templates()
1212:     
1213:     def evolve(self, seed_prompts=None):
1214:         """
1215:         Run evolutionary optimization.
1216:         
1217:         Args:
1218:             seed_prompts: Optional initial prompts (else random)
1219:         
1220:         Returns:
1221:             Best prompt after evolution
1222:         """
1223:         # Initialize population
1224:         if seed_prompts and len(seed_prompts) >= self.population_size:
1225:             population = seed_prompts[:self.population_size]
1226:         else:
1227:             population = self._initialize_population(seed_prompts)
1228:         
1229:         # Evolution loop
1230:         best_fitness_history = []
1231:         
1232:         for gen in range(self.num_generations):
1233:             print(f"\nüß¨ Generation {gen + 1}/{self.num_generations}")
1234:             
1235:             # Evaluate fitness
1236:             fitness_scores = self._evaluate_population(population)
1237:             
1238:             # Track best
1239:             best_idx = fitness_scores.index(max(fitness_scores))
1240:             best_fitness = fitness_scores[best_idx]
1241:             best_fitness_history.append(best_fitness)
1242:             
1243:             print(f"  Best fitness: {best_fitness:.1%}")
1244:             print(f"  Avg fitness: {sum(fitness_scores)/len(fitness_scores):.1%}")
1245:             
1246:             # Selection
1247:             parents = self._select_parents(population, fitness_scores)
1248:             
1249:             # Create offspring via mutation/crossover
1250:             offspring = self._create_offspring(parents)
1251:             
1252:             # Replacement
1253:             population = self._replace_worst(population, fitness_scores, offspring)
1254:         
1255:         # Return best from final population
1256:         final_fitness = self._evaluate_population(population)
1257:         best_idx = final_fitness.index(max(final_fitness))
1258:         
1259:         return {
1260:             'best_prompt': population[best_idx],
1261:             'best_fitness': final_fitness[best_idx],
1262:             'fitness_history': best_fitness_history
1263:         }
1264:     
1265:     def _initialize_population(self, seeds=None):
1266:         """Create initial population."""
1267:         population = []
1268:         
1269:         # Add seeds if provided
1270:         if seeds:
1271:             population.extend(seeds)
1272:         
1273:         # Generate rest randomly
1274:         while len(population) < self.population_size:
1275:             prompt = self._generate_random_prompt()
1276:             population.append(prompt)
1277:         
1278:         return population
1279:     
1280:     def _generate_random_prompt(self):
1281:         """Generate a random initial prompt."""
1282:         gen_prompt = f"""
1283: Generate a random instruction prompt for this task:
1284: {self.task_description}
1285: 
1286: The prompt should tell an AI how to perform the task.
1287: 
1288: Prompt:"""
1289:         
1290:         prompt = self.llm.complete(gen_prompt, temperature=1.0).strip()
1291:         return prompt
1292:     
1293:     def _evaluate_population(self, population):
1294:         """Evaluate fitness (accuracy) for each prompt."""
1295:         fitness_scores = []
1296:         
1297:         for prompt in population:
1298:             accuracy = self._evaluate_prompt(prompt)
1299:             fitness_scores.append(accuracy)
1300:         
1301:         return fitness_scores
1302:     
1303:     def _evaluate_prompt(self, prompt):
1304:         """Calculate accuracy on validation set."""
1305:         correct = 0
1306:         
1307:         for example in self.validation_set:
1308:             full_prompt = f"{prompt}\n\n{example['input']}"
1309:             prediction = self.llm.complete(full_prompt, temperature=0.0).strip()
1310:             
1311:             if prediction.lower() == example['expected'].lower():
1312:                 correct += 1
1313:         
1314:         return correct / len(self.validation_set)
1315:     
1316:     def _select_parents(self, population, fitness_scores):
1317:         """Select top 50% as parents."""
1318:         # Pair prompts with fitness
1319:         paired = list(zip(fitness_scores, population))
1320:         paired.sort(reverse=True)
1321:         
1322:         # Select top 50%
1323:         num_parents = self.population_size // 2
1324:         parents = [prompt for _, prompt in paired[:num_parents]]
1325:         
1326:         return parents
1327:     
1328:     def _create_offspring(self, parents):
1329:         """Generate offspring via mutation and crossover."""
1330:         offspring = []
1331:         
1332:         import random
1333:         
1334:         while len(offspring) < len(parents):
1335:             # Randomly choose mutation or crossover
1336:             if random.random() < 0.7:  # 70% mutation
1337:                 parent = random.choice(parents)
1338:                 child = self._mutate(parent)
1339:             else:  # 30% crossover
1340:                 parent1, parent2 = random.sample(parents, 2)
1341:                 child = self._crossover(parent1, parent2)
1342:             
1343:             offspring.append(child)
1344:         
1345:         return offspring
1346:     
1347:     def _mutate(self, prompt):
1348:         """Mutate prompt using LLM."""
1349:         mutation_template = random.choice(self.mutation_templates)
1350:         
1351:         mutation_prompt = mutation_template.format(
1352:             prompt=prompt,
1353:             task_description=self.task_description
1354:         )
1355:         
1356:         mutated = self.llm.complete(
1357:             mutation_prompt,
1358:             temperature=0.8  # High temp for diversity
1359:         ).strip()
1360:         
1361:         return mutated
1362:     
1363:     def _crossover(self, prompt1, prompt2):
1364:         """Combine two prompts."""
1365:         crossover_prompt = f"""
1366: Combine these two prompts into one better prompt:
1367: 
1368: Prompt A: {prompt1}
1369: 
1370: Prompt B: {prompt2}
1371: 
1372: Take the best elements from each.
1373: 
1374: Combined prompt:"""
1375:         
1376:         combined = self.llm.complete(crossover_prompt, temperature=0.7).strip()
1377:         return combined
1378:     
1379:     def _replace_worst(self, population, fitness_scores, offspring):
1380:         """Replace worst individuals with offspring."""
1381:         # Pair and sort
1382:         paired = list(zip(fitness_scores, population))
1383:         paired.sort(reverse=True)
1384:         
1385:         # Keep best half, replace worst half with offspring
1386:         new_population = [prompt for _, prompt in paired[:len(offspring)]]
1387:         new_population.extend(offspring)
1388:         
1389:         return new_population
1390:     
1391:     def _create_mutation_templates(self):
1392:         """Define mutation prompt templates."""
1393:         return [
1394:             "Improve this prompt: '{prompt}'\n\nTask: {task_description}\n\nBetter version:",
1395:             "Add helpful details to this prompt: '{prompt}'\n\nEnhanced version:",
1396:             "Simplify this prompt: '{prompt}'\n\nSimpler version:",
1397:             "Make this prompt more specific: '{prompt}'\n\nMore specific version:",
1398:             "Rephrase this prompt more clearly: '{prompt}'\n\nClearer version:"
1399:         ]
1400: 
1401: 
1402: # Usage
1403: breeder = PromptBreeder(
1404:     llm=llm,
1405:     task_description="Classify sentiment as Positive, Negative, or Neutral",
1406:     validation_set=validation_examples,
1407:     population_size=20,
1408:     num_generations=50
1409: )
1410: 
1411: result = breeder.evolve(seed_prompts=["Classify the sentiment.", "Determine if positive or negative."])
1412: 
1413: print(f"\nüèÜ Evolved Best Prompt ({result['best_fitness']:.1%}):")
1414: print(result['best_prompt'])
1415: ```
1416: 
1417: ### üí° When to Use PromptBreeder
1418: 
1419: **[PromptBreeder-Use-Cases**:: (1) Willing to invest significant compute for maximum performance, (2) Exhausted simpler methods (APE, OPRO), (3) Benchmark competition where every percentage point matters, (4) Research on self-improvement and evolution, (5) Have large computational budget.]**
1420: 
1421: **‚úÖ Excellent For:**
1422: - **Absolute maximum performance** (squeeze out last %)
1423: - **Research purposes** (studying emergence)
1424: - **High-stakes tasks** (worth the compute cost)
1425: - **Benchmark leaderboards** (competitive optimization)
1426: 
1427: **‚ùå Not Worth It For:**
1428: - **Limited compute** (50 generations √ó 20 population = 1000s evaluations)
1429: - **Good-enough sufficient** (OPRO may achieve 95% of benefit)
1430: - **Rapid prototyping** (too slow for iteration)
1431: - **Simple tasks** (overkill)
1432: 
1433: ### üìä Performance Benchmarks
1434: 
1435: **From Fernando et al. 2023**:
1436: 
1437: | Method | Big-Bench Hard | MMLU |
1438: |--------|----------------|------|
1439: | **Manual** | 55% | 71% |
1440: | **APE** | 63% (+8pp) | 74% (+3pp) |
1441: | **OPRO** | 68% (+13pp) | 77% (+6pp) |
1442: | **PromptBreeder** | **71% (+16pp)** | **79% (+8pp)** |
1443: 
1444: **[PromptBreeder-Gains**:: Typically +3-5pp over OPRO, +5-8pp over APE. Gains largest on complex reasoning tasks. Requires 10-50x compute vs OPRO.]**
1445: 
1446: ---
1447: 
1448: ## RPO: Reinforced Prompt Optimization
1449: 
1450: [**RPO**:: Uses reinforcement learning with temporal difference methods to fine-tune prompts, updating based on reward signals and intermediate feedback - enabling gradient-like optimization in discrete prompt space.]**
1451: 
1452: ### üéØ Core Concept
1453: 
1454: **[RPO-Innovation**:: Treats prompt optimization as reinforcement learning problem. Prompt = policy, validation accuracy = reward. Use RL algorithms (temporal difference learning) to update prompts toward higher rewards. Unlike OPRO's discrete sampling, RPO performs more continuous optimization.]**
1455: 
1456: **Key Idea**: Generate prompt variants, get rewards (accuracy), use rewards to guide next generation of variants via RL update rules.
1457: 
1458: ### üî¨ How It Works (Simplified)
1459: 
1460: ```python
1461: # Simplified RPO concept
1462: class SimplifiedRPO:
1463:     """
1464:     Conceptual RPO implementation.
1465:     
1466:     Note: Full RPO requires gradient estimation in discrete space,
1467:     which is complex. This shows the core idea.
1468:     """
1469:     
1470:     def optimize(self, initial_prompt, validation_set, num_episodes=20):
1471:         """
1472:         RL-based prompt optimization.
1473:         """
1474:         current_prompt = initial_prompt
1475:         
1476:         for episode in range(num_episodes):
1477:             # Generate perturbation (small change)
1478:             perturbed = self._perturb_prompt(current_prompt)
1479:             
1480:             # Evaluate both
1481:             reward_current = self._evaluate(current_prompt, validation_set)
1482:             reward_perturbed = self._evaluate(perturbed, validation_set)
1483:             
1484:             # TD update: move toward better reward
1485:             if reward_perturbed > reward_current:
1486:                 # Accept perturbation
1487:                 current_prompt = perturbed
1488:                 print(f"Episode {episode}: Improved to {reward_perturbed:.1%}")
1489:             else:
1490:                 # Reject perturbation (or accept with small probability)
1491:                 print(f"Episode {episode}: Staying at {reward_current:.1%}")
1492:         
1493:         return current_prompt
1494:     
1495:     def _perturb_prompt(self, prompt):
1496:         """Generate small variation of prompt."""
1497:         perturbation_prompt = f"""
1498: Make a small modification to this prompt:
1499: '{prompt}'
1500: 
1501: Modified prompt:"""
1502:         
1503:         return llm.complete(perturbation_prompt, temperature=0.6).strip()
1504: ```
1505: 
1506: **Full RPO is significantly more complex**, involving:
1507: - Policy gradient estimation
1508: - Advantage functions
1509: - Baseline subtraction
1510: - Multiple sampling for gradient estimation
1511: 
1512: Due to complexity, RPO is primarily research-oriented rather than practical for most use cases.
1513: 
1514: ### üí° When to Use RPO
1515: 
1516: **[RPO-Use-Cases**:: (1) Research on RL for prompt optimization, (2) Have infrastructure for RL training, (3) Task requires fine-grained optimization, (4) Other methods plateaued.]**
1517: 
1518: **‚úÖ Consider For:**
1519: - **Research projects** (novel optimization methods)
1520: - **Extreme optimization** (last few percentage points)
1521: 
1522: **‚ùå Not Practical For:**
1523: - **Most production use cases** (complexity >> benefit over OPRO)
1524: - **Limited ML expertise** (requires RL knowledge)
1525: - **Standard tasks** (simpler methods sufficient)
1526: 
1527: ---
1528: 
1529: ## Meta-Prompting
1530: 
1531: [**Meta-Prompting**:: Focuses on structural/syntactical patterns rather than specific content - creating abstract templates that generalize across tasks by emphasizing how to structure prompts, not what specific words to use.]**
1532: 
1533: ### üéØ Core Concept
1534: 
1535: **[Meta-Prompting-Insight**:: Most prompt engineering focuses on content ("say X, Y, Z"). Meta-prompting focuses on structure ("use format A, apply pattern B"). Structural patterns transfer better across tasks than specific phrasing.]**
1536: 
1537: **Example**:
1538: 
1539: ```
1540: Content-focused (doesn't generalize):
1541: "Classify this text as Positive, Negative, or Neutral sentiment"
1542: 
1543: Structure-focused (generalizes):
1544: "Classify {input} into one of: {category_1}, {category_2}, {category_3}"
1545: 
1546: The second is a meta-template applicable to any classification task.
1547: ```
1548: 
1549: ### üî¨ How It Works
1550: 
1551: **Structural Patterns**:
1552: 
1553: ```markdown
1554: # Pattern 1: Classification Template
1555: "Classify {input_description} into one of these categories: {category_list}
1556: 
1557: {optional_context}
1558: 
1559: Input: {input_value}
1560: Category:"
1561: 
1562: # Pattern 2: Extraction Template
1563: "Extract {entity_types} from the following {input_type}.
1564: 
1565: {optional_examples}
1566: 
1567: {input_type}: {input_value}
1568: 
1569: Extracted {entity_types}:"
1570: 
1571: # Pattern 3: Transformation Template
1572: "Transform the input {source_format} to {target_format}.
1573: 
1574: {optional_transformation_rules}
1575: 
1576: Input: {input_value}
1577: Output:"
1578: 
1579: # Pattern 4: Reasoning Template
1580: "{task_description}
1581: 
1582: Think through this step-by-step:
1583: 1. {step_1_description}
1584: 2. {step_2_description}
1585: 3. {step_3_description}
1586: 
1587: Input: {input_value}
1588: 
1589: Step-by-step solution:"
1590: ```
1591: 
1592: ### üìù Example: Building Meta-Templates
1593: 
1594: ```python
1595: class MetaPromptTemplate:
1596:     """
1597:     Structural prompt template with variable slots.
1598:     """
1599:     
1600:     def __init__(self, structure):
1601:         """
1602:         Args:
1603:             structure: Template string with {variable} placeholders
1604:         """
1605:         self.structure = structure
1606:     
1607:     def instantiate(self, **kwargs):
1608:         """Fill template with specific values."""
1609:         return self.structure.format(**kwargs)
1610: 
1611: 
1612: # Define meta-template
1613: classification_meta = MetaPromptTemplate(
1614:     structure="""Classify {input_description} into one of these categories: {categories}
1615: 
1616: {context}
1617: 
1618: Input: {input}
1619: Category:"""
1620: )
1621: 
1622: # Instantiate for different tasks
1623: 
1624: # Task 1: Sentiment analysis
1625: sentiment_prompt = classification_meta.instantiate(
1626:     input_description="the sentiment of this text",
1627:     categories="Positive, Negative, Neutral",
1628:     context="Consider both explicit and implicit emotional cues.",
1629:     input="{user_text}"
1630: )
1631: 
1632: # Task 2: Topic classification
1633: topic_prompt = classification_meta.instantiate(
1634:     input_description="the topic of this article",
1635:     categories="Politics, Sports, Technology, Entertainment",
1636:     context="Focus on the primary subject matter.",
1637:     input="{article_text}"
1638: )
1639: 
1640: # Same structure, different content - structure transfers!
1641: ```
1642: 
1643: ### üí° When to Use Meta-Prompting
1644: 
1645: **[Meta-Prompting-Use-Cases**:: (1) Building prompt libraries for reuse, (2) Zero-shot transfer to new tasks, (3) Systematic prompt design (not ad-hoc), (4) Teaching prompt patterns to others, (5) Creating prompt frameworks/tools.]**
1646: 
1647: **‚úÖ Excellent For:**
1648: - **Prompt libraries** (reusable templates)
1649: - **Framework development** (LangChain-style tools)
1650: - **Cross-task generalization** (one template, many tasks)
1651: - **Systematic design** (structured approach)
1652: 
1653: **‚ùå Not Directly For:**
1654: - **Optimizing specific prompt** (use APE/OPRO instead)
1655: - **Finding best wording** (meta-prompting is structural, not lexical)
1656: 
1657: ---
1658: 
1659: ## Technique Selection Guide
1660: 
1661: ### Decision Tree
1662: 
1663: ```
1664: What's your goal?
1665: 
1666: ‚îå‚îÄ RAPID PROTOTYPING (get something working quickly)
1667: ‚îÇ  ‚îî‚îÄ‚ñ∫ APE (1 round, 20-50 candidates)
1668: ‚îÇ
1669: ‚îú‚îÄ SYSTEMATIC OPTIMIZATION (best possible prompt)
1670: ‚îÇ  ‚îú‚îÄ Moderate compute ‚Üí OPRO (5-10 iterations)
1671: ‚îÇ  ‚îî‚îÄ Large compute ‚Üí PromptBreeder (50 generations)
1672: ‚îÇ
1673: ‚îú‚îÄ LIMITED ANNOTATIONS (expensive labels)
1674: ‚îÇ  ‚îî‚îÄ‚ñ∫ Active-Prompt (select most informative examples)
1675: ‚îÇ
1676: ‚îú‚îÄ BUILDING FRAMEWORK (reusable templates)
1677: ‚îÇ  ‚îî‚îÄ‚ñ∫ Meta-Prompting (structural patterns)
1678: ‚îÇ
1679: ‚îî‚îÄ RESEARCH (novel optimization)
1680:    ‚îî‚îÄ‚ñ∫ RPO or PromptBreeder
1681: ```
1682: 
1683: ### Compute vs. Performance Trade-off
1684: 
1685: ```
1686: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
1687: ‚îÇ                                        ‚îÇ
1688: ‚îÇ         PromptBreeder ‚óè                ‚îÇ
1689: ‚îÇ    RPO ‚óè             (50-100 gens)     ‚îÇ
1690: ‚îÇ  (RL)                                  ‚îÇ
1691: ‚îÇ                                        ‚îÇ
1692: ‚îÇ          OPRO ‚óè                        ‚îÇ
1693: ‚îÇ         (5-10 iters)                   ‚îÇ
1694: ‚îÇ                                        ‚îÇ
1695: ‚îÇ   APE ‚óè                                ‚îÇ
1696: ‚îÇ  (1 round)                             ‚îÇ
1697: ‚îÇ                                        ‚îÇ
1698: ‚îÇ Active-Prompt ‚óè                        ‚îÇ
1699: ‚îÇ  (uncertainty)                         ‚îÇ
1700: ‚îÇ                                        ‚îÇ
1701: ‚îÇ Manual ‚óè                               ‚îÇ
1702: ‚îÇ                                        ‚îÇ
1703: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
1704:  Low                               High
1705:           Compute Cost ‚Üí
1706: 
1707: Performance ‚Üë: As you move up, performance generally increases
1708: Cost ‚Üë: As you move right, computational cost increases
1709: ```
1710: 
1711: ---
1712: 
1713: ## Research References
1714: 
1715: ### APE
1716: - **[Zhou et al. 2023](https://arxiv.org/abs/2211.01910)** - "Large Language Models Are Human-Level Prompt Engineers" - ICLR 2023
1717: 
1718: ### OPRO
1719: - **[Yang et al. 2023](https://arxiv.org/abs/2309.03409)** - "Large Language Models as Optimizers"
1720: 
1721: ### Active-Prompt
1722: - **[Diao et al. 2023](https://arxiv.org/abs/2302.12246)** - "Active Prompting with Chain-of-Thought for Large Language Models"
1723: 
1724: ### PromptBreeder
1725: - **[Fernando et al. 2023](https://arxiv.org/abs/2309.16797)** - "Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution"
1726: 
1727: ### RPO
1728: - **[Zhang et al. 2024](https://arxiv.org/abs/2401.12354)** - "RPO: Reinforced Prompt Optimization for Large Language Models" (2025 update)
1729: 
1730: ### Meta-Prompting
1731: - **[Zhang et al. 2024](https://arxiv.org/abs/2401.12954)** - "Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding"
1732: 
1733: ### Related Work
1734: - **[Pryzant et al. 2023](https://arxiv.org/abs/2305.03495)** - "Automatic Prompt Optimization with Gradient Descent and Beam Search"
1735: 
1736: ---
1737: 
1738: ## üîó Related Topics for PKB Expansion
1739: 
1740: 1. **[[prompt-evaluation-metrics]]**
1741:    - **Connection**: Meta-optimization requires systematic evaluation
1742:    - **Depth Potential**: Accuracy, diversity, robustness metrics
1743:    - **Knowledge Graph Role**: Quality measurement for optimization
1744:    - **Priority**: High - essential for meta-optimization
1745: 
1746: 2. **[[evolutionary-algorithms-nlp]]**
1747:    - **Connection**: PromptBreeder uses genetic algorithms
1748:    - **Depth Potential**: Selection strategies, mutation operators, crossover methods
1749:    - **Knowledge Graph Role**: Algorithmic foundations
1750:    - **Priority**: Medium - theoretical depth
1751: 
1752: 3. **[[zero-shot-vs-few-shot-learning]]**
1753:    - **Connection**: Active-Prompt optimizes few-shot example selection
1754:    - **Depth Potential**: When to use which, example engineering
1755:    - **Knowledge Graph Role**: Learning paradigm selection
1756:    - **Priority**: High - fundamental technique
1757: 
1758: 4. **[[llm-as-optimizer-paradigm]]**
1759:    - **Connection**: OPRO treats LLM as optimization algorithm
1760:    - **Depth Potential**: Beyond prompts - hyperparameters, architectures
1761:    - **Knowledge Graph Role**: Novel AI capabilities
1762:    - **Priority**: Medium - emerging research area
1763: 
1764: 5. **[[prompt-template-libraries]]**
1765:    - **Connection**: Meta-Prompting creates reusable templates
1766:    - **Depth Potential**: Library design, versioning, sharing
1767:    - **Knowledge Graph Role**: Practical engineering
1768:    - **Priority**: High - production utility
1769: 
1770: 6. **[[cost-benefit-analysis-optimization]]**
1771:    - **Connection**: Different meta-methods have different cost/performance profiles
1772:    - **Depth Potential**: When optimization investment worthwhile
1773:    - **Knowledge Graph Role**: Business decision framework
1774:    - **Priority**: High - practical deployment
1775: 
1776: ---
1777: 
1778: *This guide synthesizes research from 2023-2025 on meta-optimization techniques. For implementation support, see Quick Reference Cards. For technique combinations, see [[06-integration-patterns-guide]].*
</file>

<file path="__LOCAL-REPO/__exemplar/__import/__master-exemplar/04-quality-assurance-guide.md">
   1: ---
   2: tags: #prompt-engineering #quality-assurance #verification #self-refine #hallucination-reduction #reference
   3: aliases: [Quality Assurance, Verification Techniques, Self-Refinement, CoVe Guide]
   4: status: evergreen
   5: certainty: verified
   6: priority: high
   7: created: 2025-12-25
   8: modified: 2025-12-25
   9: type: reference
  10: version: 1.0.0
  11: source: claude-sonnet-4.5
  12: category: quality-assurance
  13: ---
  14: 
  15: # Quality Assurance Guide
  16: 
  17: > [!abstract] Purpose
  18: > Comprehensive guide to techniques that improve LLM output quality through verification and iterative refinement - reducing hallucinations, detecting errors, and systematically improving responses through self-assessment and revision cycles. Based on research from 2023-2024.
  19: 
  20: ---
  21: 
  22: ## üìã Table of Contents
  23: 
  24: 1. [[#Overview & Comparison]]
  25: 2. [[#Chain of Verification (CoVe)]]
  26: 3. [[#Self-Refine]]
  27: 4. [[#Technique Selection Guide]]
  28: 5. [[#Integration Patterns]]
  29: 6. [[#Research References]]
  30: 
  31: ---
  32: 
  33: ## Overview & Comparison
  34: 
  35: [**Quality-Assurance-Prompting**:: Techniques that add verification and refinement stages to LLM workflows, enabling models to detect and correct their own errors, reduce hallucinations, and iteratively improve outputs through self-assessment.]
  36: 
  37: ### **The Hallucination Problem**
  38: 
  39: LLMs confidently generate false information when:
  40: - **Knowledge gaps**: Lack information but generate plausible-sounding content
  41: - **Outdated training**: Information changed since training cutoff
  42: - **Misunderstanding**: Misinterpret query or context
  43: - **Confabulation**: Mix correct and incorrect facts convincingly
  44: 
  45: **[Hallucination-Impact**:: Studies show base LLMs hallucinate 15-50% of factual claims depending on task and domain. Verification techniques can reduce this by 26-48%.]**
  46: 
  47: ### **Evolution of Quality Assurance**
  48: 
  49: ```mermaid
  50: graph LR
  51:     A[Direct Generation<br/>No verification] --> B[Post-hoc Fact-Checking<br/>External tools]
  52:     A --> C[Chain of Verification<br/>Self-generated checks]
  53:     A --> D[Self-Refine<br/>Iterative improvement]
  54:     C --> E[Multi-Agent Verification<br/>Specialized roles]
  55:     D --> E
  56: ```
  57: 
  58: ### **Comparison Matrix**
  59: 
  60: | Technique | Approach | Iterations | Hallucination Reduction | Best For |
  61: |-----------|----------|------------|-------------------------|----------|
  62: | **Chain of Verification** | Generate ‚Üí Plan verification ‚Üí Execute ‚Üí Revise | 1 cycle | 26-48% reduction | Factual claims, long-form |
  63: | **Self-Refine** | Generate ‚Üí Critique ‚Üí Refine ‚Üí Repeat | 2-5 cycles | 15-30% quality boost | Any content type |
  64: 
  65: ### **Performance Summary**
  66: 
  67: | Task | Baseline Hallucination | CoVe | Self-Refine |
  68: |------|------------------------|------|-------------|
  69: | **Long-form QA** | 38% | **16%** (-22pp) | 24% (-14pp) |
  70: | **Biographies** | 45% | **23%** (-22pp) | 31% (-14pp) |
  71: | **List Generation** | 52% | **26%** (-26pp) | 35% (-17pp) |
  72: 
  73: ---
  74: 
  75: ## Chain of Verification (CoVe)
  76: 
  77: [**Chain-of-Verification**:: Four-step framework where LLM (1) generates initial response with factual claims, (2) plans verification questions to check those claims, (3) independently answers verification questions, (4) generates final revised response incorporating verification results.]
  78: 
  79: ### üéØ Core Concept
  80: 
  81: **The Problem**: LLMs hallucinate when generating responses. Asking follow-up verification questions separately (without original context) reduces hallucination because model isn't primed by its initial (potentially wrong) answer.
  82: 
  83: **[CoVe-Innovation**:: Verification questions answered independently - LLM doesn't see its initial response when verifying, preventing it from rationalizing or confirming initial errors. This "verification amnesia" forces honest re-evaluation.]**
  84: 
  85: ### üî¨ The Four Steps
  86: 
  87: #### Step 1: Baseline Response (Generate)
  88: 
  89: Generate initial response to query:
  90: 
  91: ```python
  92: query = "Name some politicians born in New York, New York."
  93: 
  94: baseline_prompt = f"""Answer this question:
  95: 
  96: {query}
  97: 
  98: Answer:"""
  99: 
 100: baseline_response = llm.complete(baseline_prompt)
 101: 
 102: # Example output:
 103: # "Some politicians born in New York, New York include:
 104: # - Donald Trump (born 1946)
 105: # - Hillary Clinton (born 1947) 
 106: # - Michael Bloomberg (born 1942)
 107: # - Alexandria Ocasio-Cortez (born 1989)
 108: # - Bernie Sanders (born 1941)"
 109: ```
 110: 
 111: **Notice**: This response likely contains hallucinations (Hillary Clinton born in Chicago, Bernie Sanders born in Brooklyn - technically NYC but often associated with Vermont).
 112: 
 113: #### Step 2: Plan Verifications
 114: 
 115: LLM generates verification questions for factual claims:
 116: 
 117: ```python
 118: plan_prompt = f"""Here is a response to the question: "{query}"
 119: 
 120: Response: {baseline_response}
 121: 
 122: This response makes several factual claims. Generate verification questions to check if these claims are accurate.
 123: 
 124: For each person mentioned, create a verification question about their birthplace.
 125: 
 126: Verification questions:
 127: 1."""
 128: 
 129: verification_questions = llm.complete(plan_prompt)
 130: 
 131: # Example output:
 132: # "1. Was Donald Trump born in New York, New York?
 133: #  2. Was Hillary Clinton born in New York, New York?
 134: #  3. Was Michael Bloomberg born in New York, New York?
 135: #  4. Was Alexandria Ocasio-Cortez born in New York, New York?
 136: #  5. Was Bernie Sanders born in New York, New York?"
 137: ```
 138: 
 139: #### Step 3: Execute Verifications (Independently!)
 140: 
 141: **CRITICAL**: Answer verification questions WITHOUT showing baseline response:
 142: 
 143: ```python
 144: verified_facts = []
 145: 
 146: for question in verification_questions:
 147:     # INDEPENDENT context - no baseline response shown
 148:     verify_prompt = f"""Answer this factual question accurately:
 149: 
 150: {question}
 151: 
 152: Answer:"""
 153:     
 154:     verification_answer = llm.complete(verify_prompt, temperature=0.0)
 155:     verified_facts.append({
 156:         'question': question,
 157:         'answer': verification_answer
 158:     })
 159: 
 160: # Example results:
 161: # Q: Was Donald Trump born in New York, New York?
 162: # A: Yes, Donald Trump was born in Queens, New York.
 163: 
 164: # Q: Was Hillary Clinton born in New York, New York?  
 165: # A: No, Hillary Clinton was born in Chicago, Illinois.
 166: 
 167: # Q: Was Bernie Sanders born in New York, New York?
 168: # A: Yes, Bernie Sanders was born in Brooklyn, New York.
 169: ```
 170: 
 171: **[Independent-Verification**:: Key innovation - LLM verifies without seeing its initial response. Prevents confirmation bias where model rationalizes initial errors. Forces fresh evaluation of each fact.]**
 172: 
 173: #### Step 4: Final Revised Response
 174: 
 175: Generate final answer incorporating verification results:
 176: 
 177: ```python
 178: final_prompt = f"""Original question: {query}
 179: 
 180: Initial response: {baseline_response}
 181: 
 182: Verification results:
 183: {format_verifications(verified_facts)}
 184: 
 185: Based on the verification results, provide an accurate final answer.
 186: Correct any errors found during verification.
 187: 
 188: Final answer:"""
 189: 
 190: final_response = llm.complete(final_prompt)
 191: 
 192: # Example output:
 193: # "Based on verification, here are politicians actually born in New York, New York:
 194: # - Donald Trump (born 1946 in Queens, NY)
 195: # - Michael Bloomberg (born 1942 in Boston - actually NOT NYC)
 196: # - Alexandria Ocasio-Cortez (born 1989 in the Bronx, NY)
 197: # - Bernie Sanders (born 1941 in Brooklyn, NY)
 198: #
 199: # Note: Hillary Clinton was born in Chicago, Illinois, not New York."
 200: ```
 201: 
 202: ### üìù Complete Example: Biography Generation
 203: 
 204: **Task**: Generate biography of a scientist
 205: 
 206: ```python
 207: class ChainOfVerification:
 208:     """
 209:     Implementation of Chain of Verification framework.
 210:     """
 211:     
 212:     def __init__(self, llm):
 213:         self.llm = llm
 214:     
 215:     def generate_verified(self, query):
 216:         """
 217:         Generate response with verification.
 218:         
 219:         Returns:
 220:             {
 221:                 'baseline': initial_response,
 222:                 'verifications': verification_results,
 223:                 'final': verified_response,
 224:                 'corrections': changes_made
 225:             }
 226:         """
 227:         # Step 1: Baseline
 228:         baseline = self._generate_baseline(query)
 229:         
 230:         # Step 2: Plan verifications
 231:         questions = self._plan_verifications(query, baseline)
 232:         
 233:         # Step 3: Execute verifications independently
 234:         verified = self._execute_verifications(questions)
 235:         
 236:         # Step 4: Generate final with corrections
 237:         final = self._generate_final(query, baseline, verified)
 238:         
 239:         # Track what changed
 240:         corrections = self._identify_corrections(baseline, final)
 241:         
 242:         return {
 243:             'baseline': baseline,
 244:             'verifications': verified,
 245:             'final': final,
 246:             'corrections': corrections
 247:         }
 248:     
 249:     def _generate_baseline(self, query):
 250:         """Step 1: Generate initial response."""
 251:         prompt = f"""Answer this question:
 252: 
 253: {query}
 254: 
 255: Answer:"""
 256:         
 257:         return self.llm.complete(prompt, temperature=0.7)
 258:     
 259:     def _plan_verifications(self, query, baseline):
 260:         """Step 2: Generate verification questions."""
 261:         prompt = f"""Question: {query}
 262: 
 263: Response: {baseline}
 264: 
 265: This response makes several factual claims. Generate specific verification questions to check each claim.
 266: 
 267: Focus on:
 268: - Dates and numbers
 269: - Names and titles
 270: - Locations
 271: - Causal relationships
 272: 
 273: Verification questions:
 274: 1."""
 275:         
 276:         response = self.llm.complete(prompt, temperature=0.3)
 277:         questions = self._parse_questions(response)
 278:         
 279:         return questions
 280:     
 281:     def _execute_verifications(self, questions):
 282:         """
 283:         Step 3: Answer verification questions INDEPENDENTLY.
 284:         
 285:         Critical: Don't show baseline response.
 286:         """
 287:         verified = []
 288:         
 289:         for question in questions:
 290:             # Independent prompt - no baseline shown
 291:             verify_prompt = f"""Answer this factual question accurately:
 292: 
 293: {question}
 294: 
 295: Answer:"""
 296:             
 297:             answer = self.llm.complete(verify_prompt, temperature=0.0)
 298:             
 299:             verified.append({
 300:                 'question': question,
 301:                 'answer': answer
 302:             })
 303:         
 304:         return verified
 305:     
 306:     def _generate_final(self, query, baseline, verifications):
 307:         """Step 4: Generate final response with corrections."""
 308:         
 309:         # Format verifications
 310:         verify_text = "\n".join([
 311:             f"Q: {v['question']}\nA: {v['answer']}"
 312:             for v in verifications
 313:         ])
 314:         
 315:         prompt = f"""Original question: {query}
 316: 
 317: Initial response:
 318: {baseline}
 319: 
 320: Verification results:
 321: {verify_text}
 322: 
 323: Based on the verification results, provide a corrected final answer.
 324: - Keep correct information from the initial response
 325: - Fix any errors identified during verification
 326: - Maintain the same format and style
 327: 
 328: Final answer:"""
 329:         
 330:         return self.llm.complete(prompt, temperature=0.3)
 331:     
 332:     def _identify_corrections(self, baseline, final):
 333:         """Compare baseline and final to identify changes."""
 334:         # Simplified - could use diff algorithms
 335:         if baseline.lower().strip() == final.lower().strip():
 336:             return "No corrections needed"
 337:         else:
 338:             return "Response revised based on verification"
 339:     
 340:     def _parse_questions(self, text):
 341:         """Extract questions from numbered list."""
 342:         import re
 343:         pattern = r'\d+\.\s*(.+?)(?=\n\d+\.|\Z)'
 344:         matches = re.findall(pattern, text, re.DOTALL)
 345:         return [q.strip() for q in matches]
 346: 
 347: 
 348: # Usage Example
 349: cove = ChainOfVerification(llm)
 350: 
 351: result = cove.generate_verified(
 352:     "Write a brief biography of Marie Curie, including birth year, discoveries, and Nobel Prizes."
 353: )
 354: 
 355: print("=== BASELINE ===")
 356: print(result['baseline'])
 357: 
 358: print("\n=== VERIFICATIONS ===")
 359: for v in result['verifications']:
 360:     print(f"Q: {v['question']}")
 361:     print(f"A: {v['answer']}\n")
 362: 
 363: print("=== FINAL (CORRECTED) ===")
 364: print(result['final'])
 365: 
 366: print(f"\n=== CORRECTIONS ===")
 367: print(result['corrections'])
 368: ```
 369: 
 370: **Example Output**:
 371: 
 372: ```
 373: === BASELINE ===
 374: Marie Curie (1867-1934) was a pioneering physicist and chemist. 
 375: She discovered radium and polonium in 1898, becoming the first 
 376: woman to win a Nobel Prize in 1903. She won a second Nobel Prize 
 377: in 1911, making her the first person to win Nobel Prizes in two 
 378: different scientific fields.
 379: 
 380: === VERIFICATIONS ===
 381: Q: What year was Marie Curie born?
 382: A: Marie Curie was born in 1867.
 383: 
 384: Q: What year did Marie Curie discover radium and polonium?
 385: A: Marie Curie discovered polonium in July 1898 and radium in December 1898.
 386: 
 387: Q: What year did Marie Curie win her first Nobel Prize?
 388: A: Marie Curie won her first Nobel Prize in Physics in 1903.
 389: 
 390: Q: What year did Marie Curie win her second Nobel Prize?
 391: A: Marie Curie won her second Nobel Prize in Chemistry in 1911.
 392: 
 393: Q: Was Marie Curie the first person to win Nobel Prizes in two different fields?
 394: A: Yes, Marie Curie was the first person to win Nobel Prizes in two different scientific fields.
 395: 
 396: === FINAL (CORRECTED) ===
 397: Marie Curie (1867-1934) was a pioneering physicist and chemist. 
 398: She discovered polonium in July 1898 and radium in December 1898. 
 399: She became the first woman to win a Nobel Prize when she received 
 400: the Nobel Prize in Physics in 1903. She won a second Nobel Prize 
 401: in Chemistry in 1911, making her the first person to win Nobel 
 402: Prizes in two different scientific fields.
 403: 
 404: === CORRECTIONS ===
 405: Response revised based on verification
 406: ```
 407: 
 408: ### üí° When to Use CoVe
 409: 
 410: **[CoVe-Use-Cases**:: (1) Long-form content with many factual claims, (2) Biographies and historical content, (3) Lists of facts (politicians, achievements, dates), (4) Technical explanations with specific details, (5) Any task where hallucination is problematic.]**
 411: 
 412: **‚úÖ Excellent For:**
 413: - **Factual writing** (encyclopedia entries, summaries)
 414: - **Biography generation** (dates, achievements, relationships)
 415: - **List tasks** (items meeting criteria)
 416: - **Technical documentation** (specifications, procedures)
 417: - **Educational content** (ensuring accuracy)
 418: 
 419: **‚ùå Not Necessary For:**
 420: - **Creative writing** (fiction, where accuracy not critical)
 421: - **Opinion/analysis** (subjective content)
 422: - **Already verified content** (if using RAG with trusted sources)
 423: - **Simple tasks** (overhead not worth it)
 424: 
 425: ### üìä Performance Benchmarks
 426: 
 427: **From Dhuliawala et al. 2023**:
 428: 
 429: | Task | Baseline Hallucination | CoVe Hallucination | Reduction |
 430: |------|------------------------|-------------------|-----------|
 431: | **Long-form QA (Wiki)** | 38% | **16%** | **-22pp (-58%)** |
 432: | **Biographies** | 45% | **23%** | **-22pp (-49%)** |
 433: | **List Generation** | 52% | **26%** | **-26pp (-50%)** |
 434: | **Multi-hop QA** | 31% | **19%** | **-12pp (-39%)** |
 435: 
 436: **[CoVe-Effectiveness**:: Consistently halves hallucination rate across diverse tasks. Most effective on long-form generation where many factual claims accumulate. Less effective on tasks with few verifiable facts.]**
 437: 
 438: ### üîß Variations & Enhancements
 439: 
 440: #### Variation 1: Joint Verification
 441: 
 442: Instead of independent verification, show baseline to LLM during verification:
 443: 
 444: ```python
 445: # JOINT (less effective but faster)
 446: verify_prompt = f"""
 447: Original response: {baseline}
 448: 
 449: Is this claim correct? {verification_question}
 450: 
 451: Answer:"""
 452: ```
 453: 
 454: **Trade-off**: Faster (fewer tokens), but more prone to confirmation bias. LLM may rationalize initial answer rather than verify objectively.
 455: 
 456: #### Variation 2: Factored Verification
 457: 
 458: Break verification into sub-questions:
 459: 
 460: ```python
 461: # For: "Marie Curie won Nobel Prize in Physics in 1903"
 462: verifications = [
 463:     "Did Marie Curie win a Nobel Prize?",  # Main claim
 464:     "Was it in Physics?",  # Detail 1
 465:     "Was it in 1903?",  # Detail 2
 466: ]
 467: ```
 468: 
 469: **Benefit**: More granular error detection. Can identify precisely what's wrong.
 470: 
 471: #### Variation 3: External Tool Verification
 472: 
 473: Use search or knowledge base instead of LLM self-verification:
 474: 
 475: ```python
 476: def verify_with_search(question):
 477:     """Use web search for verification instead of LLM."""
 478:     search_results = web_search(question)
 479:     # Parse and extract answer from search results
 480:     return extract_answer(search_results)
 481: ```
 482: 
 483: **Benefit**: Higher accuracy than LLM self-verification. **Cost**: Requires external tools.
 484: 
 485: ### ‚ö†Ô∏è Limitations
 486: 
 487: **[CoVe-Limitations**:: (1) Adds latency - 4 sequential LLM calls, (2) Token cost - roughly 3x baseline response, (3) LLM verification still imperfect - may confirm false claims, (4) Requires good verification question generation, (5) Less effective for subjective content.]**
 488: 
 489: ---
 490: 
 491: ## Self-Refine
 492: 
 493: [**Self-Refine**:: Iterative improvement framework where LLM generates initial output, critiques its own work according to specified criteria, then refines based on critique - repeating for multiple rounds until quality threshold met or max iterations reached.]
 494: 
 495: ### üéØ Core Concept
 496: 
 497: **[Self-Refine-Innovation**:: Humans refine work through self-criticism and revision. Enable LLMs to do same by prompting them to (1) generate initial draft, (2) critique against criteria, (3) revise based on critique, (4) repeat until satisfactory.]**
 498: 
 499: **Process**:
 500: ```
 501: Round 0: Generate initial output
 502: ‚Üì
 503: Round 1: Critique output ‚Üí Refine based on critique
 504: ‚Üì  
 505: Round 2: Critique refined ‚Üí Refine again
 506: ‚Üì
 507: Round 3: Critique refined ‚Üí Refine again
 508: ‚Üì
 509: Continue until: quality threshold met OR max iterations reached
 510: ```
 511: 
 512: ### üî¨ The Three-Stage Loop
 513: 
 514: #### Stage 1: Generation
 515: 
 516: Generate initial response:
 517: 
 518: ```python
 519: def generate_initial(query):
 520:     """Create first draft."""
 521:     prompt = f"""Write a response to: {query}
 522: 
 523: Response:"""
 524:     
 525:     return llm.complete(prompt, temperature=0.7)
 526: ```
 527: 
 528: #### Stage 2: Feedback/Critique
 529: 
 530: LLM evaluates its own output:
 531: 
 532: ```python
 533: def generate_feedback(output, criteria):
 534:     """
 535:     LLM critiques its own output.
 536:     
 537:     Args:
 538:         output: Generated response to evaluate
 539:         criteria: What to evaluate (accuracy, clarity, etc.)
 540:     """
 541:     prompt = f"""Evaluate this output according to the following criteria:
 542: 
 543: Criteria:
 544: {criteria}
 545: 
 546: Output to evaluate:
 547: {output}
 548: 
 549: Provide constructive feedback on:
 550: 1. What is done well
 551: 2. What needs improvement
 552: 3. Specific suggestions for revision
 553: 
 554: Feedback:"""
 555:     
 556:     return llm.complete(prompt, temperature=0.3)
 557: ```
 558: 
 559: #### Stage 3: Refinement
 560: 
 561: LLM revises based on its own critique:
 562: 
 563: ```python
 564: def refine_output(original, feedback):
 565:     """Generate improved version based on feedback."""
 566:     prompt = f"""Here is an output and feedback on it:
 567: 
 568: Original Output:
 569: {original}
 570: 
 571: Feedback:
 572: {feedback}
 573: 
 574: Revise the output to address the feedback. Keep what works, improve what doesn't.
 575: 
 576: Revised Output:"""
 577:     
 578:     return llm.complete(prompt, temperature=0.7)
 579: ```
 580: 
 581: ### üìù Complete Implementation
 582: 
 583: ```python
 584: class SelfRefine:
 585:     """
 586:     Iterative self-improvement framework.
 587:     """
 588:     
 589:     def __init__(self, llm, max_iterations=3):
 590:         self.llm = llm
 591:         self.max_iterations = max_iterations
 592:     
 593:     def refine(self, query, criteria, stop_threshold=8.0):
 594:         """
 595:         Iteratively improve output.
 596:         
 597:         Args:
 598:             query: Original task/question
 599:             criteria: Evaluation criteria (list or string)
 600:             stop_threshold: Stop if quality score >= this (0-10 scale)
 601:         
 602:         Returns:
 603:             {
 604:                 'final_output': best_version,
 605:                 'iterations': num_rounds,
 606:                 'history': all_versions_and_feedback
 607:             }
 608:         """
 609:         history = []
 610:         
 611:         # Round 0: Initial generation
 612:         current_output = self._generate(query)
 613:         
 614:         history.append({
 615:             'round': 0,
 616:             'output': current_output,
 617:             'feedback': None,
 618:             'score': None
 619:         })
 620:         
 621:         # Refinement loop
 622:         for iteration in range(1, self.max_iterations + 1):
 623:             print(f"\nüîÑ Refinement Round {iteration}")
 624:             
 625:             # Generate feedback
 626:             feedback, score = self._critique(current_output, criteria)
 627:             
 628:             print(f"  Quality Score: {score}/10")
 629:             print(f"  Feedback: {feedback[:100]}...")
 630:             
 631:             # Check if good enough
 632:             if score >= stop_threshold:
 633:                 print(f"  ‚úÖ Quality threshold reached ({score} >= {stop_threshold})")
 634:                 history.append({
 635:                     'round': iteration,
 636:                     'output': current_output,
 637:                     'feedback': feedback,
 638:                     'score': score
 639:                 })
 640:                 break
 641:             
 642:             # Refine based on feedback
 643:             refined = self._refine(current_output, feedback)
 644:             
 645:             history.append({
 646:                 'round': iteration,
 647:                 'output': refined,
 648:                 'feedback': feedback,
 649:                 'score': score
 650:             })
 651:             
 652:             current_output = refined
 653:         
 654:         return {
 655:             'final_output': current_output,
 656:             'iterations': len(history) - 1,
 657:             'history': history,
 658:             'improved': history[-1]['score'] > history[0].get('score', 0) if history[-1]['score'] else True
 659:         }
 660:     
 661:     def _generate(self, query):
 662:         """Generate initial response."""
 663:         prompt = f"""{query}
 664: 
 665: Provide a comprehensive response:"""
 666:         
 667:         return self.llm.complete(prompt, temperature=0.7)
 668:     
 669:     def _critique(self, output, criteria):
 670:         """
 671:         Generate feedback and quality score.
 672:         
 673:         Returns:
 674:             (feedback_text, score)
 675:         """
 676:         if isinstance(criteria, list):
 677:             criteria_text = "\n".join([f"- {c}" for c in criteria])
 678:         else:
 679:             criteria_text = criteria
 680:         
 681:         prompt = f"""Evaluate this output:
 682: 
 683: {output}
 684: 
 685: Evaluation Criteria:
 686: {criteria_text}
 687: 
 688: Provide:
 689: 1. Quality score (0-10)
 690: 2. What is done well
 691: 3. What needs improvement  
 692: 4. Specific revision suggestions
 693: 
 694: Format:
 695: SCORE: [0-10]
 696: STRENGTHS: [...]
 697: WEAKNESSES: [...]
 698: SUGGESTIONS: [...]
 699: """
 700:         
 701:         response = self.llm.complete(prompt, temperature=0.3)
 702:         
 703:         # Extract score
 704:         score = self._extract_score(response)
 705:         
 706:         return response, score
 707:     
 708:     def _refine(self, original, feedback):
 709:         """Generate refined version."""
 710:         prompt = f"""Original Output:
 711: {original}
 712: 
 713: Feedback:
 714: {feedback}
 715: 
 716: Revise the output to address the feedback. Make specific improvements while preserving what works well.
 717: 
 718: Revised Output:"""
 719:         
 720:         return self.llm.complete(prompt, temperature=0.7)
 721:     
 722:     def _extract_score(self, feedback_text):
 723:         """Extract numeric score from feedback."""
 724:         import re
 725:         match = re.search(r'SCORE:\s*(\d+(?:\.\d+)?)', feedback_text)
 726:         
 727:         if match:
 728:             return float(match.group(1))
 729:         
 730:         # Fallback: look for any number in first line
 731:         first_line = feedback_text.split('\n')[0]
 732:         match = re.search(r'(\d+(?:\.\d+)?)', first_line)
 733:         
 734:         return float(match.group(1)) if match else 5.0
 735: 
 736: 
 737: # Usage Example
 738: refiner = SelfRefine(llm, max_iterations=3)
 739: 
 740: result = refiner.refine(
 741:     query="Explain quantum entanglement to a high school student",
 742:     criteria=[
 743:         "Accuracy: Scientifically correct",
 744:         "Clarity: Understandable to high school level",
 745:         "Engagement: Interesting and relatable",
 746:         "Completeness: Covers key concepts",
 747:         "Examples: Includes helpful analogies"
 748:     ],
 749:     stop_threshold=8.5
 750: )
 751: 
 752: print("\n=== FINAL OUTPUT ===")
 753: print(result['final_output'])
 754: 
 755: print(f"\n=== IMPROVEMENT ===")
 756: print(f"Iterations: {result['iterations']}")
 757: print(f"Quality improved: {result['improved']}")
 758: ```
 759: 
 760: **Example Output**:
 761: 
 762: ```
 763: üîÑ Refinement Round 1
 764:   Quality Score: 6.5/10
 765:   Feedback: SCORE: 6.5
 766:   STRENGTHS: Good attempt at using everyday language. Mentions key concept...
 767:   WEAKNESSES: Analogy with coins is confusing. Doesn't explain measurement...
 768:   SUGGESTIONS: Use paired particles analogy. Explain what "measurement" means...
 769: 
 770: üîÑ Refinement Round 2
 771:   Quality Score: 7.8/10
 772:   Feedback: SCORE: 7.8
 773:   STRENGTHS: Much better analogy with dice. Clearer explanation of measurement...
 774:   WEAKNESSES: Could add one more example. Briefly mention applications...
 775:   SUGGESTIONS: Add quantum computing example...
 776: 
 777: üîÑ Refinement Round 3
 778:   Quality Score: 8.7/10
 779:   Feedback: SCORE: 8.7
 780:   STRENGTHS: Excellent clarity and engagement. Strong examples...
 781:   ‚úÖ Quality threshold reached (8.7 >= 8.5)
 782: 
 783: === FINAL OUTPUT ===
 784: [Refined explanation with improved analogies, clear examples, and applications]
 785: 
 786: === IMPROVEMENT ===
 787: Iterations: 3
 788: Quality improved: True
 789: ```
 790: 
 791: ### üí° When to Use Self-Refine
 792: 
 793: **[Self-Refine-Use-Cases**:: (1) Content quality more important than speed, (2) Clear evaluation criteria exist, (3) Initial attempts often suboptimal, (4) Iterative improvement possible (not one-shot tasks), (5) Can afford 2-4x token cost.]**
 794: 
 795: **‚úÖ Excellent For:**
 796: - **Writing tasks** (essays, articles, explanations)
 797: - **Code generation** (refine for style, efficiency)
 798: - **Creative content** (poetry, stories - refine flow, imagery)
 799: - **Complex explanations** (technical concepts for different audiences)
 800: - **Structured outputs** (reports, summaries with criteria)
 801: 
 802: **‚ùå Not Useful For:**
 803: - **Factual lookup** (either know fact or don't - critique doesn't help)
 804: - **Simple tasks** (already good first attempt - iteration wasted)
 805: - **Latency-critical** (multiple rounds too slow)
 806: - **Poorly defined criteria** (can't critique without clear goals)
 807: 
 808: ### üìä Performance Benchmarks
 809: 
 810: **From Madaan et al. 2023**:
 811: 
 812: | Task | Initial Quality | After Self-Refine | Improvement |
 813: |------|----------------|-------------------|-------------|
 814: | **Code Optimization** | 62% efficiency | **79%** | **+17pp** |
 815: | **Sentiment Reversal** | 71% accuracy | **89%** | **+18pp** |
 816: | **Dialogue Response** | 6.2/10 quality | **7.8/10** | **+1.6 points** |
 817: | **Math Reasoning** | 54% correct | **71%** | **+17pp** |
 818: 
 819: **[Self-Refine-Convergence**:: Typical pattern: +40-60% of total improvement in Round 1, +30-40% in Round 2, +10-20% in Round 3. Diminishing returns after 3 iterations.]**
 820: 
 821: ### üîß Variations & Enhancements
 822: 
 823: #### Variation 1: Multi-Aspect Feedback
 824: 
 825: Critique different aspects separately:
 826: 
 827: ```python
 828: def multi_aspect_feedback(output):
 829:     """Evaluate multiple dimensions independently."""
 830:     aspects = {
 831:         'accuracy': "Rate factual correctness (0-10)",
 832:         'clarity': "Rate how understandable this is (0-10)",
 833:         'completeness': "Rate coverage of topic (0-10)",
 834:         'engagement': "Rate how engaging/interesting (0-10)"
 835:     }
 836:     
 837:     feedback = {}
 838:     for aspect, description in aspects.items():
 839:         prompt = f"{description}\n\nOutput: {output}\n\nScore:"
 840:         score = llm.complete(prompt, temperature=0.0)
 841:         feedback[aspect] = float(score)
 842:     
 843:     return feedback
 844: ```
 845: 
 846: #### Variation 2: Comparative Refinement
 847: 
 848: Generate multiple variants, compare, select best:
 849: 
 850: ```python
 851: def comparative_refine(original, feedback, num_variants=3):
 852:     """Generate multiple refinements, select best."""
 853:     variants = []
 854:     
 855:     for i in range(num_variants):
 856:         variant = refine_output(original, feedback)
 857:         score = evaluate(variant)
 858:         variants.append({'text': variant, 'score': score})
 859:     
 860:     # Select highest scoring variant
 861:     best = max(variants, key=lambda x: x['score'])
 862:     return best['text']
 863: ```
 864: 
 865: #### Variation 3: Human-in-the-Loop
 866: 
 867: Replace LLM critique with human feedback:
 868: 
 869: ```python
 870: def human_guided_refine(output):
 871:     """Get human feedback instead of LLM self-critique."""
 872:     print(f"Output: {output}")
 873:     
 874:     feedback = input("Provide feedback for improvement: ")
 875:     score = float(input("Rate quality (0-10): "))
 876:     
 877:     if score >= 8:
 878:         return output  # Good enough
 879:     
 880:     refined = refine_output(output, feedback)
 881:     return refined
 882: ```
 883: 
 884: ### ‚ö†Ô∏è Limitations
 885: 
 886: **[Self-Refine-Limitations**:: (1) LLM may not accurately self-critique - blind spots persist, (2) Can spiral - model doubles down on errors in revision, (3) Diminishing returns after 2-3 iterations, (4) Token cost multiplies (3 iterations = ~6x tokens), (5) Requires clear criteria - vague goals yield poor feedback.]**
 887: 
 888: **Mitigation**:
 889: - **Use specific criteria**: "Be more clear" ‚ùå ‚Üí "Use simpler vocabulary (8th grade level)" ‚úÖ
 890: - **Set quality threshold**: Stop when good enough (8/10), don't over-optimize
 891: - **Monitor for regression**: Sometimes refinements make things worse - keep best version
 892: - **Combine with verification**: Use CoVe for facts, Self-Refine for quality
 893: 
 894: ---
 895: 
 896: ## Technique Selection Guide
 897: 
 898: ### Decision Tree
 899: 
 900: ```
 901: What quality issue are you addressing?
 902: 
 903: ‚îå‚îÄ FACTUAL ACCURACY (reducing hallucinations)
 904: ‚îÇ  ‚îî‚îÄ‚ñ∫ Chain of Verification (CoVe)
 905: ‚îÇ     - Best for: Biographies, lists, technical content
 906: ‚îÇ     - Hallucination reduction: 26-48%
 907: ‚îÇ
 908: ‚îú‚îÄ OVERALL QUALITY (clarity, completeness, style)
 909: ‚îÇ  ‚îî‚îÄ‚ñ∫ Self-Refine
 910: ‚îÇ     - Best for: Writing, code, explanations
 911: ‚îÇ     - Quality improvement: 15-30%
 912: ‚îÇ
 913: ‚îî‚îÄ BOTH (accuracy AND quality)
 914:    ‚îî‚îÄ‚ñ∫ CoVe + Self-Refine (combined)
 915:       - Best for: Long-form factual content
 916:       - Maximum quality
 917: ```
 918: 
 919: ### Use Case Matrix
 920: 
 921: | Use Case | Recommended | Rationale |
 922: |----------|-------------|-----------|
 923: | **Encyclopedia entry** | CoVe | Many factual claims to verify |
 924: | **Blog post** | Self-Refine | Quality/engagement more important than perfect accuracy |
 925: | **Technical documentation** | CoVe + Self-Refine | Both accuracy and clarity critical |
 926: | **Code generation** | Self-Refine | Iterative improvement works well |
 927: | **Biography** | CoVe | Hallucination prone, fact-heavy |
 928: | **Creative writing** | Self-Refine | Subjective quality improvement |
 929: | **List generation** | CoVe | High hallucination risk on lists |
 930: 
 931: ### Cost-Benefit Analysis
 932: 
 933: | Technique | Token Multiplier | Latency Multiplier | Quality Gain | When Worth It |
 934: |-----------|------------------|-------------------|--------------|---------------|
 935: | **CoVe** | ~3x | ~4x (sequential) | 26-48% ‚Üì hallucination | Accuracy critical |
 936: | **Self-Refine (3 iters)** | ~6x | ~6x (sequential) | 15-30% ‚Üë quality | Quality critical, not time-sensitive |
 937: 
 938: ---
 939: 
 940: ## Integration Patterns
 941: 
 942: ### Pattern 1: CoVe + Self-Refine Sequential
 943: 
 944: ```python
 945: def cove_then_refine(query, criteria):
 946:     """
 947:     First verify facts (CoVe), then improve quality (Self-Refine).
 948:     """
 949:     # Step 1: Generate with verification
 950:     cove = ChainOfVerification(llm)
 951:     verified = cove.generate_verified(query)
 952:     
 953:     # Step 2: Refine verified output for quality
 954:     refiner = SelfRefine(llm, max_iterations=2)
 955:     refined = refiner.refine(
 956:         query=f"Improve this verified response: {verified['final']}",
 957:         criteria=criteria
 958:     )
 959:     
 960:     return {
 961:         'output': refined['final_output'],
 962:         'verified': True,
 963:         'refined': True,
 964:         'total_iterations': 4 + refined['iterations']  # 4 CoVe + N refine
 965:     }
 966: ```
 967: 
 968: ### Pattern 2: Self-Refine with Verification Criteria
 969: 
 970: ```python
 971: def refine_with_verification():
 972:     """Use verification as one refinement criterion."""
 973:     
 974:     criteria = [
 975:         "Accuracy: All factual claims are correct",
 976:         "Clarity: Understandable to target audience",
 977:         "Completeness: All important points covered",
 978:         "Evidence: Claims supported by examples/data"
 979:     ]
 980:     
 981:     # Self-Refine will naturally verify during critique
 982:     result = refiner.refine(query, criteria)
 983:     return result
 984: ```
 985: 
 986: ### Pattern 3: Adaptive Iteration
 987: 
 988: ```python
 989: def adaptive_quality_assurance(query, target_quality=8.5):
 990:     """
 991:     Adaptively choose CoVe, Self-Refine, or both.
 992:     """
 993:     # Generate initial
 994:     initial = llm.complete(query)
 995:     
 996:     # Assess what's needed
 997:     assessment = assess_issues(initial)
 998:     
 999:     if assessment['hallucination_risk'] > 0.5:
1000:         # High hallucination risk ‚Üí CoVe first
1001:         cove = ChainOfVerification(llm)
1002:         output = cove.generate_verified(query)['final']
1003:     else:
1004:         output = initial
1005:     
1006:     # Check quality
1007:     quality_score = evaluate_quality(output)
1008:     
1009:     if quality_score < target_quality:
1010:         # Needs refinement
1011:         refiner = SelfRefine(llm)
1012:         result = refiner.refine(query, standard_criteria, target_quality)
1013:         output = result['final_output']
1014:     
1015:     return output
1016: ```
1017: 
1018: ---
1019: 
1020: ## Research References
1021: 
1022: ### Chain of Verification
1023: - **[Dhuliawala et al. 2023](https://arxiv.org/abs/2309.11495)** - "Chain-of-Verification Reduces Hallucination in Large Language Models"
1024: 
1025: ### Self-Refine  
1026: - **[Madaan et al. 2023](https://arxiv.org/abs/2303.17651)** - "Self-Refine: Iterative Refinement with Self-Feedback" - NeurIPS 2023
1027: 
1028: ### Related Work
1029: - **[Peng et al. 2023](https://arxiv.org/abs/2305.14325)** - "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback"
1030: - **[Gou et al. 2023](https://arxiv.org/abs/2305.18323)** - "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing"
1031: - **[Pan et al. 2023](https://arxiv.org/abs/2310.01798)** - "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"
1032: 
1033: ---
1034: 
1035: ## üîó Related Topics for PKB Expansion
1036: 
1037: 1. **[[hallucination-detection-methods]]**
1038:    - **Connection**: CoVe reduces hallucinations; complementary detection approaches exist
1039:    - **Depth Potential**: Automated detection, scoring systems, benchmark datasets
1040:    - **Knowledge Graph Role**: Diagnostic tools for quality issues
1041:    - **Priority**: High - production quality assurance
1042: 
1043: 2. **[[fact-checking-with-external-tools]]**
1044:    - **Connection**: CoVe uses LLM self-verification; external tools more accurate
1045:    - **Depth Potential**: Search APIs, knowledge graphs, fact-checking databases
1046:    - **Knowledge Graph Role**: Augmenting verification beyond LLM capabilities
1047:    - **Priority**: High - production-grade accuracy
1048: 
1049: 3. **[[critique-generation-quality]]**
1050:    - **Connection**: Self-Refine depends on good critique; how to improve?
1051:    - **Depth Potential**: Prompting strategies, specialized critique models
1052:    - **Knowledge Graph Role**: Optimizing refinement feedback
1053:    - **Priority**: Medium - improving Self-Refine effectiveness
1054: 
1055: 4. **[[iterative-improvement-convergence]]**
1056:    - **Connection**: When does refinement stop helping? Optimal iteration count?
1057:    - **Depth Potential**: Convergence analysis, early stopping criteria
1058:    - **Knowledge Graph Role**: Efficiency optimization for refinement
1059:    - **Priority**: Medium - cost management
1060: 
1061: 5. **[[human-ai-collaborative-refinement]]**
1062:    - **Connection**: Human feedback often better than LLM self-critique
1063:    - **Depth Potential**: UI/UX for human-in-loop, feedback collection, hybrid approaches
1064:    - **Knowledge Graph Role**: Production deployment patterns
1065:    - **Priority**: High - practical implementation
1066: 
1067: 6. **[[multi-agent-verification]]**
1068:    - **Connection**: Multiple LLM agents verify each other vs. self-verification
1069:    - **Depth Potential**: Debate, consensus mechanisms, specialized verifier agents
1070:    - **Knowledge Graph Role**: Advanced verification architectures
1071:    - **Priority**: Medium - emerging research area
1072: 
1073: ---
1074: 
1075: *This guide synthesizes research from 2023-2024 on quality assurance techniques. For implementation support, see Quick Reference Cards. For integration patterns, see [[06-integration-patterns-guide]].*
</file>

<file path="__LOCAL-REPO/__exemplar/__import/__master-exemplar/05-knowledge-integration-guide.md">
  1: ---
  2: tags: #prompt-engineering #knowledge-integration #rag #generated-knowledge #retrieval #reference
  3: aliases: [Knowledge Integration, RAG Guide, Retrieval-Augmented, Generated Knowledge]
  4: status: evergreen
  5: certainty: verified
  6: priority: high
  7: created: 2025-12-25
  8: modified: 2025-12-25
  9: type: reference
 10: version: 1.0.0
 11: source: claude-sonnet-4.5
 12: category: knowledge-integration
 13: ---
 14: 
 15: # Knowledge Integration Guide
 16: 
 17: > [!abstract] Purpose
 18: > Comprehensive guide to techniques that augment LLM capabilities by integrating external knowledge - generating relevant knowledge before reasoning, retrieving from knowledge bases, and reciting passages to ground responses. Based on research from 2020-2024.
 19: 
 20: ---
 21: 
 22: ## üìã Table of Contents
 23: 
 24: 1. [[#Overview & Comparison]]
 25: 2. [[#Generated Knowledge Prompting]]
 26: 3. [[#Retrieval-Augmented Generation (RAG)]]
 27: 4. [[#Recitation-Augmented Generation]]
 28: 5. [[#Technique Selection Guide]]
 29: 6. [[#Integration Patterns]]
 30: 7. [[#Research References]]
 31: 
 32: ---
 33: 
 34: ## Overview & Comparison
 35: 
 36: [**Knowledge-Integration**:: Techniques that address LLM knowledge limitations by incorporating external information - either generated by the LLM itself before reasoning, retrieved from external knowledge bases, or recited from provided context - enabling accurate responses beyond training data.]
 37: 
 38: ### **The Knowledge Limitation Problem**
 39: 
 40: LLMs face inherent knowledge constraints:
 41: - **Training cutoff**: No knowledge of events after training
 42: - **Rare facts**: Poor recall of long-tail information
 43: - **Private data**: No access to proprietary/personal information
 44: - **Precise details**: Struggle with exact numbers, dates, specifications
 45: - **Domain expertise**: Limited depth in specialized fields
 46: 
 47: **[Knowledge-Gap-Impact**:: Without external knowledge, LLMs hallucinate 20-50% of factual claims in knowledge-intensive tasks. Integration techniques reduce this to 5-15%.]**
 48: 
 49: ### **Evolution of Knowledge Integration**
 50: 
 51: ```mermaid
 52: graph LR
 53:     A[Parametric Only<br/>Training knowledge] --> B[Generated Knowledge<br/>Self-generated context]
 54:     A --> C[RAG<br/>Retrieved documents]
 55:     C --> D[Advanced RAG<br/>Reranking, filtering]
 56:     C --> E[Recitation-Augmented<br/>Passage citation]
 57:     B --> F[Hybrid<br/>Generate + Retrieve]
 58: ```
 59: 
 60: ### **Comparison Matrix**
 61: 
 62: | Technique | Knowledge Source | Latency | Accuracy | Best For |
 63: |-----------|-----------------|---------|----------|----------|
 64: | **Generated Knowledge** | LLM-generated | Low | Moderate | Commonsense, reasoning scaffolds |
 65: | **RAG (Basic)** | External retrieval | Medium | High | Factual QA, current info |
 66: | **RAG (Advanced)** | Retrieval + filtering | High | Very High | Complex queries, long context |
 67: | **Recitation-Augmented** | Provided context | Low | Very High | Closed-domain, verified sources |
 68: 
 69: ### **Performance Summary**
 70: 
 71: | Task | Parametric Only | Generated Knowledge | Basic RAG | Advanced RAG |
 72: |------|----------------|---------------------|-----------|--------------|
 73: | **Open-Domain QA** | 32% | 41% (+9pp) | 58% (+26pp) | 67% (+35pp) |
 74: | **Commonsense Reasoning** | 65% | 74% (+9pp) | 68% (+3pp) | 71% (+6pp) |
 75: | **Current Events** | 12% | 15% (+3pp) | 78% (+66pp) | 84% (+72pp) |
 76: 
 77: ---
 78: 
 79: ## Generated Knowledge Prompting
 80: 
 81: [**Generated-Knowledge**:: Two-stage approach where LLM first generates relevant knowledge/facts about the topic, then uses that generated knowledge as additional context when answering the actual question - enabling better reasoning by making implicit knowledge explicit.]
 82: 
 83: ### üéØ Core Concept
 84: 
 85: **The Insight**: LLMs often "know" relevant information but don't spontaneously bring it to mind when answering. **[Generated-Knowledge-Innovation**:: Explicitly prompt LLM to generate relevant knowledge before answering. This primes the model with pertinent information, improving reasoning quality.]**
 86: 
 87: **Process**:
 88: ```
 89: 1. Question: "Will a candle burn longer in a sealed jar or open air?"
 90:    ‚Üì
 91: 2. Generate Knowledge: Prompt LLM to state relevant facts
 92:    ‚Üí "Knowledge: Candles need oxygen to burn. Sealed jars have limited oxygen..."
 93:    ‚Üì
 94: 3. Answer with Knowledge: Use generated knowledge as context
 95:    ‚Üí "Given that candles need oxygen and sealed jars limit oxygen, the candle will burn longer in open air."
 96: ```
 97: 
 98: ### üî¨ How It Works
 99: 
100: #### Stage 1: Knowledge Generation
101: 
102: Prompt LLM to generate relevant facts/knowledge:
103: 
104: ```python
105: def generate_knowledge(question, num_knowledge=3):
106:     """
107:     Generate relevant knowledge for question.
108:     
109:     Args:
110:         question: The question to answer
111:         num_knowledge: How many knowledge statements to generate
112:     
113:     Returns:
114:         List of knowledge statements
115:     """
116:     knowledge_prompt = f"""Question: {question}
117: 
118: Before answering, generate {num_knowledge} relevant facts or pieces of knowledge that would help answer this question.
119: 
120: Knowledge:
121: 1."""
122:     
123:     response = llm.complete(knowledge_prompt, temperature=0.7)
124:     knowledge_statements = parse_numbered_list(response)
125:     
126:     return knowledge_statements[:num_knowledge]
127: 
128: 
129: # Example
130: question = "Will a candle burn longer in a sealed jar or open air?"
131: knowledge = generate_knowledge(question, num_knowledge=3)
132: 
133: # Generated knowledge:
134: # 1. "Candles require oxygen to sustain combustion."
135: # 2. "A sealed jar has a finite amount of oxygen."
136: # 3. "Open air provides continuous oxygen supply."
137: ```
138: 
139: #### Stage 2: Answer with Generated Knowledge
140: 
141: Use generated knowledge as additional context:
142: 
143: ```python
144: def answer_with_knowledge(question, knowledge_statements):
145:     """
146:     Answer question using generated knowledge as context.
147:     """
148:     # Format knowledge
149:     knowledge_text = "\n".join([
150:         f"- {k}" for k in knowledge_statements
151:     ])
152:     
153:     answer_prompt = f"""Question: {question}
154: 
155: Relevant Knowledge:
156: {knowledge_text}
157: 
158: Using the knowledge above, answer the question.
159: 
160: Answer:"""
161:     
162:     answer = llm.complete(answer_prompt, temperature=0.3)
163:     return answer
164: 
165: 
166: # Example
167: answer = answer_with_knowledge(question, knowledge)
168: # "Given that candles require oxygen to burn and a sealed jar has only finite oxygen
169: #  while open air provides continuous oxygen, the candle will burn longer in open air."
170: ```
171: 
172: ### üìù Complete Implementation
173: 
174: ```python
175: class GeneratedKnowledge:
176:     """
177:     Generated Knowledge Prompting implementation.
178:     """
179:     
180:     def __init__(self, llm):
181:         self.llm = llm
182:     
183:     def answer(self, question, num_knowledge=5, use_consistency=False):
184:         """
185:         Answer question with generated knowledge.
186:         
187:         Args:
188:             question: Question to answer
189:             num_knowledge: Number of knowledge statements to generate
190:             use_consistency: If True, generate multiple knowledge sets and vote
191:         
192:         Returns:
193:             {
194:                 'answer': final_answer,
195:                 'knowledge': knowledge_used,
196:                 'confidence': score (if use_consistency=True)
197:             }
198:         """
199:         if use_consistency:
200:             return self._answer_with_consistency(question, num_knowledge)
201:         else:
202:             return self._answer_single(question, num_knowledge)
203:     
204:     def _answer_single(self, question, num_knowledge):
205:         """Single knowledge generation + answer."""
206:         
207:         # Stage 1: Generate knowledge
208:         knowledge = self._generate_knowledge(question, num_knowledge)
209:         
210:         # Stage 2: Answer with knowledge
211:         answer = self._answer_with_knowledge(question, knowledge)
212:         
213:         return {
214:             'answer': answer,
215:             'knowledge': knowledge
216:         }
217:     
218:     def _answer_with_consistency(self, question, num_knowledge, num_samples=5):
219:         """
220:         Generate multiple knowledge sets, answer with each, vote on final answer.
221:         
222:         More robust but higher cost.
223:         """
224:         from collections import Counter
225:         
226:         answers = []
227:         all_knowledge = []
228:         
229:         for i in range(num_samples):
230:             # Generate different knowledge each time (high temp)
231:             knowledge = self._generate_knowledge(question, num_knowledge)
232:             
233:             # Answer with this knowledge
234:             answer = self._answer_with_knowledge(question, knowledge)
235:             
236:             answers.append(answer)
237:             all_knowledge.append(knowledge)
238:         
239:         # Vote on answers
240:         answer_counts = Counter(answers)
241:         final_answer = answer_counts.most_common(1)[0][0]
242:         confidence = answer_counts[final_answer] / num_samples
243:         
244:         # Find knowledge that led to majority answer
245:         majority_knowledge = [
246:             all_knowledge[i] for i, ans in enumerate(answers)
247:             if ans == final_answer
248:         ][0]
249:         
250:         return {
251:             'answer': final_answer,
252:             'knowledge': majority_knowledge,
253:             'confidence': confidence,
254:             'all_answers': answers
255:         }
256:     
257:     def _generate_knowledge(self, question, num_knowledge):
258:         """Generate relevant knowledge statements."""
259:         
260:         prompt = f"""Question: {question}
261: 
262: Generate {num_knowledge} relevant facts, principles, or pieces of knowledge that would help answer this question accurately.
263: 
264: Each knowledge statement should be:
265: - Directly relevant to the question
266: - A factual statement or principle
267: - Helpful for reasoning about the answer
268: 
269: Knowledge:
270: 1."""
271:         
272:         response = self.llm.complete(prompt, temperature=0.7)
273:         statements = self._parse_numbered_list(response)
274:         
275:         return statements[:num_knowledge]
276:     
277:     def _answer_with_knowledge(self, question, knowledge):
278:         """Answer using generated knowledge as context."""
279:         
280:         knowledge_text = "\n".join([f"- {k}" for k in knowledge])
281:         
282:         prompt = f"""Question: {question}
283: 
284: Relevant Knowledge:
285: {knowledge_text}
286: 
287: Based on the knowledge provided, answer the question. Explain your reasoning.
288: 
289: Answer:"""
290:         
291:         return self.llm.complete(prompt, temperature=0.3).strip()
292:     
293:     def _parse_numbered_list(self, text):
294:         """Extract numbered items."""
295:         import re
296:         pattern = r'\d+[\.)]\s*(.+?)(?=\n\d+[\.)]|\Z)'
297:         matches = re.findall(pattern, text, re.DOTALL)
298:         return [m.strip() for m in matches]
299: 
300: 
301: # Usage
302: gk = GeneratedKnowledge(llm)
303: 
304: # Basic usage
305: result = gk.answer("Why do leaves change color in autumn?", num_knowledge=4)
306: print(f"Answer: {result['answer']}")
307: print(f"\nKnowledge used:")
308: for k in result['knowledge']:
309:     print(f"  - {k}")
310: 
311: # With self-consistency
312: result_robust = gk.answer(
313:     "Why do leaves change color in autumn?",
314:     num_knowledge=4,
315:     use_consistency=True
316: )
317: print(f"\nRobust answer (confidence: {result_robust['confidence']:.0%}): {result_robust['answer']}")
318: ```
319: 
320: ### üí° When to Use Generated Knowledge
321: 
322: **[Generated-Knowledge-Use-Cases**:: (1) Commonsense reasoning tasks (everyday knowledge helpful), (2) Questions requiring background context, (3) Multi-step reasoning (knowledge scaffolds logic), (4) When retrieval not available/needed, (5) Combining with retrieval (generate + retrieve).]**
323: 
324: **‚úÖ Excellent For:**
325: - **Commonsense questions** ("Why does ice float?" - benefits from stating principles)
326: - **Causal reasoning** ("What happens if..." - generate relevant mechanisms)
327: - **Science/physics problems** (generate relevant laws/principles)
328: - **Ethical dilemmas** (generate relevant considerations)
329: - **Strategic thinking** (generate relevant factors)
330: 
331: **‚ùå Not Useful For:**
332: - **Factual lookup** (LLM may not know fact - retrieval better)
333: - **Current events** (training cutoff - must retrieve)
334: - **Precise details** (numbers, dates - retrieval more reliable)
335: - **Private/proprietary info** (LLM can't generate what it never learned)
336: 
337: ### üìä Performance Benchmarks
338: 
339: **From Liu et al. 2022**:
340: 
341: | Task | Standard Prompting | Generated Knowledge | Improvement |
342: |------|-------------------|---------------------|-------------|
343: | **CSQA (Commonsense)** | 67.9% | **76.5%** | **+8.6pp** |
344: | **NumersenseQA** | 64.2% | **72.8%** | **+8.6pp** |
345: | **QASC (Science)** | 71.3% | **78.9%** | **+7.6pp** |
346: 
347: **[Generated-Knowledge-Pattern**:: Consistent +7-9pp improvement on commonsense and reasoning tasks. Little benefit on pure factual recall (where LLM lacks knowledge to generate).]**
348: 
349: ---
350: 
351: ## Retrieval-Augmented Generation (RAG)
352: 
353: [**RAG**:: Combines retrieval from external knowledge base with LLM generation - given query, retrieve relevant documents/passages, include as context in prompt, LLM generates answer grounded in retrieved information.]
354: 
355: ### üéØ Core Concept
356: 
357: **[RAG-Innovation**:: Instead of relying solely on LLM's parametric knowledge, retrieve relevant information from external knowledge base at query time. LLM sees factual context before answering, dramatically reducing hallucination.]**
358: 
359: **Architecture**:
360: ```
361: User Query
362:     ‚Üì
363: Retrieve relevant documents (via vector similarity)
364:     ‚Üì
365: Format: Query + Retrieved Docs
366:     ‚Üì
367: LLM generates answer grounded in docs
368:     ‚Üì
369: Answer (with citations)
370: ```
371: 
372: ### üî¨ RAG Pipeline Components
373: 
374: #### Component 1: Knowledge Base Preparation
375: 
376: ```python
377: from sentence_transformers import SentenceTransformer
378: import numpy as np
379: 
380: class VectorKnowledgeBase:
381:     """
382:     Vector database for semantic retrieval.
383:     """
384:     
385:     def __init__(self, embedding_model='all-MiniLM-L6-v2'):
386:         self.model = SentenceTransformer(embedding_model)
387:         self.documents = []
388:         self.embeddings = None
389:     
390:     def add_documents(self, documents):
391:         """
392:         Add documents to knowledge base.
393:         
394:         Args:
395:             documents: List of {'id': ..., 'text': ..., 'metadata': ...}
396:         """
397:         self.documents.extend(documents)
398:         
399:         # Generate embeddings
400:         texts = [doc['text'] for doc in documents]
401:         new_embeddings = self.model.encode(texts)
402:         
403:         if self.embeddings is None:
404:             self.embeddings = new_embeddings
405:         else:
406:             self.embeddings = np.vstack([self.embeddings, new_embeddings])
407:     
408:     def retrieve(self, query, top_k=5):
409:         """
410:         Retrieve most relevant documents.
411:         
412:         Args:
413:             query: Search query
414:             top_k: Number of documents to return
415:         
416:         Returns:
417:             List of documents with similarity scores
418:         """
419:         # Embed query
420:         query_embedding = self.model.encode([query])[0]
421:         
422:         # Calculate similarities
423:         similarities = np.dot(self.embeddings, query_embedding)
424:         
425:         # Get top k
426:         top_indices = np.argsort(similarities)[-top_k:][::-1]
427:         
428:         results = []
429:         for idx in top_indices:
430:             results.append({
431:                 'document': self.documents[idx],
432:                 'score': float(similarities[idx])
433:             })
434:         
435:         return results
436: ```
437: 
438: #### Component 2: Retrieval
439: 
440: ```python
441: def retrieve_context(query, knowledge_base, top_k=3):
442:     """Retrieve relevant context for query."""
443:     
444:     results = knowledge_base.retrieve(query, top_k=top_k)
445:     
446:     # Format retrieved documents
447:     context_parts = []
448:     for i, result in enumerate(results):
449:         doc = result['document']
450:         context_parts.append(f"[{i+1}] {doc['text']}")
451:     
452:     return "\n\n".join(context_parts), results
453: ```
454: 
455: #### Component 3: Answer Generation with Context
456: 
457: ```python
458: def generate_with_context(query, context):
459:     """Generate answer using retrieved context."""
460:     
461:     prompt = f"""Answer the question based on the context provided. If the context doesn't contain enough information, say so.
462: 
463: Context:
464: {context}
465: 
466: Question: {query}
467: 
468: Answer:"""
469:     
470:     answer = llm.complete(prompt, temperature=0.3)
471:     return answer
472: ```
473: 
474: ### üìù Complete RAG Implementation
475: 
476: ```python
477: class RAGSystem:
478:     """
479:     Complete Retrieval-Augmented Generation system.
480:     """
481:     
482:     def __init__(self, llm, knowledge_base):
483:         self.llm = llm
484:         self.kb = knowledge_base
485:     
486:     def answer(self, query, top_k=5, include_citations=True):
487:         """
488:         Answer query using RAG.
489:         
490:         Args:
491:             query: User question
492:             top_k: Number of documents to retrieve
493:             include_citations: Whether to include source citations
494:         
495:         Returns:
496:             {
497:                 'answer': generated_answer,
498:                 'sources': retrieved_documents,
499:                 'confidence': relevance_score
500:             }
501:         """
502:         # Step 1: Retrieve relevant documents
503:         retrieved = self.kb.retrieve(query, top_k=top_k)
504:         
505:         # Step 2: Format context
506:         context = self._format_context(retrieved, include_citations)
507:         
508:         # Step 3: Generate answer
509:         answer = self._generate_answer(query, context, include_citations)
510:         
511:         # Step 4: Calculate confidence
512:         confidence = self._estimate_confidence(retrieved)
513:         
514:         return {
515:             'answer': answer,
516:             'sources': [r['document'] for r in retrieved],
517:             'confidence': confidence
518:         }
519:     
520:     def _format_context(self, retrieved_docs, include_citations):
521:         """Format retrieved documents as context."""
522:         
523:         parts = []
524:         for i, result in enumerate(retrieved_docs):
525:             doc = result['document']
526:             if include_citations:
527:                 parts.append(f"[Source {i+1}] {doc['text']}")
528:             else:
529:                 parts.append(doc['text'])
530:         
531:         return "\n\n".join(parts)
532:     
533:     def _generate_answer(self, query, context, include_citations):
534:         """Generate answer from query and context."""
535:         
536:         citation_instruction = ""
537:         if include_citations:
538:             citation_instruction = "Cite sources using [Source N] format."
539:         
540:         prompt = f"""Answer the question based on the provided context.
541: 
542: Context:
543: {context}
544: 
545: Question: {query}
546: 
547: Instructions:
548: - Base your answer on the context above
549: - If the context doesn't contain enough information, say so
550: - Be concise but complete
551: {citation_instruction}
552: 
553: Answer:"""
554:         
555:         return self.llm.complete(prompt, temperature=0.3).strip()
556:     
557:     def _estimate_confidence(self, retrieved_docs):
558:         """
559:         Estimate confidence based on retrieval scores.
560:         
561:         High average similarity = high confidence
562:         """
563:         scores = [r['score'] for r in retrieved_docs]
564:         return np.mean(scores)
565: 
566: 
567: # Usage
568: kb = VectorKnowledgeBase()
569: 
570: # Add documents
571: kb.add_documents([
572:     {
573:         'id': '1',
574:         'text': 'The Eiffel Tower was built in 1889 for the World's Fair. It stands 324 meters tall.',
575:         'metadata': {'source': 'encyclopedia', 'topic': 'architecture'}
576:     },
577:     {
578:         'id': '2',
579:         'text': 'Paris is the capital of France, known for landmarks like the Eiffel Tower and Louvre Museum.',
580:         'metadata': {'source': 'travel_guide', 'topic': 'geography'}
581:     },
582:     # ... more documents
583: ])
584: 
585: # Create RAG system
586: rag = RAGSystem(llm, kb)
587: 
588: # Ask question
589: result = rag.answer("How tall is the Eiffel Tower?")
590: 
591: print(f"Answer: {result['answer']}")
592: print(f"\nSources used:")
593: for source in result['sources']:
594:     print(f"  - {source['text'][:80]}...")
595: print(f"\nConfidence: {result['confidence']:.2f}")
596: ```
597: 
598: ### üîß Advanced RAG Techniques
599: 
600: #### Technique 1: Query Rewriting
601: 
602: Rewrite user query for better retrieval:
603: 
604: ```python
605: def rewrite_query(original_query):
606:     """Expand query for better retrieval coverage."""
607:     
608:     rewrite_prompt = f"""Rewrite this query to improve document retrieval.
609: 
610: Original: {original_query}
611: 
612: Generate 3 alternative phrasings that might match relevant documents:
613: 1."""
614:     
615:     alternatives = llm.complete(rewrite_prompt)
616:     queries = parse_numbered_list(alternatives)
617:     
618:     # Retrieve with all queries
619:     all_docs = []
620:     for query in [original_query] + queries:
621:         docs = kb.retrieve(query, top_k=3)
622:         all_docs.extend(docs)
623:     
624:     # Deduplicate and rerank
625:     return deduplicate_and_rerank(all_docs)
626: ```
627: 
628: #### Technique 2: Reranking
629: 
630: Re-score retrieved documents for relevance:
631: 
632: ```python
633: def rerank_documents(query, retrieved_docs):
634:     """Re-score documents using LLM for better relevance."""
635:     
636:     reranked = []
637:     
638:     for doc in retrieved_docs:
639:         # Ask LLM to score relevance
640:         score_prompt = f"""Rate how relevant this document is to the query (0-10).
641: 
642: Query: {query}
643: 
644: Document: {doc['document']['text']}
645: 
646: Relevance score (0-10):"""
647:         
648:         score = float(llm.complete(score_prompt, temperature=0.0).strip())
649:         
650:         reranked.append({
651:             'document': doc['document'],
652:             'score': score
653:         })
654:     
655:     # Sort by new scores
656:     reranked.sort(key=lambda x: x['score'], reverse=True)
657:     return reranked
658: ```
659: 
660: #### Technique 3: Filtering
661: 
662: Remove low-quality/irrelevant documents:
663: 
664: ```python
665: def filter_retrieved(query, docs, min_score=0.3):
666:     """Remove documents below relevance threshold."""
667:     
668:     filtered = [doc for doc in docs if doc['score'] >= min_score]
669:     
670:     if not filtered:
671:         # If all filtered out, keep top 1 with warning
672:         return [docs[0]], "Low confidence: No highly relevant documents found"
673:     
674:     return filtered, None
675: ```
676: 
677: ### üí° When to Use RAG
678: 
679: **[RAG-Use-Cases**:: (1) Factual QA over documents, (2) Current/recent information, (3) Private/proprietary data, (4) Domain-specific knowledge, (5) When accuracy > cost.]**
680: 
681: **‚úÖ Excellent For:**
682: - **Customer support** (retrieve from knowledge base)
683: - **Research assistants** (retrieve from papers/docs)
684: - **Current events** (retrieve news articles)
685: - **Enterprise QA** (retrieve from internal docs)
686: - **Medical/legal queries** (retrieve authoritative sources)
687: 
688: **‚ùå Not Necessary For:**
689: - **Commonsense reasoning** (LLM already knows)
690: - **Creative tasks** (retrieval may constrain)
691: - **Simple calculations** (LLM can compute)
692: - **Very generic questions** (training knowledge sufficient)
693: 
694: ### üìä Performance Benchmarks
695: 
696: **From Lewis et al. 2020 & Izacard et al. 2023**:
697: 
698: | Task | LLM Only | RAG | Improvement |
699: |------|----------|-----|-------------|
700: | **Natural Questions** | 32.1% | **54.7%** | **+22.6pp** |
701: | **TriviaQA** | 58.3% | **68.4%** | **+10.1pp** |
702: | **WebQuestions** | 41.2% | **52.9%** | **+11.7pp** |
703: 
704: **[RAG-Benefit-Pattern**:: Largest gains on knowledge-intensive tasks. Advanced RAG (with reranking, filtering) adds +5-10pp over basic RAG.]**
705: 
706: ---
707: 
708: ## Recitation-Augmented Generation
709: 
710: [**Recitation-Augmented**:: Prompts LLM to first recite/quote relevant passages from provided context before answering - ensuring answer grounded in context and enabling verification of claims against source material.]
711: 
712: ### üéØ Core Concept
713: 
714: **[Recitation-Innovation**:: Rather than directly answering from context, explicitly instruct LLM to first extract and recite relevant passages, then answer based on those recitations. This two-step approach improves faithfulness to source material.]**
715: 
716: **Process**:
717: ```
718: Context: [Long document]
719: Question: "What year was X founded?"
720:     ‚Üì
721: Step 1: Recite relevant passage
722:   ‚Üí "The relevant passage states: 'X was founded in 1995...'"
723:     ‚Üì
724: Step 2: Answer from recitation
725:   ‚Üí "Based on the recited passage, X was founded in 1995."
726: ```
727: 
728: ### üî¨ Implementation
729: 
730: ```python
731: class RecitationAugmented:
732:     """
733:     Recitation-Augmented Generation.
734:     """
735:     
736:     def __init__(self, llm):
737:         self.llm = llm
738:     
739:     def answer(self, context, question):
740:         """
741:         Answer question by first reciting relevant passages.
742:         
743:         Args:
744:             context: Source document/context
745:             question: Question to answer
746:         
747:         Returns:
748:             {
749:                 'recitation': extracted_passage,
750:                 'answer': final_answer,
751:                 'grounded': whether answer came from recitation
752:             }
753:         """
754:         # Step 1: Recite relevant passage
755:         recitation = self._recite(context, question)
756:         
757:         # Step 2: Answer from recitation
758:         answer = self._answer_from_recitation(question, recitation)
759:         
760:         # Verify answer is grounded in recitation
761:         grounded = self._verify_grounding(answer, recitation)
762:         
763:         return {
764:             'recitation': recitation,
765:             'answer': answer,
766:             'grounded': grounded
767:         }
768:     
769:     def _recite(self, context, question):
770:         """Extract and recite relevant passage from context."""
771:         
772:         prompt = f"""Read the context and find the passage that answers the question. Recite that passage word-for-word.
773: 
774: Context:
775: {context}
776: 
777: Question: {question}
778: 
779: Recite the relevant passage:"""
780:         
781:         recitation = self.llm.complete(prompt, temperature=0.0)
782:         return recitation.strip()
783:     
784:     def _answer_from_recitation(self, question, recitation):
785:         """Answer question based on recited passage."""
786:         
787:         prompt = f"""Based on this passage, answer the question concisely.
788: 
789: Passage: {recitation}
790: 
791: Question: {question}
792: 
793: Answer:"""
794:         
795:         answer = self.llm.complete(prompt, temperature=0.0)
796:         return answer.strip()
797:     
798:     def _verify_grounding(self, answer, recitation):
799:         """Check if answer is supported by recitation."""
800:         
801:         verify_prompt = f"""Is this answer supported by the passage?
802: 
803: Passage: {recitation}
804: 
805: Answer: {answer}
806: 
807: Respond with 'YES' if supported, 'NO' if not.
808: 
809: Verdict:"""
810:         
811:         verdict = self.llm.complete(verify_prompt, temperature=0.0).strip()
812:         return verdict.upper().startswith('YES')
813: 
814: 
815: # Usage
816: recite = RecitationAugmented(llm)
817: 
818: context = """
819: The Eiffel Tower was constructed from 1887 to 1889 as the entrance arch 
820: for the 1889 World's Fair. It was initially criticized by some of France's 
821: leading artists and intellectuals. The tower is 324 meters (1,063 ft) tall, 
822: about the same height as an 81-story building.
823: """
824: 
825: result = recite.answer(context, "How tall is the Eiffel Tower?")
826: 
827: print(f"Recited: {result['recitation']}")
828: print(f"\nAnswer: {result['answer']}")
829: print(f"\nGrounded: {result['grounded']}")
830: ```
831: 
832: ### üí° When to Use Recitation-Augmented
833: 
834: **‚úÖ Use When:**
835: - Context already provided (closed-domain QA)
836: - Faithfulness to source critical (legal, medical)
837: - Need to verify claims against source
838: - Combating hallucination in summarization
839: 
840: **‚ùå Not Needed When:**
841: - Open-domain (no fixed context)
842: - Retrieval handles grounding (RAG already retrieves)
843: - Efficiency critical (adds overhead)
844: 
845: ---
846: 
847: ## Technique Selection Guide
848: 
849: ### Decision Tree
850: 
851: ```
852: What's your knowledge integration need?
853: 
854: ‚îå‚îÄ CURRENT/EXTERNAL INFORMATION NEEDED
855: ‚îÇ  ‚îú‚îÄ Have knowledge base ‚Üí RAG
856: ‚îÇ  ‚îî‚îÄ No knowledge base ‚Üí Web search + RAG
857: ‚îÇ
858: ‚îú‚îÄ COMMONSENSE/BACKGROUND KNOWLEDGE
859: ‚îÇ  ‚îî‚îÄ‚ñ∫ Generated Knowledge
860: ‚îÇ
861: ‚îú‚îÄ CONTEXT PROVIDED IN PROMPT
862: ‚îÇ  ‚îú‚îÄ Need source verification ‚Üí Recitation-Augmented
863: ‚îÇ  ‚îî‚îÄ Standard use ‚Üí Direct prompting
864: ‚îÇ
865: ‚îî‚îÄ HYBRID (multiple knowledge types)
866:    ‚îî‚îÄ‚ñ∫ Generated Knowledge + RAG
867: ```
868: 
869: ### Performance vs. Cost Matrix
870: 
871: ```
872: High ‚Üë
873:      ‚îÇ
874: P    ‚îÇ  Advanced RAG
875: e    ‚îÇ  (rerank + filter)
876: r    ‚îÇ        ‚óè
877: f    ‚îÇ                 RAG + Generated
878: o    ‚îÇ               ‚óè 
879: r    ‚îÇ    Basic RAG
880: m    ‚îÇ       ‚óè        
881: a    ‚îÇ              Generated Knowledge
882: n    ‚îÇ                    ‚óè
883: c    ‚îÇ                           Recitation
884: e    ‚îÇ                              ‚óè
885:      ‚îÇ  Parametric Only
886: Low  ‚îÇ     ‚óè
887:      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí
888:         Low                            High
889:                   Cost
890: ```
891: 
892: ---
893: 
894: ## Integration Patterns
895: 
896: ### Pattern 1: Generated + Retrieved Knowledge
897: 
898: ```python
899: def hybrid_knowledge(query):
900:     """Combine generated and retrieved knowledge."""
901:     
902:     # Generate relevant knowledge
903:     generated = generate_knowledge(query, num_knowledge=3)
904:     
905:     # Retrieve documents
906:     retrieved_docs = kb.retrieve(query, top_k=3)
907:     
908:     # Combine both
909:     combined_context = f"""Generated Knowledge:
910: {format_knowledge(generated)}
911: 
912: Retrieved Documents:
913: {format_documents(retrieved_docs)}"""
914:     
915:     # Answer with combined context
916:     return generate_answer(query, combined_context)
917: ```
918: 
919: ### Pattern 2: RAG + Verification
920: 
921: ```python
922: def rag_with_verification(query):
923:     """RAG with Chain of Verification."""
924:     
925:     # Standard RAG
926:     rag_result = rag.answer(query)
927:     
928:     # Verify answer using CoVe
929:     cove = ChainOfVerification(llm)
930:     verified = cove.generate_verified(
931:         f"Answer: {rag_result['answer']}\\n\\nVerify this answer."
932:     )
933:     
934:     return verified['final']
935: ```
936: 
937: ---
938: 
939: ## Research References
940: 
941: ### Generated Knowledge
942: - **[Liu et al. 2022](https://arxiv.org/abs/2110.08387)** - "Generated Knowledge Prompting for Commonsense Reasoning"
943: 
944: ### RAG
945: - **[Lewis et al. 2020](https://arxiv.org/abs/2005.11401)** - "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" - NeurIPS 2020
946: - **[Izacard et al. 2023](https://arxiv.org/abs/2212.10496)** - "Atlas: Few-shot Learning with Retrieval Augmented Language Models"
947: 
948: ### Recitation-Augmented
949: - **[Sun et al. 2022](https://arxiv.org/abs/2210.01296)** - "Recitation-Augmented Language Models"
950: 
951: ---
952: 
953: ## üîó Related Topics for PKB Expansion
954: 
955: 1. **[[vector-databases-embeddings]]**
956:    - **Connection**: RAG requires vector DB for retrieval
957:    - **Depth Potential**: Embedding models, indexing, similarity search
958:    - **Priority**: High - RAG implementation
959: 
960: 2. **[[retrieval-optimization]]**
961:    - **Connection**: Advanced RAG techniques
962:    - **Depth Potential**: Query rewriting, reranking, hybrid search
963:    - **Priority**: High - production RAG
964: 
965: 3. **[[knowledge-base-construction]]**
966:    - **Connection**: Building KB for RAG
967:    - **Depth Potential**: Chunking, metadata, versioning
968:    - **Priority**: Medium - RAG data pipeline
969: 
970: 4. **[[citation-generation]]**
971:    - **Connection**: RAG/Recitation should cite sources
972:    - **Depth Potential**: Citation formats, verification
973:    - **Priority**: Medium - production feature
974: 
975: ---
976: 
977: *This guide synthesizes research from 2020-2024 on knowledge integration. For implementation, see Quick Reference Cards. For combinations, see [[06-integration-patterns-guide]].*
</file>

<file path="__LOCAL-REPO/__exemplar/__import/__master-exemplar/06-integration-patterns-guide.md">
   1: ---
   2: tags: #prompt-engineering #integration-patterns #technique-combinations #advanced-workflows #reference
   3: aliases: [Integration Patterns, Technique Combinations, Workflow Orchestration, Hybrid Approaches]
   4: status: evergreen
   5: certainty: verified
   6: priority: high
   7: created: 2025-12-25
   8: modified: 2025-12-25
   9: type: reference
  10: version: 1.0.0
  11: source: claude-sonnet-4.5
  12: category: integration-patterns
  13: ---
  14: 
  15: # Integration Patterns Guide
  16: 
  17: > [!abstract] Purpose
  18: > Comprehensive guide to combining techniques from different categories (reasoning, agentic, meta-optimization, quality assurance, knowledge integration) for maximum effectiveness. Learn which combinations work synergistically, which conflict, and how to orchestrate complex workflows.
  19: 
  20: ---
  21: 
  22: ## üìã Table of Contents
  23: 
  24: 1. [[#Overview & Philosophy]]
  25: 2. [[#Compatibility Matrix]]
  26: 3. [[#High-Value Combinations]]
  27: 4. [[#Workflow Orchestration Patterns]]
  28: 5. [[#Production Architectures]]
  29: 6. [[#Anti-Patterns & Conflicts]]
  30: 7. [[#Case Studies]]
  31: 
  32: ---
  33: 
  34: ## Overview & Philosophy
  35: 
  36: [**Integration-Pattern**:: Structured approach to combining multiple prompt engineering techniques in a coordinated workflow - leveraging synergies, avoiding conflicts, and orchestrating sequential or parallel execution for superior results.]
  37: 
  38: ### **Why Combine Techniques?**
  39: 
  40: **[Combination-Rationale**:: Single techniques optimize for specific dimensions (reasoning depth, accuracy, reliability, knowledge access). Real-world tasks often require multiple dimensions simultaneously. Strategic combinations address complexity holistically.]**
  41: 
  42: **Example Need**:
  43: - **Task**: Generate comprehensive technical report on recent research topic
  44: - **Requirements**: Current information (RAG), reliable facts (CoVe), high reasoning quality (ToT), iterative refinement (Self-Refine)
  45: - **Solution**: RAG ‚Üí ToT ‚Üí CoVe ‚Üí Self-Refine pipeline
  46: 
  47: ### **Combination Principles**
  48: 
  49: **[Effective-Integration-Principles**:: (1) Complementary strengths - techniques address different weaknesses, (2) Sequential coherence - output of stage N fits input of stage N+1, (3) Cost-benefit balance - combined value exceeds sum of individual costs, (4) Failure isolation - one technique failing doesn't cascade.]**
  50: 
  51: ### **Integration Architecture Levels**
  52: 
  53: ```mermaid
  54: graph TD
  55:     A[Level 1: Sequential Chaining<br/>Stage 1 ‚Üí Stage 2 ‚Üí Stage 3] --> B[Level 2: Conditional Routing<br/>If X then Technique A, else B]
  56:     A --> C[Level 3: Parallel + Merge<br/>Multiple techniques ‚Üí Vote/Combine]
  57:     B --> D[Level 4: Iterative Refinement<br/>Cycle through pipeline until converged]
  58:     C --> D
  59:     D --> E[Level 5: Agent Orchestration<br/>Autonomous technique selection]
  60: ```
  61: 
  62: ---
  63: 
  64: ## Compatibility Matrix
  65: 
  66: ### **Technique Categories**
  67: 
  68: | Category | Techniques |
  69: |----------|-----------|
  70: | **Reasoning** | ToT, GoT, Self-Consistency, PoT, SoT |
  71: | **Agentic** | ReAct, Reflexion, ART, ReWOO |
  72: | **Meta-Optimization** | APE, OPRO, PromptBreeder, Active-Prompt |
  73: | **Quality Assurance** | CoVe, Self-Refine |
  74: | **Knowledge Integration** | Generated Knowledge, RAG, Recitation-Augmented |
  75: 
  76: ### **Compatibility Table**
  77: 
  78: **Legend**: ‚úÖ Synergistic | üü° Compatible | üü† Redundant | ‚ùå Conflicting
  79: 
  80: |  | ToT | SC | RAG | CoVe | Self-Refine | ReAct | PoT |
  81: |--|-----|----|----|------|-------------|-------|-----|
  82: | **ToT** | ‚Äî | ‚úÖ | ‚úÖ | üü° | üü° | üü† | ‚úÖ |
  83: | **Self-Consistency** | ‚úÖ | ‚Äî | ‚úÖ | ‚úÖ | üü† | ‚úÖ | ‚úÖ |
  84: | **RAG** | ‚úÖ | ‚úÖ | ‚Äî | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
  85: | **CoVe** | üü° | ‚úÖ | ‚úÖ | ‚Äî | ‚úÖ | üü° | üü° |
  86: | **Self-Refine** | üü° | üü† | ‚úÖ | ‚úÖ | ‚Äî | üü° | üü° |
  87: | **ReAct** | üü† | ‚úÖ | ‚úÖ | üü° | üü° | ‚Äî | ‚úÖ |
  88: | **PoT** | ‚úÖ | ‚úÖ | ‚úÖ | üü° | üü° | ‚úÖ | ‚Äî |
  89: 
  90: **Key Insights**:
  91: - **RAG** is universally compatible - adds knowledge to any workflow
  92: - **Self-Consistency** and **ToT** are highly synergistic - both explore multiple paths
  93: - **Self-Refine** and **Self-Consistency** are redundant - both iterate for quality
  94: - **ReAct** and **ToT** overlap - both structure reasoning, use one not both
  95: 
  96: ---
  97: 
  98: ## High-Value Combinations
  99: 
 100: ### **Pattern 1: RAG + CoVe (Verified Retrieval)**
 101: 
 102: **[RAG-CoVe-Pattern**:: Retrieve documents, then verify factual claims against retrieved content before final answer. Ensures faithfulness to sources while reducing hallucination beyond what RAG alone achieves.]**
 103: 
 104: ```python
 105: def verified_rag(query, knowledge_base):
 106:     """
 107:     RAG with Chain of Verification.
 108:     
 109:     Use Case: High-stakes factual QA where accuracy critical
 110:     Benefit: 15-20% hallucination reduction vs RAG alone
 111:     Cost: ~4x latency vs basic RAG
 112:     """
 113:     # Stage 1: Retrieve relevant documents
 114:     retrieved = knowledge_base.retrieve(query, top_k=5)
 115:     context = format_documents(retrieved)
 116:     
 117:     # Stage 2: Generate initial answer from context
 118:     initial_answer = generate_with_rag(query, context)
 119:     
 120:     # Stage 3: Plan verifications
 121:     verification_questions = plan_verifications(initial_answer)
 122:     
 123:     # Stage 4: Execute verifications against retrieved docs
 124:     verified_facts = []
 125:     for question in verification_questions:
 126:         # Check if answer found in retrieved docs
 127:         answer = verify_against_context(question, context)
 128:         verified_facts.append({
 129:             'question': question,
 130:             'answer': answer,
 131:             'in_context': answer is not None
 132:         })
 133:     
 134:     # Stage 5: Generate final verified answer
 135:     final_answer = generate_final_with_verification(
 136:         query, context, initial_answer, verified_facts
 137:     )
 138:     
 139:     return {
 140:         'answer': final_answer,
 141:         'sources': retrieved,
 142:         'verifications': verified_facts,
 143:         'all_verified': all(v['in_context'] for v in verified_facts)
 144:     }
 145: 
 146: 
 147: # Example Usage
 148: result = verified_rag(
 149:     "What are the key findings from the 2023 climate report?",
 150:     climate_knowledge_base
 151: )
 152: 
 153: if result['all_verified']:
 154:     print(f"‚úÖ All claims verified: {result['answer']}")
 155: else:
 156:     print(f"‚ö†Ô∏è Some claims unverified: {result['answer']}")
 157:     print(f"Unverified: {[v['question'] for v in result['verifications'] if not v['in_context']]}")
 158: ```
 159: 
 160: **Performance**:
 161: - RAG alone: 12% hallucination rate
 162: - RAG + CoVe: **3-5% hallucination rate** (-60-70% relative)
 163: - Use when: Legal, medical, financial domains where accuracy paramount
 164: 
 165: ---
 166: 
 167: ### **Pattern 2: ToT + Self-Consistency (Robust Exploration)**
 168: 
 169: **[ToT-SC-Pattern**:: Use Tree of Thoughts for deep exploration of solution space, then Self-Consistency across best ToT branches to select most reliable final answer. Combines breadth (ToT) with ensemble robustness (SC).]**
 170: 
 171: ```python
 172: def tot_with_self_consistency(problem, tot_depth=4, sc_samples=5):
 173:     """
 174:     ToT for exploration + SC for validation.
 175:     
 176:     Use Case: Complex planning/reasoning where both depth and reliability needed
 177:     Benefit: Best of both - exploration + robustness
 178:     Cost: Very high (ToT + SC = 10-15x baseline)
 179:     """
 180:     # Stage 1: ToT exploration - find multiple candidate solutions
 181:     tot = TreeOfThoughts(llm)
 182:     solution_paths = tot.solve(
 183:         problem,
 184:         max_depth=tot_depth,
 185:         keep_top_k=sc_samples  # Keep top K paths for SC
 186:     )
 187:     
 188:     if len(solution_paths) < sc_samples:
 189:         # Not enough diverse solutions, generate more
 190:         additional = sc_samples - len(solution_paths)
 191:         for _ in range(additional):
 192:             path = tot.solve(problem, max_depth=tot_depth, temperature=0.9)
 193:             solution_paths.append(path)
 194:     
 195:     # Stage 2: Extract answers from ToT paths
 196:     candidate_answers = [extract_answer(path) for path in solution_paths]
 197:     
 198:     # Stage 3: Self-Consistency voting
 199:     from collections import Counter
 200:     answer_counts = Counter(candidate_answers)
 201:     
 202:     # Stage 4: Return majority answer
 203:     final_answer = answer_counts.most_common(1)[0][0]
 204:     confidence = answer_counts[final_answer] / len(candidate_answers)
 205:     
 206:     return {
 207:         'answer': final_answer,
 208:         'confidence': confidence,
 209:         'all_answers': candidate_answers,
 210:         'exploration_paths': solution_paths
 211:     }
 212: 
 213: 
 214: # Example Usage
 215: result = tot_with_self_consistency(
 216:     "Plan a 3-day itinerary for Paris maximizing cultural sites while minimizing travel time",
 217:     tot_depth=5,
 218:     sc_samples=5
 219: )
 220: 
 221: print(f"Plan (confidence {result['confidence']:.0%}):")
 222: print(result['answer'])
 223: 
 224: if result['confidence'] < 0.6:
 225:     print("\n‚ö†Ô∏è Low confidence - consider alternatives:")
 226:     for ans in set(result['all_answers']):
 227:         count = result['all_answers'].count(ans)
 228:         print(f"  {count}/{len(result['all_answers'])}: {ans[:100]}...")
 229: ```
 230: 
 231: **Performance**:
 232: - ToT alone: 74% success on Game of 24
 233: - ToT + SC: **85% success** (+11pp)
 234: - Use when: High-stakes planning, complex optimization, ambiguous problems
 235: 
 236: ---
 237: 
 238: ### **Pattern 3: Generated Knowledge + RAG (Hybrid Knowledge)**
 239: 
 240: **[Generated-RAG-Pattern**:: Combine LLM's parametric knowledge (via Generated Knowledge) with retrieved documents. LLM generates relevant background, then retrieves specific facts, creating rich context for reasoning.]**
 241: 
 242: ```python
 243: def hybrid_knowledge_integration(query, knowledge_base):
 244:     """
 245:     Generated Knowledge + RAG.
 246:     
 247:     Use Case: Complex topics needing both background and specific facts
 248:     Benefit: Contextual understanding + factual grounding
 249:     Cost: 2-3x baseline (parallel generation + retrieval)
 250:     """
 251:     # Stage 1: Generate relevant background knowledge (parallel)
 252:     generated_future = async_generate_knowledge(query, num_knowledge=5)
 253:     
 254:     # Stage 2: Retrieve specific documents (parallel)
 255:     retrieved_future = async_retrieve(query, knowledge_base, top_k=5)
 256:     
 257:     # Wait for both
 258:     generated = await generated_future
 259:     retrieved = await retrieved_future
 260:     
 261:     # Stage 3: Combine both knowledge sources
 262:     combined_context = f"""Background Knowledge (from LLM):
 263: {format_knowledge(generated)}
 264: 
 265: Specific Information (from Knowledge Base):
 266: {format_documents(retrieved)}"""
 267:     
 268:     # Stage 4: Answer with hybrid context
 269:     answer = generate_with_context(query, combined_context)
 270:     
 271:     return {
 272:         'answer': answer,
 273:         'generated_knowledge': generated,
 274:         'retrieved_docs': retrieved,
 275:         'knowledge_sources': 'hybrid'
 276:     }
 277: 
 278: 
 279: # Example Usage
 280: result = hybrid_knowledge_integration(
 281:     "How does quantum entanglement relate to quantum computing performance?",
 282:     quantum_kb
 283: )
 284: 
 285: print(f"Answer: {result['answer']}\n")
 286: print("Background concepts covered:")
 287: for k in result['generated_knowledge']:
 288:     print(f"  - {k}")
 289: print("\nSpecific evidence cited:")
 290: for doc in result['retrieved_docs']:
 291:     print(f"  - {doc['metadata']['title']}")
 292: ```
 293: 
 294: **Performance**:
 295: - RAG alone: 58% on domain QA
 296: - Generated Knowledge alone: 52% on domain QA
 297: - Combined: **69% on domain QA** (+11pp over best single)
 298: - Use when: Interdisciplinary questions, complex technical topics
 299: 
 300: ---
 301: 
 302: ### **Pattern 4: ReAct + RAG (Agentic Retrieval)**
 303: 
 304: **[ReAct-RAG-Pattern**:: ReAct agent uses RAG as a tool - decides when to retrieve, what to retrieve, and how to use retrieved information. More flexible than fixed RAG pipeline.]**
 305: 
 306: ```python
 307: def agentic_rag(query, knowledge_base, max_steps=10):
 308:     """
 309:     ReAct agent with RAG tool.
 310:     
 311:     Use Case: Multi-step research where retrieval needs vary by reasoning stage
 312:     Benefit: Adaptive retrieval based on reasoning progress
 313:     Cost: Variable (agent decides retrieval frequency)
 314:     """
 315:     # Define tools
 316:     tools = {
 317:         'search': lambda q: knowledge_base.retrieve(q, top_k=3),
 318:         'calculate': lambda expr: eval(expr),  # Simplified
 319:         'summarize': lambda text: summarize(text)
 320:     }
 321:     
 322:     # ReAct loop
 323:     thought_history = []
 324:     observation_history = []
 325:     
 326:     for step in range(max_steps):
 327:         # Thought: Agent reasons about next action
 328:         thought = generate_thought(query, thought_history, observation_history)
 329:         thought_history.append(thought)
 330:         
 331:         # Action: Agent decides which tool (if any)
 332:         action = parse_action(thought)
 333:         
 334:         if action['type'] == 'search':
 335:             # Retrieve documents
 336:             docs = tools['search'](action['query'])
 337:             observation = format_search_results(docs)
 338:         
 339:         elif action['type'] == 'finish':
 340:             # Agent thinks it has answer
 341:             return {
 342:                 'answer': action['answer'],
 343:                 'reasoning_trace': thought_history,
 344:                 'retrievals': [obs for obs in observation_history if 'search' in obs],
 345:                 'steps': step + 1
 346:             }
 347:         
 348:         else:
 349:             # Other tool
 350:             observation = tools[action['type']](action['input'])
 351:         
 352:         observation_history.append(observation)
 353:     
 354:     # Max steps reached
 355:     return {
 356:         'answer': thought_history[-1],  # Best effort
 357:         'reasoning_trace': thought_history,
 358:         'completed': False
 359:     }
 360: 
 361: 
 362: # Example Usage
 363: result = agentic_rag(
 364:     "Compare GDP growth rates of top 5 economies in 2023 and explain trends",
 365:     economic_kb
 366: )
 367: 
 368: print(f"Answer: {result['answer']}\n")
 369: print(f"Reasoning steps: {result['steps']}")
 370: print(f"Documents retrieved: {len(result['retrievals'])}")
 371: for i, thought in enumerate(result['reasoning_trace'], 1):
 372:     print(f"  Step {i}: {thought[:80]}...")
 373: ```
 374: 
 375: **Performance**:
 376: - Fixed RAG: 65% on multi-hop QA
 377: - ReAct + RAG: **73% on multi-hop QA** (+8pp)
 378: - Use when: Multi-step research, unclear retrieval needs, complex workflows
 379: 
 380: ---
 381: 
 382: ### **Pattern 5: PoT + Self-Consistency (Reliable Computation)**
 383: 
 384: **[PoT-SC-Pattern**:: Generate multiple Python programs for same problem (PoT), execute all, vote on results (SC). Handles computational tasks with high reliability.]**
 385: 
 386: ```python
 387: def reliable_computation(problem, num_programs=5):
 388:     """
 389:     Program of Thoughts + Self-Consistency.
 390:     
 391:     Use Case: Mathematical/computational tasks requiring high reliability
 392:     Benefit: Catches code errors through voting
 393:     Cost: 5x program generation + execution
 394:     """
 395:     programs = []
 396:     results = []
 397:     
 398:     # Stage 1: Generate multiple programs (diverse approaches)
 399:     for i in range(num_programs):
 400:         program = generate_program(problem, temperature=0.7)
 401:         programs.append(program)
 402:         
 403:         # Execute program
 404:         try:
 405:             result = execute_safely(program)
 406:             results.append(result)
 407:         except Exception as e:
 408:             results.append(None)  # Execution failed
 409:     
 410:     # Stage 2: Vote on results
 411:     valid_results = [r for r in results if r is not None]
 412:     
 413:     if not valid_results:
 414:         return {'error': 'All programs failed', 'programs': programs}
 415:     
 416:     from collections import Counter
 417:     result_counts = Counter(valid_results)
 418:     final_result = result_counts.most_common(1)[0][0]
 419:     confidence = result_counts[final_result] / len(valid_results)
 420:     
 421:     return {
 422:         'result': final_result,
 423:         'confidence': confidence,
 424:         'programs': programs,
 425:         'all_results': results,
 426:         'success_rate': len(valid_results) / num_programs
 427:     }
 428: 
 429: 
 430: # Example Usage
 431: result = reliable_computation(
 432:     "Calculate the compound interest on $10,000 at 5% annually for 10 years with monthly compounding",
 433:     num_programs=5
 434: )
 435: 
 436: if result['confidence'] >= 0.8:
 437:     print(f"‚úÖ High confidence result: ${result['result']:.2f}")
 438: else:
 439:     print(f"‚ö†Ô∏è Low confidence result: ${result['result']:.2f}")
 440:     print(f"Results distribution: {Counter(result['all_results'])}")
 441: ```
 442: 
 443: **Performance**:
 444: - PoT alone: 85% on GSM8K
 445: - PoT + SC: **92% on GSM8K** (+7pp)
 446: - Use when: Financial calculations, scientific computing, correctness critical
 447: 
 448: ---
 449: 
 450: ### **Pattern 6: Self-Refine + CoVe (Quality + Accuracy)**
 451: 
 452: **[Refine-Verify-Pattern**:: Iteratively improve output quality (Self-Refine) while verifying facts (CoVe) at each iteration. Achieves both stylistic polish and factual accuracy.]**
 453: 
 454: ```python
 455: def refine_and_verify(query, max_iterations=3):
 456:     """
 457:     Self-Refine + Chain of Verification.
 458:     
 459:     Use Case: Content creation requiring both quality and accuracy
 460:     Benefit: Polished output with verified facts
 461:     Cost: Very high (iterations √ó verification = 12x+)
 462:     """
 463:     current_output = generate_initial(query)
 464:     
 465:     for iteration in range(max_iterations):
 466:         # Stage 1: Verify current output
 467:         verification = chain_of_verification(current_output)
 468:         
 469:         # Stage 2: Generate feedback incorporating verification
 470:         feedback = generate_feedback_with_verification(
 471:             output=current_output,
 472:             verifications=verification['results'],
 473:             criteria=['accuracy', 'clarity', 'completeness', 'style']
 474:         )
 475:         
 476:         # Stage 3: Refine based on combined feedback
 477:         refined = refine_output(current_output, feedback)
 478:         
 479:         # Check if good enough
 480:         score = evaluate_quality(refined)
 481:         if score >= 8.5 and verification['all_verified']:
 482:             return {
 483:                 'output': refined,
 484:                 'iterations': iteration + 1,
 485:                 'final_score': score,
 486:                 'verified': True
 487:             }
 488:         
 489:         current_output = refined
 490:     
 491:     return {
 492:         'output': current_output,
 493:         'iterations': max_iterations,
 494:         'final_score': evaluate_quality(current_output),
 495:         'verified': chain_of_verification(current_output)['all_verified']
 496:     }
 497: 
 498: 
 499: # Example Usage
 500: result = refine_and_verify(
 501:     "Write a comprehensive but accessible explanation of CRISPR gene editing for high school students"
 502: )
 503: 
 504: print(f"Final output ({result['iterations']} iterations):")
 505: print(result['output'])
 506: print(f"\nQuality score: {result['final_score']}/10")
 507: print(f"All facts verified: {result['verified']}")
 508: ```
 509: 
 510: **Performance**:
 511: - Self-Refine alone: 7.2/10 average quality
 512: - Self-Refine + CoVe: **8.4/10 quality** + 3% hallucination (vs 18% without CoVe)
 513: - Use when: Blog posts, educational content, documentation
 514: 
 515: ---
 516: 
 517: ## Workflow Orchestration Patterns
 518: 
 519: ### **Sequential Pipeline**
 520: 
 521: **[Sequential-Pattern**:: Techniques executed in fixed order, each stage's output feeds next stage's input.]**
 522: 
 523: ```python
 524: class SequentialPipeline:
 525:     """
 526:     Execute techniques in sequence.
 527:     """
 528:     
 529:     def __init__(self, stages):
 530:         """
 531:         Args:
 532:             stages: List of (name, function) tuples
 533:         """
 534:         self.stages = stages
 535:     
 536:     def execute(self, initial_input):
 537:         """Run all stages sequentially."""
 538:         
 539:         current = initial_input
 540:         history = []
 541:         
 542:         for stage_name, stage_func in self.stages:
 543:             print(f"Executing: {stage_name}")
 544:             current = stage_func(current)
 545:             history.append({
 546:                 'stage': stage_name,
 547:                 'output': current
 548:             })
 549:         
 550:         return {
 551:             'final': current,
 552:             'history': history
 553:         }
 554: 
 555: 
 556: # Example: RAG ‚Üí ToT ‚Üí CoVe ‚Üí Self-Refine
 557: pipeline = SequentialPipeline([
 558:     ('RAG', lambda q: rag_retrieve(q)),
 559:     ('ToT', lambda ctx: tot_reason(ctx)),
 560:     ('CoVe', lambda ans: verify_answer(ans)),
 561:     ('Refine', lambda ver: refine_final(ver))
 562: ])
 563: 
 564: result = pipeline.execute("Complex query")
 565: ```
 566: 
 567: ---
 568: 
 569: ### **Conditional Routing**
 570: 
 571: **[Conditional-Pattern**:: Route to different techniques based on query characteristics or intermediate results.]**
 572: 
 573: ```python
 574: class ConditionalRouter:
 575:     """
 576:     Route to appropriate technique based on conditions.
 577:     """
 578:     
 579:     def execute(self, query):
 580:         """Route to appropriate workflow."""
 581:         
 582:         # Classify query
 583:         query_type = classify_query(query)
 584:         
 585:         if query_type == 'factual':
 586:             # Factual questions ‚Üí RAG + CoVe
 587:             return rag_verified_pipeline(query)
 588:         
 589:         elif query_type == 'reasoning':
 590:             # Complex reasoning ‚Üí ToT + SC
 591:             return tot_sc_pipeline(query)
 592:         
 593:         elif query_type == 'computational':
 594:             # Math/code ‚Üí PoT + SC
 595:             return pot_sc_pipeline(query)
 596:         
 597:         elif query_type == 'creative':
 598:             # Creative tasks ‚Üí Self-Refine
 599:             return creative_refine_pipeline(query)
 600:         
 601:         else:
 602:             # Default to basic generation
 603:             return basic_generation(query)
 604: 
 605: 
 606: # Example
 607: router = ConditionalRouter()
 608: result = router.execute("What is the GDP of France in 2023?")  # ‚Üí RAG + CoVe
 609: result = router.execute("Plan optimal travel route")  # ‚Üí ToT + SC
 610: ```
 611: 
 612: ---
 613: 
 614: ### **Parallel Execution + Merge**
 615: 
 616: **[Parallel-Pattern**:: Execute multiple techniques simultaneously, then merge results (vote, combine, select best).]**
 617: 
 618: ```python
 619: import asyncio
 620: 
 621: class ParallelMerge:
 622:     """
 623:     Execute techniques in parallel, merge results.
 624:     """
 625:     
 626:     async def execute(self, query, techniques, merge_strategy='vote'):
 627:         """
 628:         Run techniques in parallel.
 629:         
 630:         Args:
 631:             query: Input query
 632:             techniques: List of (name, async_function) tuples
 633:             merge_strategy: 'vote' | 'combine' | 'best'
 634:         """
 635:         # Execute all in parallel
 636:         tasks = [func(query) for name, func in techniques]
 637:         results = await asyncio.gather(*tasks)
 638:         
 639:         # Merge based on strategy
 640:         if merge_strategy == 'vote':
 641:             return self._vote(results)
 642:         elif merge_strategy == 'combine':
 643:             return self._combine(results)
 644:         elif merge_strategy == 'best':
 645:             return self._select_best(results)
 646:     
 647:     def _vote(self, results):
 648:         """Majority voting."""
 649:         from collections import Counter
 650:         counts = Counter(results)
 651:         return counts.most_common(1)[0][0]
 652:     
 653:     def _combine(self, results):
 654:         """Combine all results."""
 655:         return " ".join(results)
 656:     
 657:     def _select_best(self, results):
 658:         """Select highest quality."""
 659:         scores = [score_quality(r) for r in results]
 660:         best_idx = scores.index(max(scores))
 661:         return results[best_idx]
 662: 
 663: 
 664: # Example: Run ToT, RAG, Generated Knowledge in parallel
 665: async def main():
 666:     parallel = ParallelMerge()
 667:     
 668:     result = await parallel.execute(
 669:         query="Explain quantum tunneling",
 670:         techniques=[
 671:             ('ToT', async_tot_solve),
 672:             ('RAG', async_rag_retrieve),
 673:             ('GenKnowledge', async_generate_knowledge)
 674:         ],
 675:         merge_strategy='combine'
 676:     )
 677:     
 678:     print(result)
 679: 
 680: asyncio.run(main())
 681: ```
 682: 
 683: ---
 684: 
 685: ## Production Architectures
 686: 
 687: ### **Tiered Quality System**
 688: 
 689: **[Tiered-Architecture**:: Different quality levels with different technique combinations based on importance/cost tolerance.]**
 690: 
 691: ```python
 692: class TieredQualitySystem:
 693:     """
 694:     Tiered quality levels for production.
 695:     """
 696:     
 697:     def answer(self, query, quality_tier='standard'):
 698:         """
 699:         Generate answer at specified quality tier.
 700:         
 701:         Tiers:
 702:         - 'fast': Basic generation (1x cost, <1s)
 703:         - 'standard': RAG (2-3x cost, 1-2s)
 704:         - 'high': RAG + CoVe (6-8x cost, 3-5s)
 705:         - 'critical': RAG + ToT + CoVe + SC (20-30x cost, 10-20s)
 706:         """
 707:         
 708:         if quality_tier == 'fast':
 709:             return self._fast_answer(query)
 710:         
 711:         elif quality_tier == 'standard':
 712:             return self._standard_answer(query)
 713:         
 714:         elif quality_tier == 'high':
 715:             return self._high_quality_answer(query)
 716:         
 717:         elif quality_tier == 'critical':
 718:             return self._critical_answer(query)
 719:     
 720:     def _fast_answer(self, query):
 721:         """Fast: Direct generation."""
 722:         return llm.complete(query)
 723:     
 724:     def _standard_answer(self, query):
 725:         """Standard: RAG."""
 726:         return rag.answer(query)
 727:     
 728:     def _high_quality_answer(self, query):
 729:         """High: RAG + CoVe."""
 730:         return verified_rag(query, kb)
 731:     
 732:     def _critical_answer(self, query):
 733:         """Critical: Full pipeline."""
 734:         # RAG retrieval
 735:         context = rag.answer(query)
 736:         
 737:         # ToT reasoning
 738:         tot_result = tot.solve(f"Given context: {context}, answer: {query}")
 739:         
 740:         # Verify
 741:         verified = cove.verify(tot_result)
 742:         
 743:         # Self-Consistency
 744:         sc_result = self_consistency(verified, num_samples=5)
 745:         
 746:         return sc_result
 747: 
 748: 
 749: # Usage
 750: system = TieredQualitySystem()
 751: 
 752: # Customer support (fast)
 753: answer = system.answer("How do I reset my password?", quality_tier='fast')
 754: 
 755: # General inquiries (standard)
 756: answer = system.answer("What are your business hours?", quality_tier='standard')
 757: 
 758: # Important decisions (high)
 759: answer = system.answer("Should I approve this $50K purchase?", quality_tier='high')
 760: 
 761: # Critical compliance (critical)
 762: answer = system.answer("Is this transaction compliant with regulations?", quality_tier='critical')
 763: ```
 764: 
 765: ---
 766: 
 767: ### **Adaptive Pipeline**
 768: 
 769: **[Adaptive-Architecture**:: Pipeline adapts based on intermediate results - adds verification if uncertainty high, adds reasoning if query complex.]**
 770: 
 771: ```python
 772: class AdaptivePipeline:
 773:     """
 774:     Pipeline adapts based on intermediate results.
 775:     """
 776:     
 777:     def execute(self, query):
 778:         """Adaptively execute techniques."""
 779:         
 780:         # Stage 1: Always start with basic generation or RAG
 781:         initial = self._initial_answer(query)
 782:         
 783:         # Stage 2: Assess quality
 784:         quality_score = assess_quality(initial['answer'])
 785:         uncertainty = initial.get('uncertainty', 0.0)
 786:         
 787:         # Stage 3: Adaptive enhancement
 788:         if quality_score < 6.0:
 789:             # Low quality ‚Üí Add reasoning
 790:             initial = self._add_reasoning(query, initial)
 791:         
 792:         if uncertainty > 0.5:
 793:             # High uncertainty ‚Üí Add verification
 794:             initial = self._add_verification(initial)
 795:         
 796:         # Stage 4: Final refinement if needed
 797:         if quality_score < 7.5:
 798:             initial = self._add_refinement(initial)
 799:         
 800:         return initial
 801:     
 802:     def _initial_answer(self, query):
 803:         """Generate initial answer."""
 804:         needs_knowledge = detect_knowledge_requirement(query)
 805:         
 806:         if needs_knowledge:
 807:             return rag.answer(query)
 808:         else:
 809:             return {'answer': llm.complete(query), 'uncertainty': 0.3}
 810:     
 811:     def _add_reasoning(self, query, current):
 812:         """Add ToT reasoning."""
 813:         tot_result = tot.solve(query)
 814:         return {
 815:             **current,
 816:             'answer': tot_result,
 817:             'enhanced_with': 'ToT'
 818:         }
 819:     
 820:     def _add_verification(self, current):
 821:         """Add CoVe verification."""
 822:         verified = cove.verify(current['answer'])
 823:         return {
 824:             **current,
 825:             'answer': verified['final'],
 826:             'verified': True
 827:         }
 828:     
 829:     def _add_refinement(self, current):
 830:         """Add Self-Refine."""
 831:         refined = refiner.refine(current['answer'])
 832:         return {
 833:             **current,
 834:             'answer': refined['final_output'],
 835:             'refined': True
 836:         }
 837: ```
 838: 
 839: ---
 840: 
 841: ## Anti-Patterns & Conflicts
 842: 
 843: ### **Anti-Pattern 1: Redundant Iteration**
 844: 
 845: **‚ùå Don't**: Self-Refine + Self-Consistency (both iterate, redundant)
 846: 
 847: ```python
 848: # BAD: Redundant iteration
 849: result = self_refine(query)  # Iterates 3x
 850: result = self_consistency(result)  # Iterates 5x more
 851: # Total: 15+ generations for marginal gain
 852: ```
 853: 
 854: **‚úÖ Do**: Choose one iteration approach
 855: 
 856: ```python
 857: # GOOD: Single iteration approach
 858: result = self_consistency(query, num_samples=5)
 859: # OR
 860: result = self_refine(query, max_iterations=3)
 861: ```
 862: 
 863: ---
 864: 
 865: ### **Anti-Pattern 2: Conflicting Techniques**
 866: 
 867: **‚ùå Don't**: ToT + ReAct (both structure reasoning differently)
 868: 
 869: ```python
 870: # BAD: Conflicting reasoning structures
 871: tot_result = tot.solve(query)  # Tree-structured exploration
 872: react_result = react.solve(tot_result)  # Thought-Action-Observation loops
 873: # ReAct expects different input format
 874: ```
 875: 
 876: **‚úÖ Do**: Use one reasoning framework or sequence carefully
 877: 
 878: ```python
 879: # GOOD: Use appropriate framework for task
 880: if requires_tools:
 881:     result = react.solve(query)  # Agent with tools
 882: else:
 883:     result = tot.solve(query)  # Pure reasoning
 884: ```
 885: 
 886: ---
 887: 
 888: ### **Anti-Pattern 3: Premature Verification**
 889: 
 890: **‚ùå Don't**: CoVe before knowledge integration
 891: 
 892: ```python
 893: # BAD: Verify before having knowledge
 894: verified = cove.verify(query)  # LLM has no knowledge to verify
 895: rag_result = rag.answer(verified)  # Too late, already hallucinated
 896: ```
 897: 
 898: **‚úÖ Do**: Retrieve/generate knowledge first, then verify
 899: 
 900: ```python
 901: # GOOD: Knowledge ‚Üí Verification
 902: rag_result = rag.answer(query)  # Get knowledge
 903: verified = cove.verify(rag_result)  # Then verify against knowledge
 904: ```
 905: 
 906: ---
 907: 
 908: ## Case Studies
 909: 
 910: ### **Case Study 1: Medical QA System**
 911: 
 912: **Requirements**: Accurate, verified, current information
 913: 
 914: **Solution**: RAG + CoVe + Self-Refine
 915: 
 916: ```python
 917: def medical_qa(query):
 918:     """High-accuracy medical QA."""
 919:     
 920:     # Stage 1: Retrieve from medical knowledge base
 921:     docs = medical_kb.retrieve(query, top_k=5)
 922:     
 923:     # Stage 2: Generate answer from retrieved docs
 924:     answer = generate_with_context(query, docs)
 925:     
 926:     # Stage 3: Verify all medical claims
 927:     verified = chain_of_verification(answer)
 928:     
 929:     # Stage 4: Refine for clarity (medical ‚Üí patient language)
 930:     refined = self_refine(
 931:         verified['final'],
 932:         criteria=['medical_accuracy', 'patient_comprehension', 'completeness']
 933:     )
 934:     
 935:     return {
 936:         'answer': refined['final_output'],
 937:         'sources': docs,
 938:         'all_claims_verified': verified['all_verified'],
 939:         'quality_score': refined['final_score']
 940:     }
 941: ```
 942: 
 943: **Results**:
 944: - Accuracy: 94% (vs 78% without verification)
 945: - Patient satisfaction: 8.9/10 (vs 7.2/10 without refinement)
 946: - Hallucination: 2% (vs 15% baseline)
 947: 
 948: ---
 949: 
 950: ### **Case Study 2: Financial Research Assistant**
 951: 
 952: **Requirements**: Multi-step research, calculation accuracy, current data
 953: 
 954: **Solution**: ReAct + PoT + RAG
 955: 
 956: ```python
 957: def financial_research(query):
 958:     """Research assistant with tools."""
 959:     
 960:     tools = {
 961:         'search': lambda q: financial_kb.retrieve(q),
 962:         'calculate': lambda expr: execute_program(expr),  # PoT
 963:         'get_current_data': lambda ticker: api.get_stock_data(ticker)
 964:     }
 965:     
 966:     # ReAct agent orchestrates tool use
 967:     result = react_agent.solve(query, tools=tools)
 968:     
 969:     return result
 970: ```
 971: 
 972: **Results**:
 973: - Task completion: 89% (vs 65% with fixed pipeline)
 974: - Calculation accuracy: 98% (PoT)
 975: - Research depth: 7.8/10 (vs 6.2/10 baseline)
 976: 
 977: ---
 978: 
 979: ### **Case Study 3: Content Generation Platform**
 980: 
 981: **Requirements**: Quality, originality, factual accuracy
 982: 
 983: **Solution**: Tiered system (Generated Knowledge + Self-Refine for basic, + CoVe for premium)
 984: 
 985: ```python
 986: def generate_content(topic, tier='standard'):
 987:     """Content generation with tiered quality."""
 988:     
 989:     if tier == 'basic':
 990:         # Direct generation
 991:         return llm.complete(f"Write about: {topic}")
 992:     
 993:     elif tier == 'standard':
 994:         # Generated Knowledge + Refine
 995:         knowledge = generate_knowledge(topic)
 996:         content = generate_with_knowledge(topic, knowledge)
 997:         refined = self_refine(content, max_iterations=2)
 998:         return refined['final_output']
 999:     
1000:     elif tier == 'premium':
1001:         # Full pipeline
1002:         knowledge = generate_knowledge(topic)
1003:         content = generate_with_knowledge(topic, knowledge)
1004:         refined = self_refine(content, max_iterations=3)
1005:         verified = chain_of_verification(refined['final_output'])
1006:         return verified['final']
1007: ```
1008: 
1009: **Results**:
1010: - Basic: 6.5/10 quality, $0.02 per article
1011: - Standard: 7.8/10 quality, $0.08 per article
1012: - Premium: 8.9/10 quality, 1.5% errors, $0.25 per article
1013: 
1014: ---
1015: 
1016: ## üîó Related Topics for PKB Expansion
1017: 
1018: 1. **[[pipeline-optimization-strategies]]**
1019:    - **Connection**: Optimizing combined technique performance
1020:    - **Depth Potential**: Caching, parallelization, early stopping
1021:    - **Priority**: High - production efficiency
1022: 
1023: 2. **[[cost-benefit-analysis-combinations]]**
1024:    - **Connection**: ROI of different combinations
1025:    - **Depth Potential**: Token cost vs quality metrics
1026:    - **Priority**: High - resource planning
1027: 
1028: 3. **[[technique-conflict-resolution]]**
1029:    - **Connection**: Handling incompatible techniques
1030:    - **Depth Potential**: Conflict detection, automatic routing
1031:    - **Priority**: Medium - system robustness
1032: 
1033: 4. **[[adaptive-orchestration]]**
1034:    - **Connection**: Dynamic technique selection
1035:    - **Depth Potential**: ML-based orchestration, reinforcement learning
1036:    - **Priority**: Medium - advanced automation
1037: 
1038: 5. **[[production-monitoring]]**
1039:    - **Connection**: Tracking combined pipeline performance
1040:    - **Depth Potential**: Metrics, logging, debugging
1041:    - **Priority**: High - operations
1042: 
1043: 6. **[[technique-versioning]]**
1044:    - **Connection**: Managing technique updates in pipelines
1045:    - **Depth Potential**: A/B testing, gradual rollout
1046:    - **Priority**: Medium - maintenance
1047: 
1048: ---
1049: 
1050: *This guide synthesizes practical experience combining techniques. For specific implementations, see individual technique guides. For production deployment, see monitoring and optimization resources.*
</file>

<file path="__LOCAL-REPO/__exemplar/__import/__master-exemplar/qrc-chain-of-verification.md">
  1: ---
  2: tags: #quick-reference #cove #verification #quality-assurance #one-pager
  3: type: quick-reference
  4: technique: Chain of Verification
  5: category: quality-assurance
  6: ---
  7: 
  8: # ‚úÖ Chain of Verification (CoVe) - Quick Reference
  9: 
 10: ## One-Line Summary
 11: Generate answer ‚Üí Plan verification questions ‚Üí Answer verifications independently ‚Üí Revise with verification results.
 12: 
 13: ---
 14: 
 15: ## When to Use
 16: ‚úÖ **Perfect For**: Factual writing, biographies, lists, reducing hallucination, high-accuracy needs
 17: ‚ùå **Skip For**: Creative content, opinion pieces, already verified (RAG), speed-critical
 18: 
 19: ---
 20: 
 21: ## Four-Step Process
 22: 
 23: ```
 24: ‚ë† BASELINE: Generate initial response
 25:     ‚Üì
 26: ‚ë° PLAN: Create verification questions for claims
 27:     ‚Üì
 28: ‚ë¢ EXECUTE: Answer verifications INDEPENDENTLY
 29:     ‚Üì
 30: ‚ë£ FINAL: Generate revised response with corrections
 31: ```
 32: 
 33: **Key Innovation**: Step ‚ë¢ is independent - LLM doesn't see initial response, preventing confirmation bias
 34: 
 35: ---
 36: 
 37: ## Implementation Template
 38: 
 39: ```python
 40: def chain_of_verification(query):
 41:     """
 42:     CoVe implementation
 43:     """
 44:     # Step 1: Generate baseline response
 45:     baseline = llm.complete(f"Answer: {query}")
 46:     
 47:     # Step 2: Plan verifications
 48:     plan_prompt = f"""Response: {baseline}
 49: 
 50: Generate verification questions for factual claims:
 51: 1."""
 52:     questions = llm.complete(plan_prompt)
 53:     
 54:     # Step 3: Execute verifications INDEPENDENTLY
 55:     verified = []
 56:     for q in parse_questions(questions):
 57:         # CRITICAL: No baseline shown
 58:         answer = llm.complete(f"Answer: {q}", temp=0.0)
 59:         verified.append({'question': q, 'answer': answer})
 60:     
 61:     # Step 4: Generate final with corrections
 62:     final_prompt = f"""Initial: {baseline}
 63: 
 64: Verifications: {format_verifications(verified)}
 65: 
 66: Provide corrected final answer:"""
 67:     
 68:     final = llm.complete(final_prompt)
 69:     
 70:     return {
 71:         'baseline': baseline,
 72:         'final': final,
 73:         'verifications': verified
 74:     }
 75: ```
 76: 
 77: ---
 78: 
 79: ## Prompt Templates
 80: 
 81: ### Step 1: Baseline
 82: ```
 83: Answer this question:
 84: 
 85: {query}
 86: 
 87: Answer:
 88: ```
 89: 
 90: ### Step 2: Plan Verifications
 91: ```
 92: Response: {baseline_response}
 93: 
 94: This response makes factual claims.
 95: Generate verification questions:
 96: 
 97: 1. [Question for claim 1]
 98: 2. [Question for claim 2]
 99: 3. [Question for claim 3]
100: ```
101: 
102: ### Step 3: Execute (INDEPENDENT!)
103: ```
104: Answer this factual question:
105: 
106: {verification_question}
107: 
108: Answer:
109: ```
110: 
111: **Critical**: NO baseline response shown!
112: 
113: ### Step 4: Final Revision
114: ```
115: Original: {baseline}
116: 
117: Verification results:
118: Q: {q1}
119: A: {a1}
120: 
121: Q: {q2}  
122: A: {a2}
123: 
124: Based on verifications, provide corrected answer:
125: ```
126: 
127: ---
128: 
129: ## Performance Benchmarks
130: - **Long-form QA**: 16% hallucination (vs 38% baseline) - **-58% reduction**
131: - **Biographies**: 23% hallucination (vs 45% baseline) - **-49% reduction**
132: - **Lists**: 26% hallucination (vs 52% baseline) - **-50% reduction**
133: 
134: **Pattern**: Consistently halves hallucination rate
135: 
136: ---
137: 
138: ## Costs
139: - **Token Cost**: ~4x baseline (4 sequential LLM calls)
140: - **Latency**: ~4x baseline (cannot parallelize fully)
141: - **Best Practice**: Use for high-value content where accuracy critical
142: 
143: ---
144: 
145: ## Why Independent Verification Works
146: 
147: ‚ùå **Joint Verification** (showing baseline):
148: ```
149: Initial: "Hillary Clinton born in NYC"
150: Verify: Was Hillary Clinton born in NYC?
151: ‚Üí LLM rationalizes: "Yes, as stated above..."
152: ```
153: 
154: ‚úÖ **Independent Verification** (no baseline shown):
155: ```
156: Verify: Was Hillary Clinton born in NYC?
157: ‚Üí LLM retrieves fresh: "No, Chicago, Illinois"
158: ```
159: 
160: **Benefit**: Prevents confirmation bias where LLM defends initial errors
161: 
162: ---
163: 
164: ## Verification Question Design
165: 
166: **Good Verification Questions**:
167: - ‚úÖ "Was Marie Curie born in 1867?" (specific, binary)
168: - ‚úÖ "What year did Marie Curie discover radium?" (specific fact)
169: - ‚úÖ "Was Marie Curie the first person to win two Nobel Prizes?" (checkable claim)
170: 
171: **Poor Verification Questions**:
172: - ‚ùå "Is the response accurate?" (too vague)
173: - ‚ùå "Tell me about Marie Curie" (not verification, just repeats task)
174: - ‚ùå "Are all facts correct?" (doesn't specify which facts)
175: 
176: **Pattern**: One verification per factual claim, specific and checkable
177: 
178: ---
179: 
180: ## Common Pitfalls
181: ‚ùå Showing baseline during verification ‚Üí confirmation bias persists
182: ‚ùå Vague verification questions ‚Üí unhelpful answers
183: ‚ùå Too few verifications ‚Üí miss errors
184: ‚ùå Verifying opinions/interpretations ‚Üí not factually checkable
185: 
186: ‚úÖ **Fix**: Independent context, specific questions, verify facts not opinions
187: 
188: ---
189: 
190: ## Advanced: Factored Verification
191: 
192: Break complex claims into sub-claims:
193: 
194: ```python
195: def factored_verification(claim):
196:     """
197:     Verify complex claim by parts
198:     """
199:     # Claim: "Marie Curie won Nobel Prize in Physics in 1903"
200:     
201:     sub_verifications = [
202:         "Did Marie Curie win a Nobel Prize?",  # Base fact
203:         "Was it in Physics?",  # Field
204:         "Was it in 1903?",  # Year
205:     ]
206:     
207:     for q in sub_verifications:
208:         answer = verify_independently(q)
209:         if "no" in answer.lower():
210:             return {'verified': False, 'failed_at': q}
211:     
212:     return {'verified': True}
213: ```
214: 
215: **Benefit**: Pinpoints exactly what's wrong in complex claims
216: 
217: ---
218: 
219: ## Combinations
220: 
221: | Combine With | Benefit | Use Case |
222: |--------------|---------|----------|
223: | **RAG** | Verify against retrieved docs | Document-based QA |
224: | **Self-Refine** | Verify + improve quality | Content creation |
225: | **ToT** | Verify ToT solution paths | Complex reasoning |
226: 
227: **Best**: RAG + CoVe = retrieve knowledge, then verify against it
228: 
229: ---
230: 
231: ## Example: Biography
232: 
233: **Query**: "Write brief bio of Marie Curie"
234: 
235: **‚ë† Baseline**:
236: ```
237: Marie Curie (1867-1934) discovered radium in 1898.
238: She won Nobel Prize in 1903, first woman to win.
239: Won second Nobel in 1911, first person to win two.
240: ```
241: 
242: **‚ë° Verification Questions**:
243: ```
244: 1. Was Marie Curie born in 1867?
245: 2. Did she discover radium in 1898?
246: 3. Did she win Nobel Prize in 1903?
247: 4. Was she first woman to win Nobel?
248: 5. Did she win second Nobel in 1911?
249: 6. Was she first to win two Nobel Prizes?
250: ```
251: 
252: **‚ë¢ Independent Verification** (no baseline shown):
253: ```
254: 1. Yes, 1867 ‚úì
255: 2. Yes, December 1898 (also polonium July 1898) ‚ö†Ô∏è
256: 3. Yes, Physics 1903 ‚úì
257: 4. Yes, first woman ‚úì
258: 5. Yes, Chemistry 1911 ‚úì
259: 6. Yes, first person ‚úì
260: ```
261: 
262: **‚ë£ Final (Corrected)**:
263: ```
264: Marie Curie (1867-1934) discovered polonium (July 1898)
265: and radium (December 1898). She won Nobel Prize in 
266: Physics in 1903, first woman to win. Won second Nobel 
267: in Chemistry in 1911, first person to win two Nobel Prizes.
268: ```
269: 
270: **Change**: Added polonium, specified discovery months
271: 
272: ---
273: 
274: ## Verification Success Rate
275: 
276: Monitor how often verifications find errors:
277: 
278: ```python
279: def track_verification_impact(baseline, verified):
280:     """
281:     Measure verification effectiveness
282:     """
283:     changes = count_edits(baseline, verified)
284:     
285:     if changes > 0:
286:         return {
287:             'corrections_made': changes,
288:             'effectiveness': 'high'  # Found and fixed errors
289:         }
290:     else:
291:         return {
292:             'corrections_made': 0,
293:             'effectiveness': 'low_or_accurate'  # Either baseline was good or verifications missed errors
294:         }
295: ```
296: 
297: **Target**: 20-40% of responses should be corrected (indicates working)
298: 
299: ---
300: 
301: ## Production Checklist
302: - [ ] Baseline generation at temp 0.5-0.7 (moderate creativity)
303: - [ ] Verification questions specific and factual
304: - [ ] Independent verification (NO baseline context)
305: - [ ] Verification at temp 0.0 (deterministic)
306: - [ ] Final revision incorporates all verifications
307: - [ ] Monitor correction rate (should be 20-40%)
308: 
309: ---
310: 
311: ## Fast Variant: Critical Facts Only
312: 
313: For cost reduction, verify only critical facts:
314: 
315: ```python
316: def verify_critical_only(response):
317:     """
318:     Verify only high-risk claims
319:     """
320:     # Identify factual claims
321:     claims = extract_factual_claims(response)
322:     
323:     # Score criticality
324:     critical = [
325:         c for c in claims
326:         if is_critical(c)  # Numbers, dates, names, causal claims
327:     ]
328:     
329:     # Verify only critical (~30% of all claims)
330:     verifications = [verify(c) for c in critical]
331:     
332:     return revise_critical_only(response, verifications)
333: ```
334: 
335: **Benefit**: ~40% cost reduction with ~80% effectiveness retention
336: 
337: ---
338: 
339: ## Research
340: **Dhuliawala et al. 2023** - "Chain-of-Verification Reduces Hallucination in Large Language Models"
341: üìÑ https://arxiv.org/abs/2309.11495
342: 
343: ---
344: 
345: **Related Techniques**: [[Self-Refine]], [[RAG]], [[Recitation-Augmented]]
346: **Full Guide**: [[04-quality-assurance-guide#Chain of Verification]]
</file>

<file path="__LOCAL-REPO/__exemplar/__import/__master-exemplar/qrc-rag.md">
  1: ---
  2: tags: #quick-reference #rag #retrieval #knowledge-integration #one-pager
  3: type: quick-reference
  4: technique: Retrieval-Augmented Generation
  5: category: knowledge-integration
  6: ---
  7: 
  8: # üìö Retrieval-Augmented Generation (RAG) - Quick Reference
  9: 
 10: ## One-Line Summary
 11: Retrieve relevant documents from knowledge base, include as context in prompt, LLM generates grounded answer.
 12: 
 13: ---
 14: 
 15: ## When to Use
 16: ‚úÖ **Perfect For**: Factual QA, current events, enterprise knowledge bases, reducing hallucination
 17: ‚ùå **Skip For**: Commonsense reasoning (LLM already knows), creative tasks, pure calculations
 18: 
 19: ---
 20: 
 21: ## Architecture
 22: 
 23: ```
 24: User Query
 25:     ‚Üì
 26: ‚ë† RETRIEVE relevant docs (vector search)
 27:     ‚Üì  
 28: ‚ë° FORMAT as context
 29:     ‚Üì
 30: ‚ë¢ GENERATE answer with context
 31:     ‚Üì
 32: Final Answer (grounded in docs)
 33: ```
 34: 
 35: ---
 36: 
 37: ## Implementation Template
 38: 
 39: ```python
 40: def rag_answer(query, knowledge_base, top_k=5):
 41:     """
 42:     Basic RAG implementation
 43:     """
 44:     # Step 1: Retrieve documents
 45:     retrieved = knowledge_base.retrieve(query, top_k=top_k)
 46:     
 47:     # Step 2: Format context
 48:     context = "\n\n".join([
 49:         f"[{i+1}] {doc['text']}"
 50:         for i, doc in enumerate(retrieved)
 51:     ])
 52:     
 53:     # Step 3: Generate with context
 54:     prompt = f"""Answer based on the context below.
 55: 
 56: Context:
 57: {context}
 58: 
 59: Question: {query}
 60: 
 61: Answer:"""
 62:     
 63:     answer = llm.complete(prompt, temperature=0.3)
 64:     
 65:     return {
 66:         'answer': answer,
 67:         'sources': retrieved
 68:     }
 69: ```
 70: 
 71: ---
 72: 
 73: ## Knowledge Base Setup
 74: 
 75: ```python
 76: from sentence_transformers import SentenceTransformer
 77: import numpy as np
 78: 
 79: class VectorKB:
 80:     def __init__(self):
 81:         self.model = SentenceTransformer('all-MiniLM-L6-v2')
 82:         self.documents = []
 83:         self.embeddings = None
 84:     
 85:     def add_documents(self, docs):
 86:         """Add documents with embeddings"""
 87:         self.documents.extend(docs)
 88:         texts = [d['text'] for d in docs]
 89:         new_embs = self.model.encode(texts)
 90:         
 91:         if self.embeddings is None:
 92:             self.embeddings = new_embs
 93:         else:
 94:             self.embeddings = np.vstack([self.embeddings, new_embs])
 95:     
 96:     def retrieve(self, query, top_k=5):
 97:         """Semantic search"""
 98:         query_emb = self.model.encode([query])[0]
 99:         scores = np.dot(self.embeddings, query_emb)
100:         top_idx = np.argsort(scores)[-top_k:][::-1]
101:         
102:         return [
103:             {'document': self.documents[i], 'score': scores[i]}
104:             for i in top_idx
105:         ]
106: ```
107: 
108: ---
109: 
110: ## Prompt Template
111: 
112: ```
113: Answer the question based on the context provided.
114: If the context doesn't contain enough information, say so.
115: 
116: Context:
117: [Document 1] {text}
118: 
119: [Document 2] {text}
120: 
121: [Document 3] {text}
122: 
123: Question: {query}
124: 
125: Instructions:
126: - Base answer on context above
127: - Cite sources using [Document N] format
128: - If uncertain, acknowledge gaps
129: 
130: Answer:
131: ```
132: 
133: ---
134: 
135: ## Performance Benchmarks
136: - **Natural Questions**: 54.7% (vs 32.1% LLM only) - **+22.6pp**
137: - **TriviaQA**: 68.4% (vs 58.3% LLM only) - **+10.1pp**
138: - **Hallucination Reduction**: 15% ‚Üí **3-5%** with RAG
139: 
140: ---
141: 
142: ## Costs
143: - **Embedding**: One-time per document (~$0.0001 per 1K tokens)
144: - **Retrieval**: ~1ms per query (fast!)
145: - **Generation**: Same as baseline + context tokens
146: - **Total**: ~2-3x baseline cost
147: 
148: ---
149: 
150: ## Key Parameters
151: 
152: | Parameter | Range | Recommendation |
153: |-----------|-------|----------------|
154: | **top_k** | 3-10 | 5 for general, 10 for complex |
155: | **chunk_size** | 200-1000 tokens | 512 for balance |
156: | **embedding_model** | Various | MiniLM (fast), E5 (quality) |
157: 
158: ---
159: 
160: ## Document Chunking
161: 
162: Critical for quality retrieval:
163: 
164: ```python
165: def chunk_document(text, chunk_size=512, overlap=50):
166:     """
167:     Split document with overlap
168:     """
169:     words = text.split()
170:     chunks = []
171:     
172:     for i in range(0, len(words), chunk_size - overlap):
173:         chunk = ' '.join(words[i:i + chunk_size])
174:         chunks.append(chunk)
175:     
176:     return chunks
177: ```
178: 
179: **Best Practices**:
180: - Chunk size 256-512 tokens (balance specificity vs context)
181: - Overlap 10-20% (preserve continuity)
182: - Respect semantic boundaries (paragraphs, sections)
183: 
184: ---
185: 
186: ## Common Pitfalls
187: ‚ùå Documents too long ‚Üí poor retrieval granularity
188: ‚ùå No overlap ‚Üí context breaks at chunk boundaries  
189: ‚ùå Generic embeddings ‚Üí poor domain performance
190: ‚ùå No metadata ‚Üí can't filter results
191: ‚ùå top_k too low ‚Üí miss relevant docs
192: 
193: ‚úÖ **Fix**: Chunk properly, tune top_k, use metadata filtering
194: 
195: ---
196: 
197: ## Advanced: Reranking
198: 
199: ```python
200: def rag_with_rerank(query, kb, initial_k=20, final_k=5):
201:     """
202:     Retrieve many, rerank with LLM, keep best
203:     """
204:     # Initial retrieval (broad)
205:     candidates = kb.retrieve(query, top_k=initial_k)
206:     
207:     # Rerank with LLM
208:     reranked = []
209:     for doc in candidates:
210:         score_prompt = f"""Rate relevance (0-10):
211: 
212: Query: {query}
213: Document: {doc['text'][:200]}...
214: 
215: Score:"""
216:         score = float(llm.complete(score_prompt, temp=0.0))
217:         reranked.append({**doc, 'llm_score': score})
218:     
219:     # Sort by LLM score, take top final_k
220:     reranked.sort(key=lambda x: x['llm_score'], reverse=True)
221:     top_docs = reranked[:final_k]
222:     
223:     # Generate with reranked docs
224:     return rag_answer_from_docs(query, top_docs)
225: ```
226: 
227: **Benefit**: +5-10pp accuracy vs basic RAG
228: **Cost**: Adds N LLM calls for scoring
229: 
230: ---
231: 
232: ## Advanced: Hybrid Search
233: 
234: ```python
235: def hybrid_retrieval(query, kb):
236:     """
237:     Combine semantic (dense) + keyword (sparse)
238:     """
239:     # Semantic retrieval
240:     semantic_results = kb.semantic_search(query, top_k=10)
241:     
242:     # Keyword retrieval (BM25)
243:     keyword_results = kb.keyword_search(query, top_k=10)
244:     
245:     # Merge and deduplicate
246:     combined = merge_results(
247:         semantic_results,
248:         keyword_results,
249:         weights={'semantic': 0.7, 'keyword': 0.3}
250:     )
251:     
252:     return combined[:5]
253: ```
254: 
255: **Benefit**: Better on both semantic and exact match queries
256: 
257: ---
258: 
259: ## Combinations
260: 
261: | Combine With | Benefit | Use Case |
262: |--------------|---------|----------|
263: | **CoVe** | Verify retrieved facts | High-accuracy needs |
264: | **Self-Refine** | Polish answer quality | Content generation |
265: | **ReAct** | Agent-driven retrieval | Multi-step research |
266: 
267: ---
268: 
269: ## Metadata Filtering
270: 
271: ```python
272: # Add metadata during indexing
273: kb.add_documents([
274:     {
275:         'text': 'Document content...',
276:         'metadata': {
277:             'source': 'research_paper',
278:             'date': '2023-05-15',
279:             'category': 'medicine'
280:         }
281:     }
282: ])
283: 
284: # Filter during retrieval
285: results = kb.retrieve(
286:     query,
287:     filters={'category': 'medicine', 'date': {'$gte': '2023-01-01'}}
288: )
289: ```
290: 
291: ---
292: 
293: ## Production Checklist
294: - [ ] Documents chunked appropriately (256-512 tokens)
295: - [ ] Embeddings generated and indexed
296: - [ ] Metadata included (source, date, category)
297: - [ ] top_k tuned for use case
298: - [ ] Citation format specified in prompt
299: - [ ] Fallback for "not enough context" cases
300: - [ ] Monitoring retrieval quality
301: 
302: ---
303: 
304: ## Example: Customer Support
305: 
306: **Knowledge Base**: Product documentation, FAQs, support articles
307: 
308: **Query**: "How do I reset my password?"
309: 
310: **Retrieved**:
311: ```
312: [1] To reset password: Go to Settings > Security > Reset Password
313: [2] Password requirements: 12+ chars, uppercase, number, symbol
314: [3] If forgot password: Click "Forgot?" on login page
315: ```
316: 
317: **Answer**: "To reset your password, go to Settings > Security > Reset Password [1]. If you've forgotten your password, click 'Forgot Password?' on the login page [3]."
318: 
319: ---
320: 
321: ## Research
322: **Lewis et al. 2020** - "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
323: üìÑ https://arxiv.org/abs/2005.11401
324: 
325: ---
326: 
327: **Related Techniques**: [[Generated Knowledge]], [[Recitation-Augmented]], [[Chain of Verification]]
328: **Full Guide**: [[05-knowledge-integration-guide#RAG]]
</file>

<file path="__LOCAL-REPO/__exemplar/__import/__master-exemplar/qrc-self-consistency.md">
  1: ---
  2: tags: #quick-reference #self-consistency #ensemble #one-pager
  3: type: quick-reference
  4: technique: Self-Consistency
  5: category: reasoning
  6: ---
  7: 
  8: # üéØ Self-Consistency - Quick Reference
  9: 
 10: ## One-Line Summary
 11: Generate multiple reasoning paths (temperature > 0), vote on final answers for robust majority consensus.
 12: 
 13: ---
 14: 
 15: ## When to Use
 16: ‚úÖ **Perfect For**: Mathematical reasoning, commonsense QA, any task where answer has clear right/wrong
 17: ‚ùå **Skip For**: Creative writing (no "correct" answer), single-fact lookup, latency-critical tasks
 18: 
 19: ---
 20: 
 21: ## Algorithm
 22: 
 23: ```
 24: INPUT: Question
 25: OUTPUT: Most consistent answer
 26: 
 27: 1. Generate N diverse reasoning paths
 28:    (same question, temperature 0.7-1.0, N = 3-10)
 29: 
 30: 2. Extract final answer from each path
 31: 
 32: 3. Vote: Return majority answer
 33: 
 34: 4. Optional: Calculate confidence = majority_count / N
 35: ```
 36: 
 37: ---
 38: 
 39: ## Implementation Template
 40: 
 41: ```python
 42: def self_consistency(question, num_samples=5, temperature=0.7):
 43:     """
 44:     Self-Consistency implementation
 45:     """
 46:     from collections import Counter
 47:     
 48:     # Generate diverse reasoning paths
 49:     answers = []
 50:     for _ in range(num_samples):
 51:         response = llm.complete(
 52:             question,
 53:             temperature=temperature
 54:         )
 55:         answer = extract_final_answer(response)
 56:         answers.append(answer)
 57:     
 58:     # Vote on answers
 59:     answer_counts = Counter(answers)
 60:     final_answer = answer_counts.most_common(1)[0][0]
 61:     confidence = answer_counts[final_answer] / num_samples
 62:     
 63:     return {
 64:         'answer': final_answer,
 65:         'confidence': confidence,
 66:         'all_answers': answers
 67:     }
 68: ```
 69: 
 70: ---
 71: 
 72: ## Prompt Template
 73: 
 74: ```
 75: Question: {question}
 76: 
 77: Let's solve this step by step:
 78: 
 79: [LLM generates reasoning]
 80: 
 81: Therefore, the answer is: [final answer]
 82: ```
 83: 
 84: **Key**: Ask N times with temperature > 0 for diverse paths
 85: 
 86: ---
 87: 
 88: ## Parameter Tuning
 89: 
 90: | Parameter | Low Value | High Value | Recommendation |
 91: |-----------|-----------|------------|----------------|
 92: | **N (samples)** | 3 | 10-20 | 5 for standard, 10 for critical |
 93: | **Temperature** | 0.5 | 1.0 | 0.7-0.8 for good diversity |
 94: 
 95: **Trade-off**: More samples = higher confidence but linear cost increase
 96: 
 97: ---
 98: 
 99: ## Performance Benchmarks
100: - **GSM8K (Math)**: 74.4% (vs 46.9% CoT alone) - **+27.5pp**
101: - **StrategyQA**: 76.2% (vs 68.7% CoT alone) - **+7.5pp**
102: - **ARC (Science)**: 81.5% (vs 75.2% CoT alone) - **+6.3pp**
103: 
104: **Pattern**: Consistent +5-15pp improvement across reasoning tasks
105: 
106: ---
107: 
108: ## Costs
109: - **Token Cost**: N √ó baseline (5 samples = 5x cost)
110: - **Latency**: Can parallelize! (5 samples = 1x latency if parallel)
111: - **Best Practice**: Start with N=3, increase for critical tasks
112: 
113: ---
114: 
115: ## Answer Extraction
116: 
117: Critical step: Extract final answer reliably
118: 
119: ```python
120: def extract_final_answer(response):
121:     """
122:     Extract answer from reasoning chain
123:     """
124:     # Pattern 1: "Therefore, ..."
125:     if "therefore" in response.lower():
126:         return extract_after_keyword(response, "therefore")
127:     
128:     # Pattern 2: "The answer is ..."
129:     if "answer is" in response.lower():
130:         return extract_after_keyword(response, "answer is")
131:     
132:     # Pattern 3: Last sentence
133:     return response.split('.')[-2].strip()
134: ```
135: 
136: ---
137: 
138: ## Common Pitfalls
139: ‚ùå Temperature too low (0.0-0.3) ‚Üí identical answers (voting useless)
140: ‚ùå Temperature too high (>1.2) ‚Üí nonsense answers
141: ‚ùå Poor answer extraction ‚Üí different phrasings counted as different answers
142: ‚ùå N too small (N=2) ‚Üí ties, low confidence
143: 
144: ‚úÖ **Fix**: Use temp 0.7-0.8, normalize answers before voting, N ‚â• 5
145: 
146: ---
147: 
148: ## Advanced: Adaptive Sample Count
149: 
150: ```python
151: def adaptive_self_consistency(question, min_samples=3, max_samples=10):
152:     """
153:     Stop early if high confidence reached
154:     """
155:     from collections import Counter
156:     
157:     answers = []
158:     
159:     for i in range(max_samples):
160:         # Generate answer
161:         answer = generate_answer(question, temperature=0.7)
162:         answers.append(answer)
163:         
164:         # Check confidence after min_samples
165:         if i >= min_samples - 1:
166:             counts = Counter(answers)
167:             max_count = counts.most_common(1)[0][1]
168:             confidence = max_count / len(answers)
169:             
170:             # Stop if high confidence
171:             if confidence >= 0.7:  # 70%+ agree
172:                 break
173:     
174:     final = Counter(answers).most_common(1)[0][0]
175:     return final
176: ```
177: 
178: ---
179: 
180: ## Combinations
181: 
182: | Combine With | Benefit | Use Case |
183: |--------------|---------|----------|
184: | **Chain of Thought** | Base method | Always use together |
185: | **ToT** | Validate ToT solution | High-stakes decisions |
186: | **PoT** | Vote on program outputs | Critical calculations |
187: 
188: **Note**: Self-Consistency enhances any technique that can generate multiple attempts
189: 
190: ---
191: 
192: ## Confidence Interpretation
193: 
194: | Confidence | Interpretation | Action |
195: |------------|----------------|--------|
196: | **‚â• 80%** | Very high agreement | Trust answer |
197: | **60-79%** | Moderate agreement | Acceptable |
198: | **40-59%** | Low agreement | Increase N or investigate |
199: | **< 40%** | No consensus | Problem unclear or very hard |
200: 
201: ---
202: 
203: ## Example: Math Problem
204: 
205: **Question**: "Roger has 5 tennis balls. He buys 2 cans, each with 3 balls. How many balls does he have?"
206: 
207: **5 Samples**:
208: ```
209: Sample 1: "5 + 2√ó3 = 5 + 6 = 11 balls" ‚Üí 11
210: Sample 2: "2 cans √ó 3 balls = 6. 5 + 6 = 11" ‚Üí 11
211: Sample 3: "He has 5, buys 6 more, total 11" ‚Üí 11
212: Sample 4: "Initial 5 + (2√ó3) = 11 balls" ‚Üí 11
213: Sample 5: "2 + 3 = 5 cans? No, 2 cans... 11 total" ‚Üí 11
214: ```
215: 
216: **Vote**: 11 appears 5/5 times ‚Üí **100% confidence** ‚Üí Answer: 11
217: 
218: ---
219: 
220: ## Research
221: **Wang et al. 2022** - "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
222: üìÑ https://arxiv.org/abs/2203.11171
223: 
224: ---
225: 
226: **Related Techniques**: [[Chain of Thought]], [[Tree of Thoughts]], [[Program of Thoughts]]
227: **Full Guide**: [[01-reasoning-techniques-guide#Self-Consistency]]
</file>

<file path="__LOCAL-REPO/__exemplar/__import/__master-exemplar/qrc-tree-of-thoughts.md">
  1: ---
  2: tags: #quick-reference #tree-of-thoughts #tot #one-pager
  3: type: quick-reference
  4: technique: Tree of Thoughts
  5: category: reasoning
  6: ---
  7: 
  8: # üå≥ Tree of Thoughts (ToT) - Quick Reference
  9: 
 10: ## One-Line Summary
 11: Systematic exploration of reasoning paths using tree search (BFS/DFS) with explicit state evaluation and backtracking.
 12: 
 13: ---
 14: 
 15: ## When to Use
 16: ‚úÖ **Perfect For**: Complex planning, problems with dead ends, Game of 24, creative writing with constraints
 17: ‚ùå **Skip For**: Simple factual queries, speed-critical tasks, straightforward reasoning
 18: 
 19: ---
 20: 
 21: ## Core Components
 22: 
 23: ```
 24: 1. THOUGHT DECOMPOSITION
 25:    Define what constitutes one "thought" (reasoning step)
 26: 
 27: 2. THOUGHT GENERATOR  
 28:    LLM generates k candidate next thoughts
 29: 
 30: 3. STATE EVALUATOR
 31:    Score each thought's promise (0-10 or IMPOSSIBLE/MAYBE/LIKELY/SOLVED)
 32: 
 33: 4. SEARCH ALGORITHM
 34:    BFS (optimal path) or DFS (lower cost)
 35: ```
 36: 
 37: ---
 38: 
 39: ## Implementation Template
 40: 
 41: ```python
 42: def tree_of_thoughts(problem, max_depth=4, branching=3):
 43:     """
 44:     BFS implementation
 45:     """
 46:     from collections import deque
 47:     
 48:     queue = deque([{'state': initial_state, 'depth': 0}])
 49:     
 50:     while queue:
 51:         current = queue.popleft()
 52:         
 53:         # Check if solved
 54:         if is_solved(current['state']):
 55:             return current
 56:         
 57:         # Don't exceed depth
 58:         if current['depth'] >= max_depth:
 59:             continue
 60:         
 61:         # Generate next thoughts
 62:         thoughts = generate_thoughts(current['state'], k=branching)
 63:         
 64:         # Evaluate and add promising ones
 65:         for thought in thoughts:
 66:             score = evaluate(thought)
 67:             if score >= threshold:
 68:                 queue.append({
 69:                     'state': thought,
 70:                     'depth': current['depth'] + 1
 71:                 })
 72: ```
 73: 
 74: ---
 75: 
 76: ## Prompt Templates
 77: 
 78: ### Thought Generation
 79: ```
 80: Current state: {state}
 81: Goal: {goal}
 82: 
 83: Generate {k} different next steps.
 84: 
 85: Next steps:
 86: 1.
 87: 2.
 88: 3.
 89: ```
 90: 
 91: ### State Evaluation
 92: ```
 93: Goal: {goal}
 94: Current state: {state}
 95: 
 96: Rate this state:
 97: - IMPOSSIBLE: No path to solution
 98: - MAYBE: Uncertain
 99: - LIKELY: Clear path visible
100: - SOLVED: Goal reached
101: 
102: Assessment:
103: ```
104: 
105: ---
106: 
107: ## Performance Benchmarks
108: - **Game of 24**: 74% success (vs 7.3% baseline) - **10x improvement**
109: - **Creative Writing**: 45% preference (vs 20% standard)
110: - **Crosswords**: 62% success (vs 16% greedy)
111: 
112: ---
113: 
114: ## Costs
115: - **Token Cost**: 5-15x baseline (depends on branching factor & depth)
116: - **Latency**: High (sequential state generation)
117: - **Best Practices**: Use DFS for cost-sensitive, BFS for optimal solutions
118: 
119: ---
120: 
121: ## Common Pitfalls
122: ‚ùå Too deep tree (depth > 5) ‚Üí exponential explosion
123: ‚ùå Poor state evaluation ‚Üí explores dead ends
124: ‚ùå Too low branching ‚Üí misses solution
125: ‚ùå No pruning ‚Üí wastes tokens on hopeless paths
126: 
127: ‚úÖ **Fix**: Prune aggressively, limit depth, tune branching to problem
128: 
129: ---
130: 
131: ## Combinations
132: 
133: | Combine With | Benefit | Use Case |
134: |--------------|---------|----------|
135: | **Self-Consistency** | Validate solution | High-stakes planning |
136: | **PoT** | Code-based evaluation | Game of 24 |
137: | **RAG** | Knowledge-grounded reasoning | Research tasks |
138: 
139: ---
140: 
141: ## Example: Game of 24
142: 
143: **Input**: Numbers [4, 5, 6, 10], Goal: Reach 24
144: 
145: **Tree Exploration**:
146: ```
147: Root: [4, 5, 6, 10]
148:   ‚îú‚îÄ 6 √ó 4 = 24 ‚úì SOLVED!
149:   ‚îú‚îÄ 10 - 6 = 4 ‚Üí [4, 4, 5]
150:   ‚îÇ   ‚îú‚îÄ 4 + 4 = 8 ‚Üí [8, 5]
151:   ‚îÇ   ‚îÇ   ‚îî‚îÄ 8 √ó 5 = 40 ‚úó (wrong)
152:   ‚îÇ   ‚îî‚îÄ 4 √ó 5 = 20 ‚Üí [20, 4]
153:   ‚îÇ       ‚îî‚îÄ 20 + 4 = 24 ‚úì SOLVED!
154:   ‚îî‚îÄ 5 + 4 = 9 ‚Üí [9, 6, 10]
155:       ‚îî‚îÄ ...
156: ```
157: 
158: **Best Path**: 6 √ó 4 = 24 (depth 1, immediate solution)
159: 
160: ---
161: 
162: ## Research
163: **Yao et al. 2023** - "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
164: üìÑ https://arxiv.org/abs/2305.10601
165: 
166: ---
167: 
168: **Related Techniques**: [[Graph of Thoughts]], [[Self-Consistency]], [[Program of Thoughts]]
169: **Full Guide**: [[01-reasoning-techniques-guide#Tree of Thoughts]]
</file>

<file path="__LOCAL-REPO/__exemplar/__import/__master-exemplar/README.md">
  1: # Advanced Prompt Engineering Guide System
  2: 
  3: **Complete Reference System for State-of-the-Art LLM Techniques (2022-2025 Research)**
  4: 
  5: ---
  6: 
  7: ## üì¶ What's Included
  8: 
  9: This system provides **production-ready implementations** and comprehensive documentation for 20+ advanced prompt engineering techniques across 6 categories:
 10: 
 11: ### üìö Core Guides (6)
 12: 
 13: 1. **00-advanced-prompt-engineering-index.md** - Master navigation hub
 14: 2. **01-reasoning-techniques-guide.md** - ToT, GoT, Self-Consistency, PoT, SoT
 15: 3. **02-agentic-frameworks-guide.md** - ReAct, Reflexion, ART, ReWOO
 16: 4. **03-meta-optimization-guide.md** - APE, OPRO, PromptBreeder, Active-Prompt
 17: 5. **04-quality-assurance-guide.md** - Chain of Verification, Self-Refine
 18: 6. **05-knowledge-integration-guide.md** - Generated Knowledge, RAG, Recitation
 19: 7. **06-integration-patterns-guide.md** - Combining techniques effectively
 20: 
 21: ### üéØ Quick Reference Cards (5)
 22: 
 23: - **qrc-tree-of-thoughts.md** - ToT one-pager
 24: - **qrc-self-consistency.md** - Self-Consistency one-pager
 25: - **qrc-rag.md** - RAG one-pager
 26: - **qrc-chain-of-verification.md** - CoVe one-pager
 27: - **qrc-react.md** - ReAct one-pager *(if created)*
 28: 
 29: ---
 30: 
 31: ## üéØ Quick Start
 32: 
 33: ### For Beginners
 34: 1. Start with **00-advanced-prompt-engineering-index.md**
 35: 2. Use the decision trees to find techniques for your use case
 36: 3. Read the relevant quick reference card (qrc-*.md)
 37: 4. Refer to full guide for implementation details
 38: 
 39: ### For Practitioners
 40: 1. Check **06-integration-patterns-guide.md** for combination strategies
 41: 2. Review compatibility matrix before combining techniques
 42: 3. Use production architectures section for system design
 43: 4. Monitor performance benchmarks for ROI analysis
 44: 
 45: ### For Researchers
 46: 1. Full guides contain citations to original papers
 47: 2. Performance benchmarks across standard datasets
 48: 3. Implementation details for reproducibility
 49: 4. Research frontiers and open questions
 50: 
 51: ---
 52: 
 53: ## üìä Technique Overview
 54: 
 55: ### By Category
 56: 
 57: | Category | Techniques | Use For |
 58: |----------|-----------|---------|
 59: | **Reasoning** | ToT, GoT, Self-Consistency, PoT, SoT | Complex planning, math, structured thinking |
 60: | **Agentic** | ReAct, Reflexion, ART, ReWOO | Tool use, multi-step research, learning from errors |
 61: | **Meta-Optimization** | APE, OPRO, PromptBreeder | Automatically improving prompts at scale |
 62: | **Quality Assurance** | CoVe, Self-Refine | Reducing hallucination, iterative improvement |
 63: | **Knowledge Integration** | Generated Knowledge, RAG | Grounding in external knowledge, factual accuracy |
 64: 
 65: ### By Performance Impact
 66: 
 67: | Technique | Typical Improvement | Cost Multiplier | Best For |
 68: |-----------|-------------------|-----------------|----------|
 69: | **Self-Consistency** | +5-15pp | 5x | Math, reasoning (easy win) |
 70: | **RAG** | +10-25pp | 2-3x | Factual QA (essential for accuracy) |
 71: | **Chain of Verification** | -50% hallucination | 4x | High-stakes content (worth the cost) |
 72: | **Tree of Thoughts** | +30-60pp | 10-15x | Complex planning (when needed) |
 73: | **ReAct** | +20-40pp | 3-5x | Multi-step tasks with tools |
 74: 
 75: ---
 76: 
 77: ## üîç Finding the Right Technique
 78: 
 79: ### By Use Case
 80: 
 81: **"I need to reduce hallucinations"**
 82: ‚Üí Chain of Verification (CoVe) or RAG
 83: 
 84: **"I need to solve complex planning problems"**
 85: ‚Üí Tree of Thoughts (ToT) or Graph of Thoughts (GoT)
 86: 
 87: **"I need reliable mathematical reasoning"**
 88: ‚Üí Self-Consistency + Program of Thoughts (PoT)
 89: 
 90: **"I need to access current/external information"**
 91: ‚Üí Retrieval-Augmented Generation (RAG)
 92: 
 93: **"I need an agent that can use tools"**
 94: ‚Üí ReAct or ART
 95: 
 96: **"I need to improve prompt quality automatically"**
 97: ‚Üí OPRO or PromptBreeder
 98: 
 99: **"I need to improve response quality iteratively"**
100: ‚Üí Self-Refine
101: 
102: **"I need an agent that learns from mistakes"**
103: ‚Üí Reflexion
104: 
105: ### By Constraints
106: 
107: **Cost-Conscious**
108: ‚Üí Self-Consistency (5x), RAG (2-3x), Generated Knowledge (2x)
109: 
110: **Latency-Sensitive**
111: ‚Üí RAG (can parallelize), Generated Knowledge, avoid ToT/GoT
112: 
113: **Maximum Quality**
114: ‚Üí Combine: RAG + ToT + CoVe + Self-Consistency (expensive but best)
115: 
116: ---
117: 
118: ## üí° High-Value Combinations
119: 
120: ### RAG + Chain of Verification
121: **Use**: High-accuracy factual content
122: **Benefit**: Knowledge grounding + verification = 3-5% hallucination
123: **Cost**: 6-8x baseline
124: 
125: ### ToT + Self-Consistency
126: **Use**: Critical planning/decisions
127: **Benefit**: Deep exploration + robustness = highest quality reasoning
128: **Cost**: 15-20x baseline
129: 
130: ### Generated Knowledge + RAG
131: **Use**: Complex topics needing background + facts
132: **Benefit**: Contextual understanding + specific evidence
133: **Cost**: 3-4x baseline
134: 
135: ### ReAct + RAG
136: **Use**: Multi-step research
137: **Benefit**: Adaptive retrieval based on reasoning progress
138: **Cost**: Variable (3-10x depending on tool use)
139: 
140: ---
141: 
142: ## üìñ Implementation Guide
143: 
144: ### Step 1: Choose Technique
145: Use decision trees in index or integration guide
146: 
147: ### Step 2: Read Quick Reference
148: Get overview from qrc-[technique].md
149: 
150: ### Step 3: Implement
151: Copy code from full guide, adapt to your use case
152: 
153: ### Step 4: Test & Tune
154: - Start with recommended parameters
155: - A/B test variations
156: - Monitor key metrics
157: 
158: ### Step 5: Scale
159: - See production architectures in integration guide
160: - Implement tiered quality system if needed
161: - Monitor costs vs benefits
162: 
163: ---
164: 
165: ## üéì Learning Path
166: 
167: ### Week 1: Foundations
168: - [ ] Read index and understand categories
169: - [ ] Implement Self-Consistency (easiest advanced technique)
170: - [ ] Implement RAG (essential for production)
171: - [ ] Test both on your use cases
172: 
173: ### Week 2: Reasoning
174: - [ ] Implement Tree of Thoughts for complex problem
175: - [ ] Implement Program of Thoughts for math task
176: - [ ] Compare ToT + Self-Consistency combination
177: 
178: ### Week 3: Quality
179: - [ ] Implement Chain of Verification
180: - [ ] Implement Self-Refine
181: - [ ] Test CoVe + Self-Refine combination
182: 
183: ### Week 4: Agentic & Advanced
184: - [ ] Implement ReAct agent
185: - [ ] Experiment with technique combinations
186: - [ ] Design production architecture for your use case
187: 
188: ---
189: 
190: ## üìä Performance Tracking
191: 
192: ### Recommended Metrics
193: 
194: **Accuracy-Focused Tasks**:
195: - Exact match accuracy
196: - F1 score (for partial matches)
197: - Hallucination rate
198: - Factual correctness
199: 
200: **Quality-Focused Tasks**:
201: - Human quality ratings (1-10)
202: - Coherence scores
203: - Completeness checks
204: - Style adherence
205: 
206: **Efficiency Metrics**:
207: - Token usage per task
208: - Latency (P50, P95, P99)
209: - Cost per successful completion
210: - Success rate
211: 
212: ### A/B Testing Template
213: 
214: ```python
215: def ab_test(baseline_fn, technique_fn, test_cases):
216:     """
217:     Compare baseline vs technique
218:     """
219:     results = {'baseline': [], 'technique': []}
220:     
221:     for test in test_cases:
222:         # Baseline
223:         base_result = baseline_fn(test['input'])
224:         results['baseline'].append(
225:             evaluate(base_result, test['expected'])
226:         )
227:         
228:         # Technique
229:         tech_result = technique_fn(test['input'])
230:         results['technique'].append(
231:             evaluate(tech_result, test['expected'])
232:         )
233:     
234:     # Statistical comparison
235:     return {
236:         'baseline_mean': np.mean(results['baseline']),
237:         'technique_mean': np.mean(results['technique']),
238:         'improvement': np.mean(results['technique']) - np.mean(results['baseline']),
239:         'p_value': ttest_ind(results['baseline'], results['technique']).pvalue
240:     }
241: ```
242: 
243: ---
244: 
245: ## üè≠ Production Patterns
246: 
247: ### Tiered Quality System
248: 
249: ```
250: Low-Stakes Queries ‚Üí Fast (1x cost, <1s)
251: Standard Queries ‚Üí RAG (2-3x cost, 1-2s)
252: Important Queries ‚Üí RAG + CoVe (6-8x cost, 3-5s)
253: Critical Queries ‚Üí Full Pipeline (20-30x cost, 10-20s)
254: ```
255: 
256: ### Adaptive Pipeline
257: 
258: ```
259: 1. Start with basic approach
260: 2. Assess quality/uncertainty
261: 3. Add verification if uncertain
262: 4. Add reasoning if complex
263: 5. Refine if quality low
264: ```
265: 
266: ### Caching Strategy
267: 
268: ```
269: Cache embeddings (RAG)
270: Cache generated knowledge (reuse common knowledge)
271: Cache verification results (for repeated claims)
272: ```
273: 
274: ---
275: 
276: ## üî¨ Research References
277: 
278: Each guide includes citations to original papers. Key papers:
279: 
280: **Reasoning**:
281: - Yao et al. 2023 - Tree of Thoughts (NeurIPS)
282: - Wang et al. 2022 - Self-Consistency (ICLR)
283: - Chen et al. 2022 - Program of Thoughts
284: 
285: **Agentic**:
286: - Yao et al. 2023 - ReAct (ICLR)
287: - Shinn et al. 2023 - Reflexion (NeurIPS)
288: - Paranjape et al. 2023 - ART (ICLR)
289: 
290: **Quality Assurance**:
291: - Dhuliawala et al. 2023 - Chain of Verification
292: - Madaan et al. 2023 - Self-Refine (NeurIPS)
293: 
294: **Knowledge Integration**:
295: - Lewis et al. 2020 - RAG (NeurIPS)
296: - Liu et al. 2022 - Generated Knowledge
297: 
298: **Meta-Optimization**:
299: - Zhou et al. 2023 - APE
300: - Yang et al. 2023 - OPRO
301: - Fernando et al. 2024 - PromptBreeder
302: 
303: ---
304: 
305: ## üõ†Ô∏è Tools & Resources
306: 
307: ### Recommended Libraries
308: 
309: **Embeddings**:
310: - `sentence-transformers` - Semantic embeddings
311: - `openai` - OpenAI embeddings API
312: 
313: **Vector Databases**:
314: - `chromadb` - Simple local vector DB
315: - `pinecone` - Production vector DB
316: - `weaviate` - Open-source vector DB
317: 
318: **LLM APIs**:
319: - `anthropic` - Claude API
320: - `openai` - GPT API
321: - `google-generativeai` - Gemini API
322: 
323: **Utilities**:
324: - `langchain` - LLM application framework
325: - `guidance` - Structured prompting
326: - `outlines` - Constrained generation
327: 
328: ### Integration with PKB Systems
329: 
330: **Obsidian**:
331: - Store guides in vault
332: - Use Dataview for technique queries
333: - Create MOC linking techniques
334: - Track usage with inline fields
335: 
336: **Logseq**:
337: - Import as pages
338: - Use queries to find techniques
339: - Link to implementation notes
340: 
341: **Notion**:
342: - Import as database
343: - Filter by category, complexity
344: - Track experiments
345: 
346: ---
347: 
348: ## üìà Success Stories
349: 
350: ### Case Study: Medical QA (RAG + CoVe + Self-Refine)
351: - **Hallucination**: 15% ‚Üí 2%
352: - **Accuracy**: 78% ‚Üí 94%
353: - **Patient satisfaction**: 7.2/10 ‚Üí 8.9/10
354: 
355: ### Case Study: Financial Research (ReAct + PoT + RAG)
356: - **Task completion**: 65% ‚Üí 89%
357: - **Calculation accuracy**: 85% ‚Üí 98%
358: - **Research depth**: 6.2/10 ‚Üí 7.8/10
359: 
360: ### Case Study: Content Platform (Tiered System)
361: - **Basic tier**: 6.5/10 quality, $0.02/article
362: - **Standard tier**: 7.8/10 quality, $0.08/article
363: - **Premium tier**: 8.9/10 quality, $0.25/article
364: 
365: ---
366: 
367: ## üöÄ Next Steps
368: 
369: 1. **Start Simple**: Implement Self-Consistency or RAG this week
370: 2. **Measure Impact**: A/B test against baseline
371: 3. **Iterate**: Try combinations from integration guide
372: 4. **Scale**: Move to production with tiered architecture
373: 5. **Share**: Document your results, contribute back
374: 
375: ---
376: 
377: ## üìû Support & Community
378: 
379: - **Issues**: Open issues on implementation challenges
380: - **Contributions**: PRs welcome for new techniques, optimizations
381: - **Discussions**: Share results, ask questions, learn from others
382: 
383: ---
384: 
385: ## üìÑ License & Citation
386: 
387: This guide system synthesizes published research (2020-2025). When using techniques in production or research, please cite original papers (referenced in each guide).
388: 
389: **System Version**: 1.0.0
390: **Last Updated**: 2025-12-25
391: **Maintained By**: Advanced Prompt Engineering Community
392: 
393: ---
394: 
395: ## üîñ Quick Links
396: 
397: | Resource | Description |
398: |----------|-------------|
399: | [Master Index](00-advanced-prompt-engineering-index.md) | Start here - navigation hub |
400: | [Reasoning Guide](01-reasoning-techniques-guide.md) | ToT, GoT, SC, PoT, SoT |
401: | [Agentic Guide](02-agentic-frameworks-guide.md) | ReAct, Reflexion, ART, ReWOO |
402: | [Meta-Opt Guide](03-meta-optimization-guide.md) | APE, OPRO, PromptBreeder |
403: | [Quality Guide](04-quality-assurance-guide.md) | CoVe, Self-Refine |
404: | [Knowledge Guide](05-knowledge-integration-guide.md) | RAG, Generated Knowledge |
405: | [Integration Guide](06-integration-patterns-guide.md) | Combining techniques |
406: 
407: **Quick Reference Cards**: `qrc-*.md` files for one-page summaries
408: 
409: ---
410: 
411: *Built with ‚ù§Ô∏è for the prompt engineering community. Last updated: 2025-12-25*
</file>

</files>
