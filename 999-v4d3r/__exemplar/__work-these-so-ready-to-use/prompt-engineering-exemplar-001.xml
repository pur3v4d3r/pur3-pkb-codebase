This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.
The content has been processed where empty lines have been removed, line numbers have been added, security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: __LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques
- Files matching patterns in .gitignore are excluded
- Empty lines have been removed from all files
- Line numbers have been added to the beginning of each line
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Analogical_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Chain_of_Draft_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Chain_of_Symbol_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Chain_of_Translation_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Chain_of_Verification_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Contrastive_CoT_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Cross_Lingual_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Faithful_Chain_of_Thought_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Few_Shot_Chain-of_Thought_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/1-chain-translation-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/1-few-cot-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/1-least-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/1-rephrase-respond-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/1-self-consistency-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/1-self-refine-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/1-zs-cot-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/2-cod-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/2-cove-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/2-cross-lingual-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/2-plan-solve-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/2-self-ask-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/2-step-back-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/2-universal-self-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/3-contrastive-cot-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/3-meta-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/3-multi-chain-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/3-program-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/4-analogical-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/4-chain-symbol-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/4-faithful-cot-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/5-meta-cognitive-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/5-tot-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/6-tcot-prompt.jpg
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/readme.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Least_to_Most_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Meta_Cognitive_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Meta_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Multi_Chain_Reasoning_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Plan_and_Solve_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Program_of_Thoughts_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Rephrase_and_Respond_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Self_Ask_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Self_Consistency_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Self_Refine_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Step_Back_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Tabular_Chain_of_Thought_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Thread_of_Thoughts_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Universal_Self_Consistency_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Zero_Shot_CoT_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Basic_Prompt_Engineering_Techniques/Batch_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Basic_Prompt_Engineering_Techniques/Emotion_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Basic_Prompt_Engineering_Techniques/few_shot_prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Basic_Prompt_Engineering_Techniques/Role_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Basic_Prompt_Engineering_Techniques/Zero_Shot_Prompting.md
__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Analogical_Prompting.md">
  1: # **Analogical Promptiing**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: Analogical prompting is a reasoning technique where the LLM is instructed to recall similar problems (analogies) before solving the main problem. Instead of giving the model examples yourself (as in few-shot prompting), you tell the model to generate its own relevant examples, just like a human remembering past problems to guide their thinking. 
  7: 
  8: By recalling similar problems first, the model creates context, activates the right concepts, and then solves the actual problem more accurately. 
  9: 
 10: ![Analogical prompting](images/4-analogical-prompt.jpg)
 11: 
 12: Figure from [Analogical prompting ](https://arxiv.org/abs/2310.01714) paper. 
 13: 
 14: ## **Prompt Template**
 15: Here is the prompt template for analogical prompting.
 16: 
 17: ```
 18: Your task is to tackle mathematical problems. When presented with a math problem, recall relevant problems as examples. Afterward, proceed to solve the initial problem.
 19: 
 20: # Problem:
 21: {question}
 22: 
 23: # Instructions:
 24: ## Relevant Problems:
 25: Recall three examples of math problems that are relevant to the initial problem. Your problems should be distinct from each other and from the initial problem (e.g., involving different numbers and names). For each problem:
 26: - After "Q: ", describe the problem
 27: - After "A: ", explain the solution and enclose the ultimate answer in \\boxed{{}}.
 28: 
 29: ## Solve the Initial Problem:
 30: Q: Copy and paste the initial problem here.
 31: A: Explain the solution step by step and enclose the final answer in \\boxed{{}}.
 32: ```
 33: 
 34: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 35: 
 36: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 37: - ‚ú® Weekly GenAI updates
 38: - üìÑ Weekly LLM, Agents and RAG research paper updates
 39: - üìù 1 fresh blog post on an interesting topic every week
 40: 
 41: ## **Implementation**
 42: 
 43: Now let's see the implementation of analogical promtping technique using LangChain v1.0
 44: 
 45: ```python
 46: # !pip install langchain langchain-google-genai pydantic
 47: 
 48: import os
 49: from google.colab import userdata
 50: from langchain.chat_models import init_chat_model
 51: from langchain_core.prompts import ChatPromptTemplate
 52: from langchain_core.output_parsers import PydanticOutputParser
 53: from pydantic import BaseModel, Field
 54: 
 55: # 1. Set your API key
 56: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 57: 
 58: # 2. Define the structured output schema
 59: class AnalogicalResponse(BaseModel):
 60:     relevant_problems: str = Field(..., description="Self-generated relevant example problems with solutions")
 61:     reasoning_chain: str = Field(..., description="Step-by-step reasoning for the original problem")
 62:     answer: str = Field(..., description="Final numeric answer only")
 63: 
 64: # 3. Create the parser
 65: parser = PydanticOutputParser(pydantic_object=AnalogicalResponse)
 66: 
 67: # 4. Initialize the chat model (Gemini 2.5 Flash)
 68: model = init_chat_model(
 69:     "gemini-2.5-flash",
 70:     model_provider="google_genai",
 71:     temperature=0
 72: )
 73: 
 74: # 5. Analogical prompting template (matches the structure in the image)
 75: prompt_template = ChatPromptTemplate.from_template(
 76:     """
 77: Your task is to tackle mathematical problems. When presented with a math problem, recall relevant problems as examples. Afterward, proceed to solve the initial problem.
 78: 
 79: # Problem:
 80: {question}
 81: 
 82: # Instructions:
 83: ## Relevant Problems:
 84: Recall three examples of math problems that are relevant to the initial problem. Your problems should be distinct from each other and from the initial problem (e.g., involving different numbers and names). For each problem:
 85: - After "Q: ", describe the problem
 86: - After "A: ", explain the solution and enclose the ultimate answer in \\boxed{{}}.
 87: 
 88: ## Solve the Initial Problem:
 89: Q: Copy and paste the initial problem here.
 90: A: Explain the solution step by step and enclose the final answer in \\boxed{{}}.
 91: 
 92: Provide the final output in the following JSON format:
 93: {format_instructions}
 94: """
 95: )
 96: 
 97: # 6. Inject format instructions
 98: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
 99: 
100: # 7. Build LCEL chain (prompt ‚Üí model ‚Üí parser)
101: chain = prompt | model | parser
102: 
103: # 8. Example problem (your chosen analogical example)
104: question = "What is the area of the rectangle with the four vertices at (1, 3), (7, 3), (7, 8), and (1, 8)?"
105: 
106: # 9. Invoke the chain
107: result = chain.invoke({"question": question})
108: 
109: # 10. Display results
110: print("\n--- Relevant Problems (Self-Generated) ---\n", result.relevant_problems)
111: print("\n--- Reasoning Chain ---\n", result.reasoning_chain)
112: print("\n--- Final Answer ---\n", result.answer)
113: ```
114: Here the output is
115: 
116: ```
117: --- Relevant Problems (Self-Generated) ---
118:  Q: A rectangular garden is 10 meters long and 7 meters wide. What is its area?
119: A: The area of a rectangle is calculated by multiplying its length by its width.
120: Area = Length √ó Width
121: Area = 10 meters √ó 7 meters
122: Area = 70 square meters.
123: The area of the garden is \boxed{70} square meters.
124: 
125: Q: What is the area of a rectangle with vertices at (2, 1), (8, 1), (8, 5), and (2, 5)?
126: A: To find the area, we need the length and width of the rectangle.
127: Let's take two adjacent vertices, for example, (2, 1) and (8, 1). The distance between these points gives one side length. Since the y-coordinates are the same, this is a horizontal side.
128: Length = |8 - 2| = 6 units.
129: Now, take another adjacent vertex, (8, 5), which shares an x-coordinate with (8, 1). The distance between (8, 1) and (8, 5) gives the other side length (width). Since the x-coordinates are the same, this is a vertical side.
130: Width = |5 - 1| = 4 units.
131: Area = Length √ó Width = 6 √ó 4 = 24 square units.
132: The area of the rectangle is \boxed{24} square units.
133: 
134: Q: A square has vertices at (-2, -1), (3, -1), (3, 4), and (-2, 4). What is its area?
135: A: To find the area of a square, we need the length of one of its sides.
136: Let's find the distance between two adjacent vertices, for example, (-2, -1) and (3, -1).
137: Side length = |3 - (-2)| = |3 + 2| = 5 units.
138: Since it's a square, all sides are equal.
139: Area = Side √ó Side = 5 √ó 5 = 25 square units.
140: The area of the square is \boxed{25} square units.
141: 
142: --- Reasoning Chain ---
143:  Q: What is the area of the rectangle with the four vertices at (1, 3), (7, 3), (7, 8), and (1, 8)?
144: A:
145: 1.  **Identify the coordinates:** The given vertices are A=(1, 3), B=(7, 3), C=(7, 8), and D=(1, 8).
146: 2.  **Determine the length of the sides:**
147:     *   We can find the length of the horizontal sides by looking at the change in x-coordinates when the y-coordinate is constant. For example, consider the side connecting (1, 3) and (7, 3). The length is the absolute difference of the x-coordinates: |7 - 1| = 6 units.
148:     *   We can find the length of the vertical sides by looking at the change in y-coordinates when the x-coordinate is constant. For example, consider the side connecting (7, 3) and (7, 8). The length is the absolute difference of the y-coordinates: |8 - 3| = 5 units.
149: 3.  **Identify length and width:** From the calculations, one side of the rectangle has a length of 6 units, and the adjacent side has a length of 5 units. These represent the length and width of the rectangle.
150: 4.  **Calculate the area:** The area of a rectangle is given by the formula Area = Length √ó Width.
151:     Area = 6 √ó 5 = 30 square units.
152: The area of the rectangle is \boxed{30}.
153: 
154: --- Final Answer ---
155:  30
156: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Chain_of_Draft_Prompting.md">
  1: # **Chain of Draft Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Chain-of-Draft (CoD) prompting is a reasoning technique where you tell the LLM to think step-by-step but in a very short, compact form. In CoD prompting, the model still performs multi-step reasoning, but each step is extremely concise (often 3‚Äì5 words), focusing only on the essential information needed to progress. This is similar to how humans solve problems by scribbling tiny notes or equations instead of writing long explanations.
  8: 
  9: Chain of draft prompting reduces response length, token cost and response time while still keeping reasoning accuracy high. 
 10: 
 11: ![chain of draft (CoD) prompting](images/2-cod-prompt.jpg)
 12: 
 13: Figure from [Chain of Draft prompting ](https://arxiv.org/abs/2502.18600) paper. 
 14: 
 15: ## **Prompt Template**
 16: 
 17: Here is the prompt template for chain of draft (CoD) prompting.
 18: 
 19: ```
 20: You are a step-by-step reasoning assistant.
 21: 
 22: Question: {question}
 23: 
 24: Answer: Let's think step by step, but only keep a minimum draft for
 25: each thinking step, with 5 words at most.
 26: ```
 27: 
 28: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 29: 
 30: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 31: - ‚ú® Weekly GenAI updates
 32: - üìÑ Weekly LLM, Agents and RAG research paper updates
 33: - üìù 1 fresh blog post on an interesting topic every week
 34: 
 35: ## **Implementation**
 36: 
 37: Now let's see the implementation of chain of draft promtping technique using LangChain v1.0
 38: 
 39: ```python
 40: !pip install langchain langchain-google-genai pydantic
 41: 
 42: import os
 43: from google.colab import userdata
 44: from langchain.chat_models import init_chat_model
 45: from langchain_core.prompts import ChatPromptTemplate
 46: from langchain_core.output_parsers import PydanticOutputParser
 47: from pydantic import BaseModel, Field
 48: 
 49: # 1. Set your API key
 50: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
 51: 
 52: # 2. Define the Pydantic schema for structured output
 53: class CoTResponse(BaseModel):
 54:     reasoning_chain: str = Field(..., description="Step-by-step reasoning")
 55:     answer: str = Field(..., description="Final numeric answer only")
 56: 
 57: # 3. Create the parser from the Pydantic model
 58: parser = PydanticOutputParser(pydantic_object=CoTResponse)
 59: 
 60: # 4. Initialize the chat model (gpt-4o-mini)
 61: model = init_chat_model(
 62:     "gemini-2.5-flash",
 63:     model_provider = "google_genai",
 64:     temperature=0
 65: )
 66: 
 67: # 5. Prompt template with explicit zero-shot CoT cue ("Let's think step by step.")
 68: prompt_template = ChatPromptTemplate.from_template(
 69:     """
 70: You are a step-by-step reasoning assistant.
 71: 
 72: Question: {question}
 73: 
 74: Answer: Let's think step by step, but only keep a minimum draft for
 75: each thinking step, with 5 words at most.
 76: 
 77: Provide your solution in the following JSON format:
 78: {format_instructions}
 79: 
 80: """
 81: )
 82: 
 83: # 6. Inject the parser's format instructions into the template
 84: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
 85: 
 86: # 7. Build the LCEL chain (prompt ‚Üí model ‚Üí parser)
 87: chain = prompt | model | parser
 88: 
 89: # 8. Example question and invocation
 90: question = "A baker made 24 cookies. Half are chocolate chip. Half of those have sprinkles. How many chocolate-chip cookies with sprinkles?"
 91: 
 92: result = chain.invoke({"question": question})
 93: 
 94: # 9. Display the result
 95: print("\n--- Reasoning Chain ---\n", result.reasoning_chain)
 96: print("\n--- Final Answer ---\n", result.answer)
 97: ```
 98: 
 99: Here the output is
100: ```
101: --- Reasoning Chain ---
102:  Total cookies: 24. Half are chocolate chip. 24 / 2 = 12. Half of those have sprinkles. 12 / 2 = 6. Final answer is 6.
103: 
104: --- Final Answer ---
105:  6
106: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Chain_of_Symbol_Prompting.md">
  1: # **Chain of Symbol Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Chain-of-Symbol (CoS) Prompting is a reasoning technique in which the model is shown example solutions, but instead of using natural-language chain-of-thought, the examples use symbolic reasoning steps. In this each example contains:
  8: 
  9: - A question (natural language description of a spatial/stacking scenario)
 10: - A symbolic representation of the intermediate steps (e.g., `U/T/R/S/V/W`)
 11: - The final symbolic answer sequence
 12: 
 13: This technique is especially powerful in tasks involving, spatial relationships, Stack-order reasoning, object manipulation etc. 
 14: 
 15: ![Chain of Symbol prompting](images/4-chain-symbol-prompt.jpg)
 16: 
 17: Figure from [Chain of Symbol prompting](https://arxiv.org/abs/2305.10276) paper. 
 18: 
 19: ## **Prompt Template**
 20: 
 21: Here is the prompt template for chain of symbol prompting.
 22: 
 23: ```
 24: You are a symbolic spatial-reasoning assistant.
 25: 
 26: Here is an example problem solved using chain-of-symbol reasoning:
 27: {few_shot_example}
 28: 
 29: Now solve the following question using a similar chain-of-symbol approach:
 30: 
 31: Question: {question}
 32: ```
 33: 
 34: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 35: 
 36: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 37: - ‚ú® Weekly GenAI updates
 38: - üìÑ Weekly LLM, Agents and RAG research paper updates
 39: - üìù 1 fresh blog post on an interesting topic every week
 40: 
 41: ## **Implementation**
 42: 
 43: Now let's see the implementation of chain of symbol promtping technique using LangChain v1.0
 44: 
 45: ```python
 46: # !pip install langchain langchain-google-genai pydantic
 47: 
 48: import os
 49: from google.colab import userdata
 50: from langchain.chat_models import init_chat_model
 51: from langchain_core.prompts import ChatPromptTemplate
 52: from langchain_core.output_parsers import PydanticOutputParser
 53: from pydantic import BaseModel, Field
 54: 
 55: # 1. Set your API key
 56: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 57: 
 58: # 2. Define Pydantic schema
 59: class CoSResponse(BaseModel):
 60:     symbol_chain: str = Field(..., description="Symbolic step-by-step chain")
 61:     answer: str = Field(..., description="Final sequence in symbolic format")
 62: 
 63: # 3. Create parser
 64: parser = PydanticOutputParser(pydantic_object=CoSResponse)
 65: 
 66: # 4. Initialize Gemini model
 67: model = init_chat_model(
 68:     "gemini-2.5-flash",
 69:     model_provider="google_genai",
 70:     temperature=0
 71: )
 72: 
 73: # 5. Few-shot CoS example (1-shot)
 74: few_shot_example = """
 75: Q: There are a set of colored blocks. 
 76: The blue block R is on top of the green block S. 
 77: The red block T is on top of R. 
 78: The yellow block U is on top of T. 
 79: The green block S is on top of the orange block V. 
 80: The orange block V is on top of the purple block W. 
 81: We need to get block S. 
 82: To grab a lower block, all blocks above it must be removed first. 
 83: How to get block S?
 84: 
 85: A:
 86: U/T/R/S/V/W
 87: T/R/S/V/W
 88: R/S/V/W
 89: S/V/W
 90: S
 91: """
 92: 
 93: # 6. Few-shot CoS prompt template
 94: prompt_template = ChatPromptTemplate.from_template(
 95:     """
 96: You are a symbolic spatial-reasoning assistant.
 97: 
 98: Here is an example problem solved using chain-of-symbol reasoning:
 99: {few_shot_example}
100: 
101: Now solve the following question using a similar chain-of-symbol approach:
102: 
103: Question: {question}
104: 
105: Provide your solution in the following JSON format:
106: {format_instructions}
107: """
108: )
109: 
110: # 7. Inject example + parser format instructions
111: prompt = prompt_template.partial(
112:     few_shot_example=few_shot_example,
113:     format_instructions=parser.get_format_instructions()
114: )
115: 
116: # 8. Build the LCEL chain
117: chain = prompt | model | parser
118: 
119: # 9. Target Question
120: question = (
121:     "There is a stack of four books. The Science book (S) is on the bottom. "
122:     "The History book (H) is on top of the Science book. "
123:     "The Math book (M) is on top of the History book. "
124:     "The Art book (A) is on the very top. "
125:     "We need to grab the Science book (S). To grab any book, all books above it must be removed first. "
126:     "What is the sequence of books to remove to get the Science book?"
127: )
128: 
129: # 10. Run the chain
130: result = chain.invoke({"question": question})
131: 
132: # 11. Display result
133: print("\n--- Symbol Chain ---\n", result.symbol_chain)
134: print("\n--- Final Answer ---\n", result.answer)
135: ```
136: Here the output is
137: 
138: ```
139: --- Symbol Chain ---
140:  A/M/H/S
141: M/H/S
142: H/S
143: S
144: 
145: --- Final Answer ---
146:  A, M, H, S
147: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Chain_of_Translation_Prompting.md">
  1: # **Chain of Translation Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Chain of Translation Prompting is a prompting technique where a non-English input sentence is first translated into English, and only then the actual task‚Äîsuch as sentiment analysis, emotion detection, toxicity classification, or hate-speech detection is performed.
  8: 
  9: This technique improves reliability because large language models are typically more accurate when processing English text. By inserting a translation step, the model gains access to richer semantic cues, clearer syntactic structure, and a more familiar linguistic environment. The result is more stable, consistent, and interpretable predictions compared to directly classifying the original sentence in a low-resource language.
 10: 
 11: ![Chain of Translation prompting](images/1-chain-translation-prompt.jpg)
 12: 
 13: Figure from [Chain of Translation prompting](https://arxiv.org/abs/2409.04512) paper.
 14: 
 15: ## **Prompt Template**
 16: 
 17: Here is the prompt template for chain of translation prompting.
 18: 
 19: ```
 20: Consider yourself to be a human annotator who is well versed in English and Telugu language.
 21: Given a Telugu sentence as input, perform the following tasks on the sentence:
 22: 
 23: 1. Translate the given Telugu sentence into English.
 24: 
 25: 2. Identify the sentiment depicted by the sentence.
 26:    If the sentence expresses a positive emotion or opinion, label it as Positive.
 27:    If the sentence expresses a negative emotion or complaint, label it as Negative.
 28:    If the sentence expresses neither positive nor negative sentiment, label it as Neutral.
 29: 
 30: 3. Give the output as:
 31:    - the original Telugu sentence,
 32:    - its English translation,
 33:    - and the sentiment label.
 34: 
 35: Sentence is as follows:
 36: {sentence}
 37: ```
 38: 
 39: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 40: 
 41: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 42: - ‚ú® Weekly GenAI updates
 43: - üìÑ Weekly LLM, Agents and RAG research paper updates
 44: - üìù 1 fresh blog post on an interesting topic every week
 45: 
 46: ## **Implementation**
 47: 
 48: Now let's see the implementation of chain of translation promtping technique using LangChain v1.0
 49: 
 50: ```python
 51: # pip install langchain langchain-google-genai pydantic
 52: 
 53: import os
 54: from google.colab import userdata
 55: from langchain.chat_models import init_chat_model
 56: from langchain_core.prompts import ChatPromptTemplate
 57: from langchain_core.output_parsers import PydanticOutputParser
 58: from pydantic import BaseModel, Field
 59: 
 60: 
 61: # ----------------------------------------------------------
 62: # 1. Set your Gemini API key
 63: # ----------------------------------------------------------
 64: 
 65: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 66: 
 67: 
 68: # ----------------------------------------------------------
 69: # 2. Define Final Structured Output Model
 70: # ----------------------------------------------------------
 71: 
 72: class TranslationSentiment(BaseModel):
 73:     telugu_sentence: str = Field(..., description="Original Telugu input")
 74:     english_translation: str = Field(..., description="English translation of the Telugu text")
 75:     sentiment_label: str = Field(..., description="Sentiment: Positive, Negative, or Neutral")
 76: 
 77: 
 78: final_parser = PydanticOutputParser(pydantic_object=TranslationSentiment)
 79: 
 80: 
 81: # ----------------------------------------------------------
 82: # 3. Initialize Gemini model (single call)
 83: # ----------------------------------------------------------
 84: 
 85: model = init_chat_model(
 86:     "gemini-2.5-flash",
 87:     model_provider="google_genai",
 88:     temperature=0
 89: )
 90: 
 91: 
 92: # ----------------------------------------------------------
 93: # 4. Single Prompt Template (Translation + Classification)
 94: # ----------------------------------------------------------
 95: 
 96: prompt_template = ChatPromptTemplate.from_template(
 97:     """
 98: Consider yourself to be a human annotator who is well versed in English and Telugu language.
 99: Given a Telugu sentence as input, perform the following tasks on the sentence:
100: 
101: 1. Translate the given Telugu sentence into English.
102: 
103: 2. Identify the sentiment depicted by the sentence.
104:    If the sentence expresses a positive emotion or opinion, label it as Positive.
105:    If the sentence expresses a negative emotion or complaint, label it as Negative.
106:    If the sentence expresses neither positive nor negative sentiment, label it as Neutral.
107: 
108: 3. Give the output as:
109:    - the original Telugu sentence,
110:    - its English translation,
111:    - and the sentiment label.
112: 
113: Sentence is as follows:
114: {sentence}
115: 
116: Provide the final output using this JSON structure:
117: {format_instructions}
118: """
119: )
120: 
121: single_prompt = prompt_template.partial(
122:     format_instructions=final_parser.get_format_instructions()
123: )
124: 
125: 
126: # ----------------------------------------------------------
127: # 5. Build LCEL Chain (single LLM call)
128: # ----------------------------------------------------------
129: 
130: chain = single_prompt | model | final_parser
131: 
132: 
133: # ----------------------------------------------------------
134: # 6. Run Chain of Translation Prompting on the Example
135: # ----------------------------------------------------------
136: 
137: telugu_sentence = "‡∞∏‡∞ø‡∞®‡∞ø‡∞Æ‡∞æ ‡∞Ö‡∞¶‡±ç‡∞≠‡±Å‡∞§‡∞Ç‡∞ó‡∞æ ‡∞â‡∞Ç‡∞¶‡∞ø! ‡∞°‡±à‡∞∞‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞™‡∞®‡∞ø‡∞§‡±Ä‡∞∞‡±Å ‡∞∏‡±Ç‡∞™‡∞∞‡±ç. ‡∞Æ‡∞≥‡±ç‡∞≥‡±Ä ‡∞ö‡±Ç‡∞°‡∞æ‡∞≤‡∞®‡∞ø ‡∞â‡∞Ç‡∞¶‡∞ø."
138: 
139: result = chain.invoke({"sentence": telugu_sentence})
140: 
141: print("\n--- FINAL OUTPUT ---\n")
142: print("Telugu Sentence       :", result.telugu_sentence)
143: print("English Translation   :", result.english_translation)
144: print("Sentiment Label       :", result.sentiment_label)
145: ```
146: 
147: Here the output is
148: ```
149: --- FINAL OUTPUT ---
150: 
151: Telugu Sentence       : ‡∞∏‡∞ø‡∞®‡∞ø‡∞Æ‡∞æ ‡∞Ö‡∞¶‡±ç‡∞≠‡±Å‡∞§‡∞Ç‡∞ó‡∞æ ‡∞â‡∞Ç‡∞¶‡∞ø! ‡∞°‡±à‡∞∞‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±ç ‡∞™‡∞®‡∞ø‡∞§‡±Ä‡∞∞‡±Å ‡∞∏‡±Ç‡∞™‡∞∞‡±ç. ‡∞Æ‡∞≥‡±ç‡∞≥‡±Ä ‡∞ö‡±Ç‡∞°‡∞æ‡∞≤‡∞®‡∞ø ‡∞â‡∞Ç‡∞¶‡∞ø.
152: English Translation   : The movie is wonderful! The director's work is super. I want to watch it again.
153: Sentiment Label       : Positive
154: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Chain_of_Verification_Prompting.md">
  1: # **Chain of Verification Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Chain-of-Verification (CoVe) Prompting is a structured reasoning technique designed to reduce factual errors (hallucinations) by forcing a model to *verify its own answer* before finalizing it.
  8: 
  9: Unlike standard Chain-of-Thought‚Äîwhere the model directly reasons and produces a , CoVe breaks the process into four deliberate stages:
 10: 
 11: 1. Draft an initial answer
 12: 2. Generate verification questions that check the facts in the draft
 13: 3. Independently answer each verification question without seeing the draft
 14: 4. Produce a corrected, final answer based on the verified facts
 15: 
 16: This separation into *‚Äúdraft ‚Üí verify ‚Üí correct‚Äù* helps prevent the model from repeating its own mistakes.
 17: 
 18: By treating verification as an independent fact-checking phase, CoVe significantly reduces hallucinations, especially in factual or knowledge-based tasks.
 19: 
 20: 
 21: ![Chain of Verification prompting](images/2-cove-prompt.jpg)
 22: 
 23: Figure from [Chain of Verification prompting](https://arxiv.org/abs/2309.11495) paper.
 24: 
 25: ## **Prompt Template**
 26: 
 27: Here is the  draft prompt template for chain of verification prompting.
 28: 
 29: ```
 30: You are a factual answering assistant.
 31: 
 32: Step 1 of Chain-of-Verification:
 33: Generate a baseline draft answer for the question. Do NOT verify anything yet.
 34: 
 35: Question:
 36: {question}
 37: ```
 38: 
 39: Here is the  verification questions generation prompt template for chain of verification prompting.
 40: ```
 41: You are now performing Step 2 of Chain-of-Verification.
 42: 
 43: Given the baseline draft answer below, generate verification questions to check EACH factual claim.
 44: 
 45: Draft Answer:
 46: {draft}
 47: 
 48: Your job:
 49: - Break the draft into factual claims.
 50: - Create one verification question for each claim.
 51: - Each question MUST be independently fact-checkable.
 52: ```
 53: 
 54: Here is the  verification answers generation prompt template for chain of verification prompting.
 55: ```
 56: Step 3 of Chain-of-Verification.
 57: 
 58: Answer the following verification questions INDEPENDENTLY.
 59: Do NOT refer to the draft answer. Use only factual knowledge.
 60: 
 61: Questions:
 62: {questions}
 63: ```
 64: 
 65: Here is the  final response generation prompt template for chain of verification prompting.
 66: ```
 67: Step 4 of Chain-of-Verification.
 68: 
 69: You are given:
 70: 1. The baseline draft answer
 71: 2. The list of verification questions
 72: 3. The factual answers to those questions
 73: 
 74: Your task:
 75: - Identify incorrect statements in the draft
 76: - Keep only the claims supported by verification answers
 77: - Remove or correct unsupported claims
 78: - Produce the final VERIFIED answer
 79: 
 80: Draft:
 81: {draft}
 82: 
 83: Verification Questions:
 84: {questions}
 85: 
 86: Verification Answers:
 87: {answers}
 88: ```
 89: 
 90: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 91: 
 92: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 93: - ‚ú® Weekly GenAI updates
 94: - üìÑ Weekly LLM, Agents and RAG research paper updates
 95: - üìù 1 fresh blog post on an interesting topic every week
 96: 
 97: ## **Implementation**
 98: 
 99: Now let's see the implementation of chain of verification prompting technique using LangChain v1.0
100: 
101: ```python
102: # pip install langchain langchain-google-genai pydantic
103: 
104: import os
105: import time 
106: from google.colab import userdata
107: from langchain.chat_models import init_chat_model
108: from langchain_core.prompts import ChatPromptTemplate
109: from langchain_core.output_parsers import PydanticOutputParser
110: from pydantic import BaseModel, Field
111: from typing import List
112: 
113: # 1. Set your Gemini API key
114: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
115: 
116: # ---------------------------------------------------------
117: # 2. Define structured outputs for all 4 CoVe stages
118: # ---------------------------------------------------------
119: 
120: class BaselineResponse(BaseModel):
121:     draft_answer: str = Field(..., description="Initial unverified answer")
122: 
123: class VerificationPlan(BaseModel):
124:     questions: List[str] = Field(..., description="List of verification questions generated from the draft")
125: 
126: class VerificationAnswers(BaseModel):
127:     answers: List[str] = Field(..., description="Answers to the verification questions in the same order")
128: 
129: class FinalVerifiedResponse(BaseModel):
130:     verified_answer: str = Field(..., description="Final corrected answer using only verified facts")
131: 
132: # ---------------------------------------------------------
133: # 3. Initialize the Gemini model (gemini-2.5-flash)
134: # ---------------------------------------------------------
135: model = init_chat_model(
136:     "gemini-2.5-flash",
137:     model_provider="google_genai",
138:     temperature=0
139: )
140: 
141: # ---------------------------------------------------------
142: # 4. PROMPTS
143: # ---------------------------------------------------------
144: 
145: # 4.1 Baseline Draft Prompt
146: baseline_prompt_tmpl = ChatPromptTemplate.from_template(
147:     """
148: You are a factual answering assistant.
149: 
150: Step 1 of Chain-of-Verification:
151: Generate a baseline draft answer for the question. Do NOT verify anything yet.
152: 
153: Question:
154: {question}
155: 
156: Return your response in JSON:
157: {format_instructions}
158: """
159: )
160: 
161: baseline_parser = PydanticOutputParser(pydantic_object=BaselineResponse)
162: baseline_prompt = baseline_prompt_tmpl.partial(format_instructions=baseline_parser.get_format_instructions())
163: 
164: 
165: # 4.2 Plan Verification Questions Prompt
166: verify_plan_tmpl = ChatPromptTemplate.from_template(
167:     """
168: You are now performing Step 2 of Chain-of-Verification.
169: 
170: Given the baseline draft answer below, generate verification questions to check EACH factual claim.
171: 
172: Draft Answer:
173: {draft}
174: 
175: Your job:
176: - Break the draft into factual claims.
177: - Create one verification question for each claim.
178: - Each question MUST be independently fact-checkable.
179: 
180: Return JSON:
181: {format_instructions}
182: """
183: )
184: 
185: verify_plan_parser = PydanticOutputParser(pydantic_object=VerificationPlan)
186: verify_plan_prompt = verify_plan_tmpl.partial(format_instructions=verify_plan_parser.get_format_instructions())
187: 
188: 
189: # 4.3 Verification Answering Prompt
190: verify_answer_tmpl = ChatPromptTemplate.from_template(
191:     """
192: Step 3 of Chain-of-Verification.
193: 
194: Answer the following verification questions INDEPENDENTLY.
195: Do NOT refer to the draft answer. Use only factual knowledge.
196: 
197: Questions:
198: {questions}
199: 
200: Return JSON:
201: {format_instructions}
202: """
203: )
204: 
205: verify_answer_parser = PydanticOutputParser(pydantic_object=VerificationAnswers)
206: verify_answer_prompt = verify_answer_tmpl.partial(format_instructions=verify_answer_parser.get_format_instructions())
207: 
208: 
209: # 4.4 Final Verified Response Prompt
210: final_answer_tmpl = ChatPromptTemplate.from_template(
211:     """
212: Step 4 of Chain-of-Verification.
213: 
214: You are given:
215: 1. The baseline draft answer
216: 2. The list of verification questions
217: 3. The factual answers to those questions
218: 
219: Your task:
220: - Identify incorrect statements in the draft
221: - Keep only the claims supported by verification answers
222: - Remove or correct unsupported claims
223: - Produce the final VERIFIED answer
224: 
225: Draft:
226: {draft}
227: 
228: Verification Questions:
229: {questions}
230: 
231: Verification Answers:
232: {answers}
233: 
234: Return JSON:
235: {format_instructions}
236: """
237: )
238: 
239: final_answer_parser = PydanticOutputParser(pydantic_object=FinalVerifiedResponse)
240: final_answer_prompt = final_answer_tmpl.partial(format_instructions=final_answer_parser.get_format_instructions())
241: 
242: # ---------------------------------------------------------
243: # 5. Build the LCEL chains
244: # ---------------------------------------------------------
245: 
246: baseline_chain = baseline_prompt | model | baseline_parser
247: time.sleep(1)
248: plan_chain = verify_plan_prompt | model | verify_plan_parser
249: time.sleep(1)
250: verify_chain = verify_answer_prompt | model | verify_answer_parser
251: time.sleep(1)
252: final_chain = final_answer_prompt | model | final_answer_parser
253: 
254: # ---------------------------------------------------------
255: # 6. Run CoVe on your example
256: # ---------------------------------------------------------
257: 
258: question = "Which US Presidents were born in the state of Texas?"
259: 
260: # Step 1: Baseline Draft
261: baseline = baseline_chain.invoke({"question": question})
262: 
263: # Step 2: Plan Verifications
264: plan = plan_chain.invoke({"draft": baseline.draft_answer})
265: 
266: # Step 3: Execute Verifications
267: verification = verify_chain.invoke({"questions": plan.questions})
268: 
269: # Step 4: Final Verified Answer
270: final = final_chain.invoke({
271:     "draft": baseline.draft_answer,
272:     "questions": plan.questions,
273:     "answers": verification.answers
274: })
275: 
276: # ---------------------------------------------------------
277: # 7. Print all outputs
278: # ---------------------------------------------------------
279: 
280: print("\n--- Baseline Draft ---\n", baseline.draft_answer)
281: print("\n--- Verification Questions ---\n", plan.questions)
282: print("\n--- Verification Answers ---\n", verification.answers)
283: print("\n--- Final Verified Answer ---\n", final.verified_answer)
284: ```
285: 
286: Here the output is
287: ```
288: --- Baseline Draft ---
289:  The US Presidents born in the state of Texas are Lyndon B. Johnson and Dwight D. Eisenhower.
290: 
291: --- Verification Questions ---
292:  ['Was Lyndon B. Johnson a US President?', 'Was Lyndon B. Johnson born in the state of Texas?', 'Was Dwight D. Eisenhower a US President?', 'Was Dwight D. Eisenhower born in the state of Texas?', 'Are there any other US Presidents, besides Lyndon B. Johnson and Dwight D. Eisenhower, who were born in the state of Texas?']
293: 
294: --- Verification Answers ---
295:  ['Yes, Lyndon B. Johnson was the 36th President of the United States.', 'Yes, Lyndon B. Johnson was born near Stonewall, Texas.', 'Yes, Dwight D. Eisenhower was the 34th President of the United States.', 'Yes, Dwight D. Eisenhower was born in Denison, Texas.', 'No, there are no other US Presidents, besides Lyndon B. Johnson and Dwight D. Eisenhower, who were born in the state of Texas.']
296: 
297: --- Final Verified Answer ---
298:  The US Presidents born in the state of Texas are Lyndon B. Johnson and Dwight D. Eisenhower.
299:  ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Contrastive_CoT_Prompting.md">
  1: # **Contrastive Chain of Thought Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Contrastive Chain-of-Thought (Contrastive CoT) Prompting is an enhanced reasoning technique in which the model is given both a correct chain-of-thought and an incorrect chain-of-thought for a single example. 
  8: 
  9: This contrast teaches the model two things:
 10: 
 11: 1. What good reasoning looks like (positive demonstration)
 12: 2. What kind of mistakes to avoid (negative demonstration)
 13: 
 14: This dual learning significantly improves reasoning performance, especially in tasks involving logic, arithmetic, dates, and multi-step reasoning. This is unlike regular few-shot CoT prompting, which gives only correct reasoning.
 15: 
 16: ![Contrastive CoT prompting](images/3-contrastive-cot-prompt.jpg)
 17: 
 18: Figure from [Contrastive CoT prompting](https://arxiv.org/abs/2311.09277) paper. 
 19: 
 20: 
 21: ## **Prompt Template**
 22: 
 23: Here is the prompt template for few-shot chain of thoughts prompting.
 24: 
 25: ```
 26: You are a reasoning assistant that learns from contrastive examples.
 27: 
 28: Below is a contrastive demonstration containing:
 29: - A correct chain-of-thought
 30: - An incorrect chain-of-thought
 31: 
 32: Use the correct reasoning patterns and avoid mistakes shown in the wrong explanation.
 33: 
 34: Contrastive Demonstration:
 35: {contrastive_example}
 36: 
 37: Now solve the following question using improved chain-of-thought reasoning:
 38: 
 39: Question: {question}
 40: Answer: 
 41: ```
 42: 
 43: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 44: 
 45: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 46: - ‚ú® Weekly GenAI updates
 47: - üìÑ Weekly LLM, Agents and RAG research paper updates
 48: - üìù 1 fresh blog post on an interesting topic every week
 49: 
 50: ## **Implementation**
 51: 
 52: Now let's see the implementation of contrastive CoT promtping technique using LangChain v1.0
 53: 
 54: ```python
 55: # !pip install langchain langchain-google-genai pydantic
 56: 
 57: import os
 58: from google.colab import userdata
 59: from langchain.chat_models import init_chat_model
 60: from langchain_core.prompts import ChatPromptTemplate
 61: from langchain_core.output_parsers import PydanticOutputParser
 62: from pydantic import BaseModel, Field
 63: 
 64: # 1. Set your API key
 65: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 66: 
 67: # 2. Define Pydantic schema
 68: class ContrastiveCoTResponse(BaseModel):
 69:     reasoning_chain: str = Field(..., description="Step-by-step reasoning derived from correct signals")
 70:     answer: str = Field(..., description="Final numeric/date answer only")
 71: 
 72: # 3. Create parser
 73: parser = PydanticOutputParser(pydantic_object=ContrastiveCoTResponse)
 74: 
 75: # 4. Initialize Gemini model
 76: model = init_chat_model(
 77:     "gemini-2.5-flash",
 78:     model_provider="google_genai",
 79:     temperature=0
 80: )
 81: 
 82: # 5. Contrastive CoT example (1-shot)
 83: contrastive_example = """
 84: Q: The historical event was originally planned for 11/05/1852, 
 85: but due to unexpected weather, it was moved forward by two days to today. 
 86: What is the date 8 days from today in MM/DD/YYYY?
 87: 
 88: Correct Explanation:
 89: Moving an event forward by two days from 11/05/1852 means today's date is 11/03/1852 (11/05/1852 - 2 days).
 90: 8 days from today (11/03/1852) is 11/11/1852.
 91: So the answer is 11/11/1852.
 92: 
 93: Wrong Explanation:
 94: Moving an event forward by two days from 11/05/1852 means today's date is 11/07/1852 (11/05/1852 + 2 days).
 95: 8 days from this incorrect 'today' (11/07/1852) would be 11/15/1852.
 96: This is incorrect because "moved forward" means earlier in time, not later.
 97: """
 98: 
 99: # 6. Contrastive CoT prompt template
100: prompt_template = ChatPromptTemplate.from_template(
101:     """
102: You are a reasoning assistant that learns from contrastive examples.
103: 
104: Below is a contrastive demonstration containing:
105: - A correct chain-of-thought
106: - An incorrect chain-of-thought
107: 
108: Use the correct reasoning patterns and avoid mistakes shown in the wrong explanation.
109: 
110: Contrastive Demonstration: 
111: {contrastive_example}
112: 
113: Now solve the following question using improved chain-of-thought reasoning:
114: 
115: Question: {question}
116: Answer: 
117: 
118: Provide your solution in the following JSON format:
119: {format_instructions}
120: """
121: )
122: 
123: # 7. Inject example + parser instructions
124: prompt = prompt_template.partial(
125:     contrastive_example=contrastive_example,
126:     format_instructions=parser.get_format_instructions()
127: )
128: 
129: # 8. Build the LCEL chain
130: chain = prompt | model | parser
131: 
132: # 9. Target Question
133: question = (
134:     "The concert was scheduled to be on 06/01/1943, but was delayed by one day to today." 
135:     "What is the date 10 days ago in MM/DD/YYYY?"
136: )
137: 
138: # 10. Run the chain
139: result = chain.invoke({"question": question})
140: 
141: # 11. Display result
142: print("\n--- Reasoning Chain ---\n", result.reasoning_chain)
143: print("\n--- Final Answer ---\n", result.answer)
144: ```
145: Here the output is
146: ```
147: --- Reasoning Chain ---
148:  The concert was scheduled for 06/01/1943, but was delayed by one day to today. "Delayed by one day" means today's date is one day after the scheduled date. So, today's date is 06/01/1943 + 1 day = 06/02/1943.
149: We need to find the date 10 days ago from today (06/02/1943).
150: 06/02/1943 - 10 days:
151: Subtracting 2 days from 06/02/1943 brings us to 05/31/1943 (since June 1st and 2nd are gone, and May has 31 days).
152: Remaining days to subtract: 10 - 2 = 8 days.
153: Subtracting 8 more days from 05/31/1943 gives us 05/23/1943.
154: 
155: --- Final Answer ---
156:  05/23/1943
157: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Cross_Lingual_Prompting.md">
  1: # **Cross Lingual Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Cross-Lingual Prompting (CLP) is a strategy designed to enhance the reasoning capabilities of Large Language Models (LLMs) when processing tasks in low-resource or non-dominant languages. While many LLMs possess vast knowledge bases, their ability to perform complex reasoning (such as arithmetic or logic) is often significantly stronger in high-resource languages like English compared to others.
  8: 
  9: CLP bridges this performance gap by separating the linguistic understanding from the logical reasoning. Instead of forcing the model to solve a complex problem directly in the source language where it may fail to reason correctly, the process is split into two distinct stages: Alignment (Translation) and Solving (Reasoning).
 10: 
 11: 1. Cross-Lingual Alignment
 12: 
 13: In this step, the model is instructed to act as an expert in multilingual understanding. It receives the task in the source language (Ex: Hindi) and is asked to understand the given task step-by-step in the target language (English).
 14: 
 15: 2. Task-Specific Solver
 16: 
 17: After clarifying the task in the target language, the model is instructed to switch roles and act as an expert in the target task (e.g., arithmetic reasoning, sentiment analysis, classification). It now solves the already-aligned problem entirely in the target language.
 18: 
 19: 
 20: ![Cross Lingual prompting](images/2-cross-lingual-prompt.jpg)
 21: 
 22: Figure from [Cross Lingual prompting](https://arxiv.org/abs/2310.14799) paper.
 23: 
 24: ## **Prompt Template**
 25: 
 26: Here is the cross-lingual alignment prompt template for cross lingual prompting.
 27: 
 28: ```
 29: You are an expert in multi-lingual understanding in Hindi language.
 30: 
 31: Request: {task}
 32: 
 33: Let's understand the task in English step by step.
 34: 
 35: ```
 36: 
 37: Here is the task-specific solver prompt template for cross lingual prompting.
 38: 
 39: ```
 40: You are an expert in arithmetic reasoning in English language.
 41: 
 42: Using the cross-lingual understanding provided below,
 43: solve the task step by step in English.
 44: 
 45: Understanding:
 46: {understanding}
 47: 
 48: Provide:
 49: 1. Step-by-step reasoning (in English)
 50: 2. The final numeric answer only
 51: ```
 52: 
 53: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 54: 
 55: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 56: - ‚ú® Weekly GenAI updates
 57: - üìÑ Weekly LLM, Agents and RAG research paper updates
 58: - üìù 1 fresh blog post on an interesting topic every week
 59: 
 60: ## **Implementation**
 61: 
 62: Now let's see the implementation of cross lingual promtping technique using LangChain v1.0
 63: 
 64: ```python
 65: # pip install langchain langchain-google-genai pydantic
 66: 
 67: import os
 68: from google.colab import userdata
 69: from langchain.chat_models import init_chat_model
 70: from langchain_core.prompts import ChatPromptTemplate
 71: from langchain_core.output_parsers import PydanticOutputParser
 72: from pydantic import BaseModel, Field
 73: 
 74: 
 75: # ----------------------------------------------------------
 76: # 1. Set Gemini API Key
 77: # ----------------------------------------------------------
 78: 
 79: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 80: 
 81: 
 82: # ----------------------------------------------------------
 83: # 2. Structured Output Models
 84: # ----------------------------------------------------------
 85: 
 86: class AlignedUnderstanding(BaseModel):
 87:     understanding: str = Field(..., description="Step-by-step meaning of the Hindi task explained in English")
 88: 
 89: 
 90: class TaskSolution(BaseModel):
 91:     reasoning: str = Field(..., description="Step-by-step reasoning in English")
 92:     final_answer: str = Field(..., description="Only the numeric final answer")
 93: 
 94: 
 95: alignment_parser = PydanticOutputParser(pydantic_object=AlignedUnderstanding)
 96: solution_parser = PydanticOutputParser(pydantic_object=TaskSolution)
 97: 
 98: 
 99: # ----------------------------------------------------------
100: # 3. Initialize Gemini (gemini-2.5-flash)
101: # ----------------------------------------------------------
102: 
103: model = init_chat_model(
104:     "gemini-2.5-flash",
105:     model_provider="google_genai",
106:     temperature=0
107: )
108: 
109: 
110: # ----------------------------------------------------------
111: # 4. Prompt Templates
112: # ----------------------------------------------------------
113: 
114: # ------------------------------
115: # 4.1 Cross-Lingual Alignment Prompt
116: # ------------------------------
117: alignment_prompt_template = ChatPromptTemplate.from_template(
118:     """
119: You are an expert in multi-lingual understanding in Hindi language.
120: 
121: Request: {task}
122: 
123: Let's understand the task in English step by step.
124: 
125: Provide the output in this JSON format:
126: {format_instructions}
127: """
128: )
129: 
130: alignment_prompt = alignment_prompt_template.partial(
131:     format_instructions=alignment_parser.get_format_instructions()
132: )
133: 
134: 
135: # ------------------------------
136: # 4.2 Task-Specific Solver Prompt
137: # ------------------------------
138: solver_prompt_template = ChatPromptTemplate.from_template(
139:     """
140: You are an expert in arithmetic reasoning in English language.
141: 
142: Using the cross-lingual understanding provided below,
143: solve the task step by step in English.
144: 
145: Understanding:
146: {understanding}
147: 
148: Provide:
149: 1. Step-by-step reasoning (in English)
150: 2. The final numeric answer only
151: 
152: Return your output in this JSON format:
153: {format_instructions}
154: """
155: )
156: 
157: solver_prompt = solver_prompt_template.partial(
158:     format_instructions=solution_parser.get_format_instructions()
159: )
160: 
161: 
162: # ----------------------------------------------------------
163: # 5. Build LCEL Chains
164: # ----------------------------------------------------------
165: 
166: alignment_chain = alignment_prompt | model | alignment_parser
167: solver_chain = solver_prompt | model | solution_parser
168: 
169: 
170: # ----------------------------------------------------------
171: # 6. Run Cross-Lingual Prompting on Hindi Example
172: # ----------------------------------------------------------
173: 
174: task = "‡§∞‡§æ‡§Æ ‡§∂‡•ç‡§Ø‡§æ‡§Æ ‡§∏‡•á ‡§§‡•Ä‡§® ‡§∏‡§æ‡§≤ ‡§¨‡§°‡§º‡§æ ‡§π‡•à‡•§ ‡§Ö‡§ó‡§∞ ‡§∂‡•ç‡§Ø‡§æ‡§Æ 10 ‡§∏‡§æ‡§≤ ‡§ï‡§æ ‡§π‡•à, ‡§§‡•ã ‡§∞‡§æ‡§Æ ‡§ï‡•Ä ‡§â‡§Æ‡•ç‡§∞ ‡§ï‡§ø‡§§‡§®‡•Ä ‡§π‡•à?"
175: 
176: # Step 1 ‚Äî Cross-Lingual Alignment
177: alignment_result = alignment_chain.invoke({"task": task})
178: print("\n--- CROSS-LINGUAL UNDERSTANDING ---\n")
179: print(alignment_result.understanding)
180: 
181: # Step 2 ‚Äî Task-Specific Solver
182: solution_result = solver_chain.invoke({
183:     "understanding": alignment_result.understanding
184: })
185: print("\n--- TASK REASONING ---\n")
186: print(solution_result.reasoning)
187: 
188: print("\n--- FINAL ANSWER ---\n")
189: print(solution_result.final_answer)
190: ```
191: Here the output is
192: ```
193: --- CROSS-LINGUAL UNDERSTANDING ---
194: 
195: The Hindi task can be broken down as follows:
196: 1.  "‡§∞‡§æ‡§Æ ‡§∂‡•ç‡§Ø‡§æ‡§Æ ‡§∏‡•á ‡§§‡•Ä‡§® ‡§∏‡§æ‡§≤ ‡§¨‡§°‡§º‡§æ ‡§π‡•à‡•§" translates to "Ram is three years older than Shyam."
197: 2.  "‡§Ö‡§ó‡§∞ ‡§∂‡•ç‡§Ø‡§æ‡§Æ 10 ‡§∏‡§æ‡§≤ ‡§ï‡§æ ‡§π‡•à," translates to "If Shyam is 10 years old,"
198: 3.  "‡§§‡•ã ‡§∞‡§æ‡§Æ ‡§ï‡•Ä ‡§â‡§Æ‡•ç‡§∞ ‡§ï‡§ø‡§§‡§®‡•Ä ‡§π‡•à?" translates to "then what is Ram's age?"
199: 
200: In essence, the task asks to calculate Ram's age given that he is three years older than Shyam, and Shyam's current age is 10 years.
201: 
202: --- TASK REASONING ---
203: 
204: 1. The problem states that Ram is three years older than Shyam. 2. It also provides Shyam's current age as 10 years. 3. To find Ram's age, we need to add the age difference (3 years) to Shyam's age (10 years). 4. Therefore, Ram's age = 10 + 3 = 13 years.
205: 
206: --- FINAL ANSWER ---
207: 
208: 13
209: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Faithful_Chain_of_Thought_Prompting.md">
  1: # **Faithful Chain of Thought Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates. 
  4: 
  5: ## **Overview**
  6: Faithful Chain-of-Thought (CoT) prompting is a technique for Large Language Models (LLMs) that ensures the final answer is directly and logically derived from the step-by-step reasoning the model provides. It works in two key stages:
  7: 1. Translation 
  8: - The LLM takes the original question (in natural language, like English) and translates it into a precise, step-by-step plan called a reasoning chain.
  9: - This reasoning chain is a mix of natural language comments (for human understanding) and symbolic language (like computer code, e.g., Python, or a formal planning language).
 10: 
 11: 2. Problem Solving
 12: - The reasoning chain, which now includes executable code, is then handed off to a deterministic solver (like a Python interpreter or calculator program).
 13: - The solver executes the steps in the chain to mathematically or logically calculate the final answer.
 14: 
 15: By embedding both *planning* (natural language decomposition) and *computation* (symbolic code), Faithful CoT produces explanations that are both interpretable and reliable.
 16: 
 17: ![Faithful CoT prompting](images/4-faithful-cot-prompt.jpg)
 18: 
 19: Figure from [Faithful CoT prompting](https://arxiv.org/abs/2301.13379) paper.
 20: 
 21: ## **Prompt Template**
 22: 
 23: Here is the prompt template for program of thoughts 
 24: 
 25: ```
 26: You are an expert reasoning assistant.
 27: 
 28: Use Faithful Chain-of-Thought (Faithful CoT) prompting.
 29: 
 30: You must output a single Python program that:
 31: 
 32: - Includes brief natural-language substeps as comments.
 33: - Uses Python variable assignments for each step.
 34: - Computes the final answer deterministically.
 35: - MUST end with: ans = <final value>
 36: - MUST be executable as-is in a Python interpreter.
 37: 
 38: Problem:
 39: {question}
 40: ```
 41: 
 42: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 43: 
 44: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 45: - ‚ú® Weekly GenAI updates
 46: - üìÑ Weekly LLM, Agents and RAG research paper updates
 47: - üìù 1 fresh blog post on an interesting topic every week
 48: 
 49: ## **Zero-Shot Implementation**
 50: 
 51: Now let's see the implementation of zero-shot faithful CoT promtping technique using LangChain v1.0
 52: 
 53: ```python
 54: # !pip install langchain langchain-google-genai pydantic
 55: 
 56: import os
 57: from google.colab import userdata
 58: from langchain.chat_models import init_chat_model
 59: from langchain_core.prompts import ChatPromptTemplate
 60: from langchain_core.output_parsers import PydanticOutputParser
 61: from pydantic import BaseModel, Field
 62: from langchain_experimental.utilities import PythonREPL
 63: 
 64: # 1. Set Google Gemini API Key
 65: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 66: 
 67: # 2. Faithful CoT Structured Output Format
 68: class FaithfulCoTResponse(BaseModel):
 69:     program: str = Field(
 70:         ..., 
 71:         description="Faithful chain-of-thought reasoning in Python. May include comments. Must end with ans = <final value>."
 72:     )
 73: 
 74: # 3. Parser
 75: parser = PydanticOutputParser(pydantic_object=FaithfulCoTResponse)
 76: 
 77: # 4. Initialize Gemini model
 78: model = init_chat_model(
 79:     "gemini-2.5-flash",
 80:     model_provider="google_genai",
 81:     temperature=0
 82: )
 83: 
 84: # 5. Python REPL
 85: python_repl = PythonREPL()
 86: 
 87: # 6. Zero-Shot Faithful CoT Prompt Template
 88: prompt_template = ChatPromptTemplate.from_template(
 89:     """
 90: You are an expert reasoning assistant.
 91: 
 92: Use **Faithful Chain-of-Thought (Faithful CoT)** prompting.
 93: 
 94: You must output a single Python program that:
 95: 
 96: - Includes brief natural-language substeps as comments.
 97: - Uses Python variable assignments for each step.
 98: - Computes the final answer deterministically.
 99: - MUST end with: ans = <final value>
100: - MUST be executable as-is in a Python interpreter.
101: 
102: The output MUST be a JSON object containing only one field: "program".
103: 
104: Problem:
105: {question}
106: 
107: Provide the solution in this JSON format:
108: {format_instructions}
109: """
110: )
111: 
112: # 7. Insert parser instructions
113: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
114: 
115: # 8. Build chain
116: chain = prompt | model | parser
117: 
118: # 9. Use your earlier example
119: question = """
120: Daniel has 17 apples. Rosy gives Daniel 5 oranges, and in return,
121: Daniel gives her 3 apples. How many apples does Daniel have now?
122: """
123: 
124: # 10. Invoke LLM
125: result = chain.invoke({"question": question})
126: 
127: print("\n--- Faithful Chain-of-Thought Program ---\n")
128: print(result.program)
129: 
130: # 11. Execute program
131: execution_output = python_repl.run(result.program)
132: 
133: # 12. Extract final answer
134: final_answer = python_repl.locals.get("ans", None)
135: 
136: print("\n--- Final Answer (from Python interpreter) ---\n")
137: print(final_answer)
138: ```
139: 
140: Here the output is
141: ```
142: # Daniel's initial number of apples.
143: initial_apples = 17
144: 
145: # Rosy gives Daniel oranges, which does not change the number of apples Daniel has.
146: # oranges_given_to_daniel = 5
147: 
148: # Daniel gives Rosy some apples.
149: apples_given_to_rosy = 3
150: 
151: # Calculate the number of apples Daniel has after giving some to Rosy.
152: apples_after_giving_to_rosy = initial_apples - apples_given_to_rosy
153: 
154: # The final number of apples Daniel has.
155: ans = apples_after_giving_to_rosy
156: 
157: --- Final Answer (from Python interpreter) ---
158: 
159: 14
160: ```
161: 
162: 
163: ## **Few-Shot Implementation**
164: 
165: Now let's see the implementation of few-shot faithful CoT promtping technique using LangChain v1.0
166: 
167: ```python
168: 
169: # pip install langchain langchain-google-genai pydantic langchain-experimental
170: 
171: import os
172: from google.colab import userdata
173: from langchain.chat_models import init_chat_model
174: from langchain_core.prompts import ChatPromptTemplate
175: from langchain_core.output_parsers import PydanticOutputParser
176: from pydantic import BaseModel, Field
177: from langchain_experimental.utilities import PythonREPL
178: 
179: # 1. Set API key
180: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
181: 
182: # 2. Structured Faithful CoT Response
183: class FaithfulCoTResponse(BaseModel):
184:     program: str = Field(
185:         ...,
186:         description="Executable Python program containing faithful chain-of-thought with comments. Must assign final value to 'ans'."
187:     )
188: 
189: # 3. Parser
190: parser = PydanticOutputParser(pydantic_object=FaithfulCoTResponse)
191: 
192: # 4. Initialize Gemini model
193: model = init_chat_model(
194:     "gemini-2.5-flash",
195:     model_provider="google_genai",
196:     temperature=0
197: )
198: 
199: # 5. Python Interpreter (can run code + comments)
200: python_repl = PythonREPL()
201: 
202: # 6. FEW-SHOT EXAMPLE (Your previous Faithful CoT example)
203: few_shot_example = """
204: Question: Daniel has 17 apples. Rosy gives Daniel 5 oranges, and in return Daniel gives her 3 apples. How many apples does Daniel have now?
205: 
206: # 1. How many apples does Daniel begin with?
207: n_apples_begin = 17
208: 
209: # 2. How many apples does Daniel give away?
210: n_apples_given = 3
211: 
212: # 3. Final apples Daniel has now
213: n_apples_final = n_apples_begin - n_apples_given
214: 
215: ans = n_apples_final
216: """
217: 
218: # 7. Few-Shot Faithful CoT Prompt Template
219: prompt_template = ChatPromptTemplate.from_template(
220:     """
221: You are an expert reasoning assistant.
222: 
223: Below is an example demonstrating **Faithful Chain-of-Thought (Faithful CoT) prompting**.
224: The solution is expressed as Python code with natural language reasoning embedded as comments.
225: 
226: The final answer MUST be assigned to `ans`.
227: 
228: {few_shot_example}
229: 
230: Now solve the following problem using the SAME Faithful CoT format:
231: 
232: Problem:
233: {question}
234: 
235: Output Instructions:
236: - Output ONLY a single Python program.
237: - Comments ARE allowed.
238: - Code must be fully executable.
239: - Must end with: ans = <final value>
240: 
241: Return the output in this JSON format:
242: {format_instructions}
243: """
244: )
245: 
246: # 8. Insert few-shot example + parser instructions
247: prompt = prompt_template.partial(
248:     few_shot_example=few_shot_example,
249:     format_instructions=parser.get_format_instructions()
250: )
251: 
252: # 9. Build chain
253: chain = prompt | model | parser
254: 
255: # 10. Current Problem
256: question = """
257: A bakery starts the day with 40 croissants. They sell 25 croissants in the morning and 12 in the afternoon.
258: If they want to have at least 5 croissants left for the evening staff, do they meet this target?
259: """
260: 
261: # 11. Invoke LLM ‚Üí generate Faithful CoT program
262: result = chain.invoke({"question": question})
263: 
264: print("\n--- Faithful CoT Program Generated by LLM ---\n")
265: print(result.program)
266: 
267: # 12. Execute program using the Python REPL
268: execution_output = python_repl.run(result.program)
269: 
270: # 13. Retrieve final answer
271: final_answer = python_repl.locals.get("ans", None)
272: 
273: print("\n--- Final Answer (from Python interpreter) ---\n")
274: print(final_answer)
275: ```
276: 
277: Here the output is
278: ```
279: --- Faithful CoT Program Generated by LLM ---
280: 
281: # 1. How many croissants did the bakery start with?
282: croissants_start = 40
283: 
284: # 2. How many croissants were sold in the morning?
285: croissants_sold_morning = 25
286: 
287: # 3. How many croissants were sold in the afternoon?
288: croissants_sold_afternoon = 12
289: 
290: # 4. Calculate the total number of croissants sold.
291: total_croissants_sold = croissants_sold_morning + croissants_sold_afternoon
292: 
293: # 5. Calculate the number of croissants remaining after sales.
294: croissants_remaining = croissants_start - total_croissants_sold
295: 
296: # 6. Determine the target number of croissants for the evening staff.
297: target_croissants_evening = 5
298: 
299: # 7. Check if the bakery meets the target (at least 5 croissants left).
300: ans = croissants_remaining >= target_croissants_evening
301: 
302: --- Final Answer (from Python interpreter) ---
303: 
304: False
305: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Few_Shot_Chain-of_Thought_Prompting.md">
  1: # **Few-Shot Chain of Thought Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Few-shot Chain-of-Thought Prompting is a technique in which you provide the model with one or more solved examples, where each example contains:
  8: 
  9: - A question
 10: - A step-by-step chain of thought
 11: - The final answer
 12: 
 13: By showing the model *how* to think step by step through an example, you teach it the format and reasoning style that it should follow when answering a new but related question.
 14: 
 15: Unlike zero-shot CoT, where the model is only instructed to *‚Äúthink step by step,‚Äù* few-shot CoT gives the model an actual demonstration of how to reason. When the model sees a worked-out example, it generalizes the reasoning pattern and applies it to new, unseen problems.
 16: 
 17: ![Few-Shot Chain of Thought prompting](images/1-few-cot-prompt.jpg)
 18: 
 19: Figure from [Few-Shot Chain of Thought prompting](https://arxiv.org/abs/2201.11903) paper. 
 20: 
 21: ## **Prompt Template**
 22: 
 23: Here is the prompt template for few-shot chain of thoughts prompting.
 24: 
 25: ```
 26: You are a step-by-step reasoning assistant.
 27: 
 28: Here is an example problem solved using chain-of-thought:
 29: {few_shot_example}
 30: 
 31: Now solve the following question using a similar chain-of-thought approach:
 32: 
 33: Question: {question}
 34: 
 35: Answer:  
 36: ```
 37: 
 38: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 39: 
 40: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 41: - ‚ú® Weekly GenAI updates
 42: - üìÑ Weekly LLM, Agents and RAG research paper updates
 43: - üìù 1 fresh blog post on an interesting topic every week
 44: 
 45: ## **Implementation**
 46: 
 47: Now let's see the implementation of few-shot chain thoughts promtping technique using LangChain v1.0
 48: 
 49: ```python
 50: 
 51: #!pip install langchain langchain-google-genai pydantic
 52: 
 53: import os
 54: from google.colab import userdata
 55: from langchain.chat_models import init_chat_model
 56: from langchain_core.prompts import ChatPromptTemplate
 57: from langchain_core.output_parsers import PydanticOutputParser
 58: from pydantic import BaseModel, Field
 59: 
 60: # 1. Set your API key
 61: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 62: 
 63: # 2. Define Pydantic schema
 64: class CoTResponse(BaseModel):
 65:     reasoning_chain: str = Field(..., description="Step-by-step reasoning")
 66:     answer: str = Field(..., description="Final numeric/date answer only")
 67: 
 68: # 3. Create parser
 69: parser = PydanticOutputParser(pydantic_object=CoTResponse)
 70: 
 71: # 4. Initialize Gemini model
 72: model = init_chat_model(
 73:     "gemini-2.5-flash",
 74:     model_provider="google_genai",
 75:     temperature=0
 76: )
 77: 
 78: # 5. Few-shot CoT example (1-shot)
 79: few_shot_example = """
 80: Question: The historical event was originally planned for 11/05/1852, 
 81: but due to unexpected weather, it was moved forward by two days to today. 
 82: What is the date 8 days from today in MM/DD/YYYY?
 83: 
 84: Answer: Moving an event forward by two days from 11/05/1852 means today's date is 11/03/1852 (11/05/1852 - 2).
 85: 8 days from today (11/03/1852) is 11/11/1852.
 86: So the answer is 11/11/1852.
 87: """
 88: 
 89: # 6. Few-shot prompt template
 90: prompt_template = ChatPromptTemplate.from_template(
 91:     """
 92: You are a step-by-step reasoning assistant.
 93: 
 94: Here is an example problem solved using chain-of-thought:
 95: {few_shot_example}
 96: 
 97: Now solve the following question using a similar chain-of-thought approach:
 98: 
 99: Question: {question}
100: 
101: Answer:  
102: 
103: Provide your solution in the following JSON format:
104: {format_instructions}
105: """
106: )
107: 
108: # 7. Inject reference example + parser formatting
109: prompt = prompt_template.partial(
110:     few_shot_example=few_shot_example,
111:     format_instructions=parser.get_format_instructions()
112: )
113: 
114: # 8. Build the LCEL chain
115: chain = prompt | model | parser
116: 
117: # 9. Target Question
118: question = (
119:     "A construction project started on 09/15/2024. The first phase took 12 days. "
120:     "The second phase was originally scheduled for 20 days, but was shortened by 3 days. "
121:     "What is the completion date of the second phase in MM/DD/YYYY?"
122: )
123: 
124: # 10. Run the chain
125: result = chain.invoke({"question": question})
126: 
127: # 11. Display result
128: print("\n--- Reasoning Chain ---\n", result.reasoning_chain)
129: print("\n--- Final Answer ---\n", result.answer)
130: 
131: ```
132: Here the output is
133: 
134: ```
135: --- Reasoning Chain ---
136:  The construction project started on 09/15/2024. The first phase took 12 days, so it ended on 09/15/2024 + 12 days = 09/27/2024. The second phase was originally scheduled for 20 days but was shortened by 3 days, meaning its actual duration was 20 - 3 = 17 days. To find the completion date of the second phase, we add 17 days to the end date of the first phase (09/27/2024). Adding 3 days to 09/27/2024 brings us to 09/30/2024 (since September has 30 days). We still need to add 17 - 3 = 14 more days. These 14 days will be in October. Therefore, the completion date of the second phase is 10/14/2024.
137: 
138: --- Final Answer ---
139:  10/14/2024
140: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/images/readme.md">
1: 
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Least_to_Most_Prompting.md">
  1: # **Least to Most Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Least-to-Most Prompting (LtM) is a prompting technique in which a complex question is solved by:
  8: 
  9: 1. Decomposing it into simpler, smaller sub-problems (the ‚Äúleast‚Äù stage)
 10: 2. Solving each sub-problem sequentially and using each solution to build toward the final answer (the ‚Äúmost‚Äù stage)
 11: 
 12: Instead of tackling the entire problem at once, the model breaks it down into manageable steps, solves each step in order, and gradually works toward the final solution. This approach mirrors how humans solve complex tasks: first understand the parts, then combine the answers.
 13: 
 14: Unlike Chain-of-Thought (CoT), which focuses on generating one long reasoning path, Least-to-Most prompting explicitly separates decomposition and solution. It forces the model to *plan first* and *compute later*, enabling stronger reasoning, especially for multi-stage problems.
 15: 
 16: ![Least to Most prompting](images/1-least-prompt.jpg)
 17: 
 18: Figure from [Least to Most prompting](https://arxiv.org/abs/2205.10625) paper. 
 19: 
 20: 
 21: ## **Prompt Template**
 22: 
 23: Here is the prompt template for least to most prompting.
 24: ```
 25: You are an expert reasoning assistant.
 26: 
 27: You must solve the problem using **Least-to-Most Prompting**, which has TWO required stages:
 28: 
 29: 1. **Decomposition (Least):**
 30:    - Break the main problem into a sequential list of simpler sub-problems.
 31: 
 32: 2. **Sequential Solving (Most):**
 33:    - Solve each sub-problem step-by-step.
 34:    - Use outputs of earlier sub-problems to solve later ones.
 35:    - Continue until the final answer is reached.
 36: 
 37: Question:
 38: {question}
 39: 
 40: Important:
 41: - decomposition must contain numbered sub-problems.
 42: - sequential_solution must show calculations for each sub-problem.
 43: - final_answer must contain ONLY the final numeric answer.
 44: ```
 45: 
 46: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 47: 
 48: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 49: - ‚ú® Weekly GenAI updates
 50: - üìÑ Weekly LLM, Agents and RAG research paper updates
 51: - üìù 1 fresh blog post on an interesting topic every week
 52: 
 53: ## **Zero-Shot Implementation**
 54: 
 55: Now let's see the implementation of zero-shot least to most promtping technique using LangChain v1.0
 56: 
 57: ```python
 58: # !pip install langchain langchain-google-genai pydantic
 59: 
 60: import os
 61: from langchain.chat_models import init_chat_model
 62: from langchain_core.prompts import ChatPromptTemplate
 63: from langchain_core.output_parsers import PydanticOutputParser
 64: from pydantic import BaseModel, Field
 65: 
 66: # 1. Set your Gemini API key
 67: os.environ["GOOGLE_API_KEY"] = "YOUR_GEMINI_API_KEY"
 68: 
 69: # 2. Define structured output for LtM
 70: class LtMResponse(BaseModel):
 71:     decomposition: str = Field(..., description="List of sub-problems in order")
 72:     sequential_solution: str = Field(..., description="Step-by-step solutions for each sub-problem")
 73:     final_answer: str = Field(..., description="Final numeric answer only")
 74: 
 75: # 3. Create parser
 76: parser = PydanticOutputParser(pydantic_object=LtMResponse)
 77: 
 78: # 4. Initialize Gemini model (gemini-2.5-flash)
 79: model = init_chat_model(
 80:     "gemini-2.5-flash",
 81:     model_provider="google",
 82:     temperature=0
 83: )
 84: 
 85: # 5. Zero-Shot Least-to-Most Prompt Template
 86: prompt_template = ChatPromptTemplate.from_template(
 87:     """
 88: You are an expert reasoning assistant.
 89: 
 90: You must solve the problem using **Least-to-Most Prompting**, which has TWO required stages:
 91: 
 92: 1. **Decomposition (Least):**
 93:    - Break the main problem into a sequential list of simpler sub-problems.
 94: 
 95: 2. **Sequential Solving (Most):**
 96:    - Solve each sub-problem step-by-step.
 97:    - Use outputs of earlier sub-problems to solve later ones.
 98:    - Continue until the final answer is reached.
 99: 
100: Question:
101: {question}
102: 
103: Provide your solution in the following JSON format:
104: {format_instructions}
105: 
106: Important:
107: - decomposition must contain numbered sub-problems.
108: - sequential_solution must show calculations for each sub-problem.
109: - final_answer must contain ONLY the final numeric answer.
110: """
111: )
112: 
113: # 6. Insert parser instructions
114: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
115: 
116: # 7. Build LCEL chain
117: chain = prompt | model | parser
118: 
119: # 8. Invoke the chain using your marathon LtM problem
120: question = """
121: A runner is preparing for a marathon. She runs 10 miles every day.
122: Last week, she ran 7 days.
123: This week, she took a 2-day rest and ran 8 miles on the remaining days.
124: If she wants to run a total of 180 miles across both weeks,
125: how many more miles must she run in the next 3 days?
126: """
127: 
128: result = chain.invoke({"question": question})
129: 
130: # 9. Output
131: print(result)
132: print("\n--- Decomposition ---\n", result.decomposition)
133: print("\n--- Sequential Solution ---\n", result.sequential_solution)
134: print("\n--- Final Answer ---\n", result.final_answer)
135: 
136: ```
137: 
138: Here the output is
139: ```
140: --- Decomposition ---
141:  1. Calculate the total miles run last week.
142: 2. Calculate the number of days the runner ran this week.
143: 3. Calculate the total miles run this week.
144: 4. Calculate the total miles run in both weeks combined.
145: 5. Calculate the remaining miles needed to reach the target of 180 miles.
146: 
147: --- Sequential Solution ---
148:  1. **Total miles run last week:**
149:    She ran 10 miles/day for 7 days.
150:    Miles last week = 10 miles/day * 7 days = 70 miles.
151: 
152: 2. **Number of days the runner ran this week:**
153:    A week has 7 days. She took a 2-day rest.
154:    Days ran this week = 7 days - 2 days = 5 days.
155: 
156: 3. **Total miles run this week:**
157:    She ran 8 miles on the 5 days she ran this week.
158:    Miles this week = 8 miles/day * 5 days = 40 miles.
159: 
160: 4. **Total miles run in both weeks combined:**
161:    Total miles so far = Miles last week + Miles this week
162:    Total miles so far = 70 miles + 40 miles = 110 miles.
163: 
164: 5. **Remaining miles needed to reach the target of 180 miles:**
165:    Target total miles = 180 miles.
166:    Remaining miles = Target total miles - Total miles so far
167:    Remaining miles = 180 miles - 110 miles = 70 miles.
168: 
169:    Therefore, she must run 70 more miles in the next 3 days.
170: 
171: --- Final Answer ---
172:  70
173: ```
174: ## **Few-Shot Implementation**
175: 
176: Now let's see the implementation of few-shot least to most promtping technique using LangChain v1.0
177: 
178: ```python
179: 
180: # !pip install langchain langchain-google-genai pydantic
181: 
182: import os
183: from langchain.chat_models import init_chat_model
184: from langchain_core.prompts import ChatPromptTemplate
185: from langchain_core.output_parsers import PydanticOutputParser
186: from pydantic import BaseModel, Field
187: 
188: # 1. Set your API key
189: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
190: 
191: # 2. Define Pydantic schema for LtM output
192: class LtMResponse(BaseModel):
193:     decomposition: str = Field(..., description="Ordered list of sub-problems")
194:     sequential_solution: str = Field(..., description="Step-by-step reasoning for each sub-problem")
195:     final_answer: str = Field(..., description="Final numeric answer only")
196: 
197: # 3. Create parser
198: parser = PydanticOutputParser(pydantic_object=LtMResponse)
199: 
200: # 4. Initialize Gemini model
201: model = init_chat_model(
202:     "gemini-2.5-flash",
203:     model_provider="google_genai",
204:     temperature=0
205: )
206: 
207: # 5. Few-shot LtM example (1-shot)
208: few_shot_example = """
209: Goal: Solve the problem using Least-to-Most prompting.
210: 
211: Problem:
212: A runner is preparing for a marathon. She runs 10 miles every day.
213: Last week, she ran 7 days. This week, she took a 2-day rest and
214: ran 8 miles on the remaining days. If she wants to run a total of
215: 180 miles across both weeks, how many more miles must she run in the next 3 days?
216: 
217: 1. Decomposition (Least):
218: - Sub-problem 1: Calculate total miles run last week.
219: - Sub-problem 2: Calculate number of running days this week.
220: - Sub-problem 3: Calculate total miles run this week.
221: - Sub-problem 4: Calculate total miles run so far.
222: - Sub-problem 5: Calculate remaining miles needed.
223: 
224: 2. Sequential Solving (Most):
225: - Sub-problem 1: 10 miles/day √ó 7 days = 70 miles
226: - Sub-problem 2: 7 days ‚àí 2 rest days = 5 days
227: - Sub-problem 3: 8 miles/day √ó 5 days = 40 miles
228: - Sub-problem 4: 70 + 40 = 110 miles
229: - Sub-problem 5: 180 ‚àí 110 = 70 miles
230: 
231: Final Answer: 70
232: """
233: 
234: # 6. Few-shot LtM prompt template
235: prompt_template = ChatPromptTemplate.from_template(
236:     """
237: You are an expert reasoning assistant.
238: 
239: Below is an example problem solved using **Least-to-Most prompting**:
240: {few_shot_example}
241: 
242: Now apply the same Least-to-Most structure to solve the following problem:
243: 
244: Question: {question}
245: 
246: Provide the answer in the following JSON format:
247: {format_instructions}
248: """
249: )
250: 
251: # 7. Inject the few-shot example + parser instructions
252: prompt = prompt_template.partial(
253:     few_shot_example=few_shot_example,
254:     format_instructions=parser.get_format_instructions()
255: )
256: 
257: # 8. Build LCEL chain
258: chain = prompt | model | parser
259: 
260: # 9. Target problem (Chef question)
261: question = (
262:     "A chef needs to make 30 croissants. It takes him 5 minutes to prepare "
263:     "the dough for one croissant, and 15 minutes to bake it. He has already "
264:     "prepared and baked 5 croissants. He has 4 hours remaining. How many more "
265:     "minutes does he have left after preparing and baking the rest of the "
266:     "required croissants?"
267: )
268: 
269: # 10. Run the chain
270: result = chain.invoke({"question": question})
271: 
272: # 11. Display result
273: print("\n--- Decomposition ---\n", result.decomposition)
274: print("\n--- Sequential Solution ---\n", result.sequential_solution)
275: print("\n--- Final Answer ---\n", result.final_answer)
276: ```
277: 
278: Here the output is
279: ```
280: --- Decomposition ---
281:  - Sub-problem 1: Calculate the number of croissants remaining to be made.
282: - Sub-problem 2: Calculate the total time (preparation + baking) required for one croissant.
283: - Sub-problem 3: Calculate the total time needed to prepare and bake the remaining croissants.
284: - Sub-problem 4: Convert the chef's total remaining time (4 hours) into minutes.
285: - Sub-problem 5: Calculate the minutes the chef has left after completing the remaining croissants.
286: 
287: --- Sequential Solution ---
288:  - Sub-problem 1: 30 total croissants - 5 already made = 25 croissants remaining.
289: - Sub-problem 2: 5 minutes (prepare) + 15 minutes (bake) = 20 minutes per croissant.
290: - Sub-problem 3: 25 remaining croissants √ó 20 minutes/croissant = 500 minutes needed.
291: - Sub-problem 4: 4 hours √ó 60 minutes/hour = 240 minutes available.
292: - Sub-problem 5: 240 minutes (available) - 500 minutes (needed) = -260 minutes.
293: 
294: --- Final Answer ---
295:  -260
296: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Meta_Cognitive_Prompting.md">
  1: # **Meta Cognitive Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Meta-Cognitive Prompting (MP) is a prompting technique that guides a Large Language Model (LLM) through a structured *self-reflection process*, mirroring how humans think about their own thinking.
  8: 
  9: Just as humans evaluate their initial interpretations, question their assumptions, and refine their understanding, MP forces the model to:
 10: 
 11: 1. Understand the input
 12: 2. Make an initial judgment
 13: 3. Reflect on and critique this judgment
 14: 4. Form a final decision with justification
 15: 5. Assess its own confidence
 16: 
 17: ![Meta Cognitive prompting](images/5-meta-cognitive-prompt.jpg)
 18: 
 19: Figure from [Meta Cognitive prompting](https://arxiv.org/abs/2308.05342) paper. 
 20: 
 21: ## **Prompt Template**
 22: 
 23: Here is the prompt template for meta cognitive prompting.
 24: 
 25: ```
 26: For the question: "{question}" and statement: "{sentence}", determine if the statement provides the answer
 27: to the question. If the statement contains the answer to the question, the status is entailment.
 28: If it does not, the status is not_entailment. As you perform this task, follow these steps:
 29: 
 30: 1. Clarify your understanding of the question and the context sentence.
 31: 2. Make a preliminary identification of whether the context sentence contains the answer to the question.
 32: 3. Critically assess your preliminary analysis. If you feel unsure about your initialentailment classification, try to reassess it.
 33: 4. Confirm your final answer and explain the reasoning behind your choice.
 34: 5. Evaluate your confidence (0-100%) in your analysis and provide an explanation for this confidence level.
 35: 
 36: Provide the answer in your final response as "The status is (entailment / not_entailment)"
 37: 
 38: As you perform the above, produce the following structured output.
 39: ```
 40: 
 41: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 42: 
 43: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 44: - ‚ú® Weekly GenAI updates
 45: - üìÑ Weekly LLM, Agents and RAG research paper updates
 46: - üìù 1 fresh blog post on an interesting topic every week
 47: 
 48: ## **Zero-Shot Implementation**
 49: 
 50: Now let's see the implementation of few-shot meta cognitive promtping technique using LangChain v1.0
 51: 
 52: ```python
 53: # pip install langchain langchain-google-genai pydantic
 54: 
 55: import os
 56: from google.colab import userdata
 57: from langchain.chat_models import init_chat_model
 58: from langchain_core.prompts import ChatPromptTemplate
 59: from langchain_core.output_parsers import PydanticOutputParser
 60: from pydantic import BaseModel, Field
 61: 
 62: # 1. Set your Gemini API key
 63: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
 64: 
 65: # 2. Define structured output for Meta-Cognitive Prompting (added final_answer field)
 66: class MetaCognitiveResponse(BaseModel):
 67:     understanding: str = Field(..., description="Clarify understanding of the question and the context sentence")
 68:     preliminary_judgment: str = Field(..., description="Initial assessment of whether the statement contains the answer")
 69:     critical_evaluation: str = Field(..., description="Reflection and reassessment of the initial judgment")
 70:     final_answer: str = Field(..., description='Final response in the exact form: "The status is (entailment / not_entailment)"')
 71:     confidence: str = Field(..., description="Confidence score (0-100%) with explanation")
 72: 
 73: # 3. Create parser
 74: parser = PydanticOutputParser(pydantic_object=MetaCognitiveResponse)
 75: 
 76: # 4. Initialize Gemini model (gemini-2.5-flash)
 77: model = init_chat_model(
 78:     "gemini-2.5-flash",
 79:     model_provider="google_genai",
 80:     temperature=0
 81: )
 82: 
 83: # 5. Zero-Shot Meta-Cognitive Prompt Template (exact wording requested)
 84: prompt_template = ChatPromptTemplate.from_template(
 85:     """
 86: For the question: "{question}" and statement: "{sentence}", determine if the statement provides the answer
 87: to the question. If the statement contains the answer to the question, the status is entailment.
 88: If it does not, the status is not_entailment. As you perform this task, follow these steps:
 89: 1. Clarify your understanding of the question and the context sentence.
 90: 2. Make a preliminary identification of whether the context sentence contains the answer to the question.
 91: 3. Critically assess your preliminary analysis. If you feel unsure about your initialentailment classification, try to reassess it.
 92: 4. Confirm your final answer and explain the reasoning behind your choice.
 93: 5. Evaluate your confidence (0-100%) in your analysis and provide an explanation for this confidence level.
 94: Provide the answer in your final response as "The status is (entailment / not_entailment)"
 95: 
 96: As you perform the above, produce the following structured output.
 97: 
 98: Provide your response in JSON format exactly matching the fields:
 99: {format_instructions}
100: """
101: )
102: 
103: # 6. Insert parser instructions
104: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
105: 
106: # 7. Build LCEL chain
107: chain = prompt | model | parser
108: 
109: # 8. Invoke the chain with your example question + statement
110: question = "What is the largest planet in our solar system?"
111: statement = (
112:     "Jupiter, the fifth planet from the Sun, is so massive that it accounts for more "
113:     "than twice the mass of all the other planets combined."
114: )
115: 
116: result = chain.invoke({
117:     "question": question,
118:     "sentence": statement
119: })
120: 
121: # 9. Output (structured)
122: print("\n--- Understanding ---\n", result.understanding)
123: print("\n--- Preliminary Judgment ---\n", result.preliminary_judgment)
124: print("\n--- Critical Evaluation ---\n", result.critical_evaluation)
125: print("\n--- Final Answer ---\n", result.final_answer)
126: print("\n--- Confidence ---\n", result.confidence)
127: ```
128: 
129: Here the output is
130: ```
131: --- Understanding ---
132:  The question asks for the name of the planet that is the largest in our solar system. The term 'largest' can refer to size (diameter/volume) or mass. The statement provides information about Jupiter's mass relative to all other planets.
133: 
134: --- Preliminary Judgment ---
135:  The statement identifies Jupiter and describes it as 'so massive that it accounts for more than twice the mass of all the other planets combined.' This strongly implies that Jupiter is the largest planet, at least in terms of mass. Given that 'largest' often encompasses mass when referring to celestial bodies, the statement appears to provide the answer.
136: 
137: --- Critical Evaluation ---
138:  The question asks 'What is the largest planet?'. The statement names 'Jupiter' and describes its extreme 'massiveness' ('so massive that it accounts for more than twice the mass of all the other planets combined'). While 'largest' can strictly mean largest in diameter or volume, being 'so massive' to that extent makes Jupiter unequivocally the largest by mass. In common astronomical discourse, the most massive planet is also considered the 'largest' in a general sense. The statement directly provides the name of the planet and a superlative characteristic (its mass) that confirms its status as the largest. Therefore, the statement does provide the answer to the question.
139: 
140: --- Final Answer ---
141:  The status is entailment
142: 
143: --- Confidence ---
144:  100%. The statement explicitly names Jupiter and provides a superlative description of its mass, which directly answers the question of which planet is the 'largest' in our solar system, as mass is a primary measure of a planet's 'largeness'.
145: ```
146: 
147: 
148: ## **Few-Shot Implementation**
149: 
150: Now let's see the implementation of few-shot meta cognitive promtping technique using LangChain v1.0
151: 
152: ```python
153: # !pip install langchain langchain-google-genai pydantic
154: 
155: import os
156: from google.colab import userdata
157: from langchain.chat_models import init_chat_model
158: from langchain_core.prompts import ChatPromptTemplate
159: from langchain_core.output_parsers import PydanticOutputParser
160: from pydantic import BaseModel, Field
161: 
162: # 1. Set your API key
163: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
164: 
165: # 2. Define Pydantic schema for Meta-Cognitive output
166: class MetaCognitiveResponse(BaseModel):
167:     understanding: str = Field(..., description="Clarify understanding of the question and the context sentence")
168:     preliminary_judgment: str = Field(..., description="Initial assessment of whether the statement contains the answer")
169:     critical_evaluation: str = Field(..., description="Reflection and reassessment of the initial judgment")
170:     final_answer: str = Field(..., description='Final response in the form: "The status is (entailment / not_entailment)"')
171:     confidence: str = Field(..., description="Confidence score (0-100%) with explanation")
172: 
173: # 3. Create parser
174: parser = PydanticOutputParser(pydantic_object=MetaCognitiveResponse)
175: 
176: # 4. Initialize Gemini model (few-shot)
177: model = init_chat_model(
178:     "gemini-2.5-flash",
179:     model_provider="google_genai",
180:     temperature=0
181: )
182: 
183: # 5. Few-shot Meta-Cognitive example
184: few_shot_example = """
185: Goal: Solve the problem using **Meta-Cognitive Prompting**.
186: 
187: Problem:
188: Question (Q): What is the largest planet in our solar system?
189: Statement (S): Jupiter, the fifth planet from the Sun, is so massive that it accounts for more
190: than twice the mass of all the other planets combined.
191: 
192: --- Understanding ---
193: The question asks for the name of the planet that is the largest in our solar system.
194: The statement provides information about Jupiter and describes its extreme mass.
195: 
196: --- Preliminary Judgment ---
197: The statement identifies Jupiter and describes it as exceptionally massive, strongly
198: suggesting it is the largest planet. The statement appears to contain the answer.
199: 
200: --- Critical Evaluation ---
201: The question asks for the "largest planet." The statement names Jupiter and describes
202: its mass as exceeding that of all other planets combined. Although "largest" can refer
203: to diameter or volume, being overwhelmingly massive supports its classification as the
204: largest. The statement directly provides the planet's name and evidence for its status.
205: Thus, the statement does provide the answer.
206: 
207: --- Final Answer ---
208: The status is entailment
209: 
210: --- Confidence ---
211: 100%. The statement explicitly names Jupiter and provides a superlative description
212: of its mass, directly answering the question about the largest planet.
213: """
214: 
215: # 6. Few-shot Meta-Cognitive Prompt template
216: prompt_template = ChatPromptTemplate.from_template(
217:     """
218: You are an expert reasoning assistant.
219: 
220: Below is an example problem solved using **Meta-Cognitive Prompting**:
221: {few_shot_example}
222: 
223: Now apply the same Meta-Cognitive structure to solve the following problem:
224: 
225: Question (Q): {question}
226: Statement (S): {statement}
227: 
228: As you perform this task, follow these steps exactly:
229: 
230: 1. Clarify your understanding of the question and the context sentence.
231: 2. Make a preliminary identification of whether the context sentence contains the answer.
232: 3. Critically assess your preliminary analysis. If you feel unsure about your initial
233:    entailment classification, try to reassess it.
234: 4. Confirm your final answer and explain the reasoning behind your choice.
235: 5. Evaluate your confidence (0-100%) in your analysis and provide an explanation
236:    for this confidence level.
237: 
238: Provide the answer in your final response as:
239: "The status is (entailment / not_entailment)"
240: 
241: Finally, produce your complete response in the following JSON format:
242: {format_instructions}
243: """
244: )
245: 
246: # 7. Insert few-shot example and parser instructions
247: prompt = prompt_template.partial(
248:     few_shot_example=few_shot_example,
249:     format_instructions=parser.get_format_instructions()
250: )
251: 
252: # 8. Build LCEL chain
253: chain = prompt | model | parser
254: 
255: # 9. Target problem
256: question = "Which explorer was the first to circumnavigate the globe?"
257: statement = (
258:     "Ferdinand Magellan initiated the first sea voyage to sail all the way around the "
259:     "world, although he was killed in the Philippines before the journey was completed."
260: )
261: 
262: # 10. Run the chain
263: result = chain.invoke({
264:     "question": question,
265:     "statement": statement
266: })
267: 
268: # 11. Display structured result
269: print("\n--- Understanding ---\n", result.understanding)
270: print("\n--- Preliminary Judgment ---\n", result.preliminary_judgment)
271: print("\n--- Critical Evaluation ---\n", result.critical_evaluation)
272: print("\n--- Final Answer ---\n", result.final_answer)
273: print("\n--- Confidence ---\n", result.confidence)
274: ```
275: 
276: Here the output is
277: ```
278: --- Understanding ---
279:  The question asks for the name of the individual explorer who was the first to successfully complete a journey around the entire globe. The statement provides information about Ferdinand Magellan, stating that he initiated the first sea voyage that aimed to sail all the way around the world, but explicitly notes that he died before completing the journey himself.
280: 
281: --- Preliminary Judgment ---
282:  The statement names Ferdinand Magellan and describes his role in the first circumnavigation voyage. However, it also clearly states that he did not complete the journey. This suggests that while he was instrumental, he might not be the answer to 'who was the first to circumnavigate'. Therefore, the statement likely does not contain the answer to the question as phrased.
283: 
284: --- Critical Evaluation ---
285:  The question specifically asks 'Which explorer was the first to *circumnavigate* the globe?' To 'circumnavigate' means to travel all the way around. The statement explicitly says that Ferdinand Magellan 'was killed in the Philippines before the journey was completed.' This means Magellan himself did not complete the circumnavigation. While his expedition was the first to do so, he personally was not the first explorer to achieve it. Therefore, the statement does not provide the answer to the question; in fact, it provides information that disqualifies Magellan as the answer to the question as phrased. The question is about the individual's achievement, not just the initiation of the voyage.
286: 
287: --- Final Answer ---
288:  The status is not_entailment
289: 
290: --- Confidence ---
291:  100%. The statement directly contradicts Magellan being the first to *complete* the circumnavigation by explicitly stating he died before the journey was completed. The question asks for the explorer who *was* the first to circumnavigate, implying completion by that individual.
292:  ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Meta_Prompting.md">
  1: # **Meta Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Zero-shot Meta Prompting is a technique where you provide the model with a structured, example-free template that tells it how to solve the given problem.  Unlike Zero-Shot Chain-of-Thought (CoT), which tells the model to *‚Äúthink step by step‚Äù*, Meta Prompting gives the model a full structured blueprint for solving a task. 
  8: 
  9: This structured blueprint includes *how to begin*, *what steps to follow*, *how to format the reasoning* and *how to present the final answer*. This technique makes the model perform well because the structure itself guides its reasoning process.
 10: 
 11: ![Meta prompting](images/3-meta-prompt.jpg)
 12: 
 13: Figure from [Meta prompting ](https://arxiv.org/abs/2311.11482) paper. 
 14: 
 15: ## **Prompt Template**
 16: 
 17: Here is the prompt template for meta prompting.
 18: 
 19: ```
 20: You are a structured reasoning assistant that solves the given problem following the given solution structure.
 21: 
 22: Problem: {question}
 23: 
 24: Solution Structure:
 25:   Step 1: Begin the response with: "Let's think step by step."
 26:   Step 2: Identify the important components of the problem.
 27:   Step 3: Break the solution process into clear, logical steps.
 28:   Step 4: Present the final result in a LaTeX formatted box, like: \\boxed{{value}}
 29: 
 30: Final Answer: Provide only the final numeric answer.
 31: ```
 32: 
 33: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 34: 
 35: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 36: - ‚ú® Weekly GenAI updates
 37: - üìÑ Weekly LLM, Agents and RAG research paper updates
 38: - üìù 1 fresh blog post on an interesting topic every week
 39: 
 40: ## **Implementation**
 41: 
 42: Now let's see the implementation of meta promtping technique using LangChain v1.0
 43: 
 44: ```python
 45: # pip install langchain langchain-google-genai pydantic
 46: 
 47: import os
 48: from google.colab import userdata
 49: from langchain.chat_models import init_chat_model
 50: from langchain_core.prompts import ChatPromptTemplate
 51: from langchain_core.output_parsers import PydanticOutputParser
 52: from pydantic import BaseModel, Field
 53: 
 54: # 1. Set your API key
 55: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
 56: 
 57: # 2. Define the Pydantic schema for Meta Prompting structured output
 58: class MetaPromptResponse(BaseModel):
 59:     reasoning_chain: str = Field(..., description="Structured reasoning following the meta-prompt steps")
 60:     answer: str = Field(..., description="Final numeric answer only")
 61: 
 62: # 3. Create the parser
 63: parser = PydanticOutputParser(pydantic_object=MetaPromptResponse)
 64: 
 65: # 4. Initialize the chat model (gemini-2.5-flash)
 66: model = init_chat_model(
 67:     "gemini-2.5-flash",
 68:     model_provider="google_genai",
 69:     temperature=0
 70: )
 71: 
 72: # 5. Zero-Shot Meta Prompt Template (structure-focused)
 73: prompt_template = ChatPromptTemplate.from_template(
 74:     """
 75: You are a structured reasoning assistant that solves the given problem following the given solution structure.
 76: 
 77: Problem: {question}
 78: 
 79: Solution Structure:
 80:   Step 1: Begin the response with: "Let's think step by step."
 81:   Step 2: Identify the important components of the problem.
 82:   Step 3: Break the solution process into clear, logical steps.
 83:   Step 4: Present the final result in a LaTeX formatted box, like: \\boxed{{value}}
 84: 
 85: Final Answer: Provide only the final numeric answer.
 86: 
 87: Return your response using this JSON format:
 88: {format_instructions}
 89: """
 90: )
 91: 
 92: # 6. Inject parser instructions
 93: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
 94: 
 95: # 7. Build the LCEL chain
 96: chain = prompt | model | parser
 97: 
 98: # 8. Example mathematical question
 99: question = "Solve for x: 3x + 12 = 39."
100: 
101: # 9. Run the chain
102: result = chain.invoke({"question": question})
103: 
104: # 10. Display the structured reasoning and final answer
105: print("\n--- Reasoning Chain (Structured Meta Prompt) ---\n", result.reasoning_chain)
106: print("\n--- Final Answer ---\n", result.answer)
107: ```
108: 
109: Here the output is
110: ```
111: --- Reasoning Chain (Structured Meta Prompt) ---
112:  Let's think step by step.
113: 
114: Step 2: The important components of the problem are the linear equation 3x + 12 = 39 and the objective to solve for the variable x.
115: 
116: Step 3: We will solve the equation by isolating x through algebraic manipulation.
117: 
118: 1.  Start with the given equation: 3x + 12 = 39
119: 2.  To isolate the term containing x (3x), subtract 12 from both sides of the equation:
120:     3x + 12 - 12 = 39 - 12
121:     3x = 27
122: 3.  To solve for x, divide both sides of the equation by 3:
123:     3x / 3 = 27 / 3
124:     x = 9
125: 
126: Step 4: The final result is \boxed{9}.
127: 
128: --- Final Answer ---
129:  9
130: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Multi_Chain_Reasoning_Prompting.md">
  1: # **Multi-Chain Reasoning Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Multi-Chain Reasoning Prompting (MCR, also called Meta-Chain Reasoning) is an advanced way to ask a Large Language Model (LLM) to solve a problem. Instead of asking the LLM for a single answer, you first ask it to generate multiple different thinking processes (called "Chain-of-Thought" paths) for the same problem. Once it has these multiple chains, it performs a second-level, or "meta," reasoning step. 
  8: 
  9: This means the model acts as a smart editor or synthesizer: it reviews and compares all the different steps it generated to form a final improved answer. Thus it can correct cases where none of the single chains were perfect, but by combining pieces of different chains you can get a better result. 
 10: 
 11: ![Multi Chain Reasoning prompting](images/3-multi-chain-prompt.jpg)
 12: 
 13: Figure from [Multi Chain Reasoning prompting](https://arxiv.org/abs/2304.13007) paper. 
 14: 
 15: ## **Prompt Template**
 16: 
 17: Here is the generation prompt template for multi chain reasoning prompting.
 18: 
 19: ```
 20: You are a careful step-by-step reasoning assistant.
 21: 
 22: Question: {question}
 23: 
 24: Instructions:
 25: - Think step by step.
 26: - Produce a clear and logically consistent chain of thought.
 27: - Then provide the final answer in free-form text.
 28: ```
 29: Here is the meta reasoning prompt template for multi chain reasoning prompting.
 30: 
 31: ```
 32: You are a meta-reasoning assistant.
 33: 
 34: You are given multiple reasoning chains generated independently for the
 35: same question. Your task is to:
 36: 
 37: 1. Compare all reasoning chains.
 38: 2. Identify errors, inconsistencies, or weaknesses.
 39: 3. Combine the correct reasoning steps from all chains to form a
 40:    refined, more robust meta-reasoning.
 41: 4. Produce the final free-form answer.
 42: 
 43: Question:
 44: {question}
 45: 
 46: Reasoning Chains:
 47: {all_chains}
 48: 
 49: Instructions:
 50: - Perform explicit meta-analysis.
 51: - Synthesize the best reasoning from all chains.
 52: - Return your combined reasoning and final answer in this JSON format:
 53: 
 54: {{
 55:   "meta_reasoning": "...",
 56:   "answer": "..."
 57: }}
 58: ```
 59: 
 60: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 61: 
 62: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 63: - ‚ú® Weekly GenAI updates
 64: - üìÑ Weekly LLM, Agents and RAG research paper updates
 65: - üìù 1 fresh blog post on an interesting topic every week
 66: 
 67: ## **Zero-Shot Implementation**
 68: 
 69: Now let's see the implementation of zero-shot multi-chain reasoning promtping technique using LangChain v1.0
 70: 
 71: ```python
 72: # ---------------------------------------------------------
 73: # Zero-Shot Multi-Chain Reasoning Prompting (MCR)
 74: # ---------------------------------------------------------
 75: 
 76: # pip install langchain langchain-google-genai pydantic
 77: 
 78: import os
 79: import time
 80: from google.colab import userdata
 81: from langchain.chat_models import init_chat_model
 82: from langchain_core.prompts import ChatPromptTemplate
 83: from langchain_core.output_parsers import PydanticOutputParser
 84: from pydantic import BaseModel, Field
 85: 
 86: # ---------------------------------------------------------
 87: # 1. Set your Gemini API key
 88: # ---------------------------------------------------------
 89: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 90: 
 91: 
 92: # ---------------------------------------------------------
 93: # 2. Structured model for each reasoning chain (free-form answer)
 94: # ---------------------------------------------------------
 95: class MCRCandidate(BaseModel):
 96:     reasoning_chain: str = Field(
 97:         ..., description="Full reasoning steps used to derive the answer"
 98:     )
 99:     answer: str = Field(
100:         ..., description="Final free-form answer (not restricted to numeric)"
101:     )
102: 
103: 
104: parser = PydanticOutputParser(pydantic_object=MCRCandidate)
105: 
106: 
107: # ---------------------------------------------------------
108: # 3. Initialize Gemini model with sampling enabled
109: # ---------------------------------------------------------
110: model = init_chat_model(
111:     "gemini-2.5-flash",
112:     model_provider="google_genai",
113:     temperature=0.8,
114:     top_k=40,
115: )
116: 
117: 
118: # ---------------------------------------------------------
119: # 4. Zero-shot chain generation prompt
120: # ---------------------------------------------------------
121: generation_prompt_template = ChatPromptTemplate.from_template(
122:     """
123: You are a careful step-by-step reasoning assistant.
124: 
125: Question: {question}
126: 
127: Instructions:
128: - Think step by step.
129: - Produce a clear and logically consistent chain of thought.
130: - Then provide the final answer in free-form text.
131: 
132: Return output in the following JSON format:
133: {format_instructions}
134: """
135: )
136: 
137: generation_prompt = generation_prompt_template.partial(
138:     format_instructions=parser.get_format_instructions()
139: )
140: 
141: gen_chain = generation_prompt | model | parser
142: 
143: 
144: # ---------------------------------------------------------
145: # 5. Meta-Reasoning Combination Prompt
146: # ---------------------------------------------------------
147: meta_prompt = ChatPromptTemplate.from_template(
148:     """
149: You are a meta-reasoning assistant.
150: 
151: You are given multiple reasoning chains generated independently for the
152: same question. Your task is to:
153: 
154: 1. Compare all reasoning chains.
155: 2. Identify errors, inconsistencies, or weaknesses.
156: 3. Combine the correct reasoning steps from all chains to form a
157:    refined, more robust meta-reasoning.
158: 4. Produce the final free-form answer.
159: 
160: Question:
161: {question}
162: 
163: Reasoning Chains:
164: {all_chains}
165: 
166: Instructions:
167: - Perform explicit meta-analysis.
168: - Synthesize the best reasoning from all chains.
169: - Return your combined reasoning and final answer in this JSON format:
170: 
171: {{
172:   "meta_reasoning": "...",
173:   "answer": "..."
174: }}
175: """
176: )
177: 
178: meta_chain = meta_prompt | model
179: 
180: 
181: # ---------------------------------------------------------
182: # 6. Multi-Chain Reasoning main function
183: # ---------------------------------------------------------
184: def multi_chain_reasoning(question: str, n_samples: int = 3):
185:     candidates = []
186: 
187:     # ---- Stage 1: Generate multiple independent reasoning chains ----
188:     for _ in range(n_samples):
189:         out = gen_chain.invoke({"question": question})
190:         candidates.append(out)
191:         time.sleep(1)
192: 
193:     # Prepare formatted reasoning chains for meta-prompt
194:     chain_text = ""
195:     for i, c in enumerate(candidates, 1):
196:         chain_text += (
197:             f"\n[{i}] Reasoning:\n{c.reasoning_chain}\nFinal Answer: {c.answer}\n"
198:         )
199: 
200:     # ---- Stage 2: Meta-combine reasoning chains ----
201:     meta_output = meta_chain.invoke(
202:         {
203:             "question": question,
204:             "all_chains": chain_text,
205:         }
206:     )
207: 
208:     return meta_output, candidates
209: 
210: 
211: # ---------------------------------------------------------
212: # 7. Run MCR on the updated example question
213: # ---------------------------------------------------------
214: question = (
215:     "A train leaves at 8:15 AM and takes 4 hours and 35 minutes "
216:     "to reach its destination. If the destination city is 2 hours "
217:     "ahead of the starting city's time zone, what time is it at the "
218:     "destination city when the train arrives?"
219: )
220: 
221: meta_output, all_candidates = multi_chain_reasoning(question, n_samples=3)
222: 
223: 
224: # ---------------------------------------------------------
225: # 8. Display results
226: # ---------------------------------------------------------
227: print("\n===== META CHAIN REASONING OUTPUT =====")
228: print(meta_output.content)
229: 
230: print("\n===== ALL GENERATED CANDIDATE CHAINS =====")
231: for i, c in enumerate(all_candidates, 1):
232:     print(f"\n--- Candidate {i} ---")
233:     print(c.reasoning_chain)
234:     print("Answer:", c.answer)
235: ```
236: 
237: Here the output is
238: ```
239: 
240: ===== META CHAIN REASONING OUTPUT =====
241: {
242:   "meta_reasoning": "All three reasoning chains correctly identify the necessary steps to solve the problem. They all begin by calculating the train's arrival time in the starting city's time zone. This is done by adding the travel duration (4 hours and 35 minutes) to the departure time (8:15 AM). \n\nStarting with 8:15 AM, adding 4 hours results in 12:15 PM. Then, adding 35 minutes to 12:15 PM yields an arrival time of 12:50 PM in the starting city's time zone.\n\nNext, all chains correctly account for the time zone difference. The problem states the destination city is 2 hours 'ahead' of the starting city's time zone. Therefore, 2 hours must be added to the arrival time calculated in the starting city's time zone.\n\nAdding 2 hours to 12:50 PM results in 2:50 PM.\n\nAll chains consistently follow these steps and arrive at the same correct final answer. There are no errors, inconsistencies, or weaknesses found across the chains; they are all sound and demonstrate a clear understanding of time calculations and time zone adjustments.",
243:   "answer": "The train arrives at 2:50 PM in the destination city."
244: }
245: 
246: ===== ALL GENERATED CANDIDATE CHAINS =====
247: 
248: --- Candidate 1 ---
249: 1. **Determine the departure time:** The train leaves at 8:15 AM.
250: 2. **Calculate the travel duration:** The journey takes 4 hours and 35 minutes.
251: 3. **Calculate the arrival time in the starting city's time zone:**
252:     *   Add 4 hours to 8:15 AM: 8:15 AM + 4 hours = 12:15 PM.
253:     *   Add 35 minutes to 12:15 PM: 12:15 PM + 35 minutes = 12:50 PM.
254:     *   So, the train arrives at 12:50 PM in the starting city's time zone.
255: 4. **Adjust for the time zone difference:** The destination city is 2 hours ahead of the starting city.
256:     *   Add 2 hours to the arrival time calculated in step 3: 12:50 PM + 2 hours = 2:50 PM.
257: 5. **State the final arrival time:** The train arrives at 2:50 PM in the destination city's time zone.
258: Answer: The train arrives at 2:50 PM in the destination city.
259: 
260: --- Candidate 2 ---
261: The train leaves at 8:15 AM. The travel duration is 4 hours and 35 minutes. First, we calculate the arrival time in the starting city's time zone. 
262: - Add 4 hours to 8:15 AM: 8:15 AM + 4 hours = 12:15 PM.
263: - Add 35 minutes to 12:15 PM: 12:15 PM + 35 minutes = 12:50 PM.
264: So, the train arrives at 12:50 PM in the starting city's time zone.
265: The destination city is 2 hours ahead of the starting city's time zone. Therefore, we need to add 2 hours to the calculated arrival time.
266: - 12:50 PM + 2 hours = 2:50 PM.
267: Thus, the train arrives at 2:50 PM in the destination city's time.
268: Answer: The train arrives at 2:50 PM at the destination city.
269: 
270: --- Candidate 3 ---
271: The train leaves at 8:15 AM. The travel duration is 4 hours and 35 minutes.
272: 
273: First, calculate the arrival time in the starting city's time zone:
274: Start Time: 8:15 AM
275: Add 4 hours: 8:15 AM + 4 hours = 12:15 PM
276: Add 35 minutes: 12:15 PM + 35 minutes = 12:50 PM
277: So, the train arrives at 12:50 PM in the starting city's time zone.
278: 
279: Next, adjust for the time zone difference. The destination city is 2 hours ahead of the starting city.
280: Arrival time in starting city's time zone: 12:50 PM
281: Add 2 hours for the time zone difference: 12:50 PM + 2 hours = 2:50 PM.
282: 
283: Therefore, the train arrives at 2:50 PM in the destination city's time.
284: Answer: The train arrives at 2:50 PM in the destination city.
285: ```
286: 
287: 
288: ## **Few-Shot Implementation**
289: 
290: Now let's see the implementation of few-shot multi-chain reasoning promtping technique using LangChain v1.0
291: 
292: ```python
293: # ---------------------------------------------------------
294: # Few-Shot Multi-Chain Reasoning Prompting (MCR)
295: # ---------------------------------------------------------
296: 
297: # pip install langchain langchain-google-genai pydantic
298: 
299: import os
300: import time
301: from pydantic import BaseModel, Field
302: from google.colab import userdata
303: from langchain.chat_models import init_chat_model
304: from langchain_core.prompts import ChatPromptTemplate
305: from langchain_core.output_parsers import PydanticOutputParser
306: 
307: 
308: # ---------------------------------------------------------
309: # 1. Set Gemini API key
310: # ---------------------------------------------------------
311: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
312: 
313: 
314: # ---------------------------------------------------------
315: # 2. Structured output schema for each chain
316: # ---------------------------------------------------------
317: class MCRCandidate(BaseModel):
318:     reasoning_chain: str = Field(
319:         ..., description="Full chain-of-thought reasoning"
320:     )
321:     answer: str = Field(
322:         ..., description="Final free-form answer"
323:     )
324: 
325: 
326: parser = PydanticOutputParser(pydantic_object=MCRCandidate)
327: 
328: 
329: # ---------------------------------------------------------
330: # 3. Initialize Gemini model with sampling enabled
331: # ---------------------------------------------------------
332: model = init_chat_model(
333:     "gemini-2.5-flash",
334:     model_provider="google_genai",
335:     temperature=0.8,
336:     top_k=40,
337: )
338: 
339: 
340: # ---------------------------------------------------------
341: # 4. Few-shot example (train/time-zone problem)
342: # ---------------------------------------------------------
343: few_shot_example = """
344: Example Problem:
345: A train leaves at 8:15 AM and takes 4 hours and 35 minutes to reach its destination.
346: If the destination city is 2 hours ahead of the starting city's time zone,
347: what time is it at the destination city when the train arrives?
348: 
349: Example Chain-of-Thought:
350: First compute the arrival time in the starting city.
351: 8:15 AM + 4 hours 35 minutes = 12:50 PM local time.
352: The destination city is 2 hours ahead, so convert 12:50 PM to destination time:
353: 12:50 PM + 2 hours = 2:50 PM.
354: Thus, when the train arrives, the destination city's local time is 2:50 PM.
355: 
356: Example Final Answer:
357: 2:50 PM at the destination city.
358: """
359: 
360: 
361: # ---------------------------------------------------------
362: # 5. Few-shot generation prompt
363: # ---------------------------------------------------------
364: generation_prompt_template = ChatPromptTemplate.from_template(
365:     """
366: You are a detailed step-by-step reasoning assistant.
367: 
368: Below is a worked example:
369: {few_shot_example}
370: 
371: Now follow the same reasoning structure to answer the new question.
372: 
373: New Question:
374: {question}
375: 
376: Instructions:
377: - Provide a full chain-of-thought reasoning.
378: - Then give a concise final answer.
379: - Respond in this JSON format:
380: {format_instructions}
381: """
382: )
383: 
384: generation_prompt = generation_prompt_template.partial(
385:     few_shot_example=few_shot_example,
386:     format_instructions=parser.get_format_instructions()
387: )
388: 
389: gen_chain = generation_prompt | model | parser
390: 
391: 
392: # ---------------------------------------------------------
393: # 6. Meta-Reasoning Combination Prompt
394: # ---------------------------------------------------------
395: meta_prompt = ChatPromptTemplate.from_template(
396:     """
397: You are a meta-reasoning assistant.
398: 
399: You are given multiple reasoning chains produced independently for the
400: same question. Your task is to:
401: 
402: 1. Compare all reasoning chains.
403: 2. Identify errors or inconsistencies.
404: 3. Combine the correct pieces of reasoning into one improved, unified chain.
405: 4. Produce the final free-form answer.
406: 
407: Question:
408: {question}
409: 
410: Candidate Reasoning Chains:
411: {all_chains}
412: 
413: Return your response in the following JSON format:
414: 
415: {{
416:   "meta_reasoning": "...",
417:   "answer": "..."
418: }}
419: """
420: )
421: 
422: meta_chain = meta_prompt | model
423: 
424: 
425: # ---------------------------------------------------------
426: # 7. Few-Shot Multi-Chain Reasoning function (n_samples = 3)
427: # ---------------------------------------------------------
428: def multi_chain_reasoning(question: str, n_samples: int = 3):
429:     candidates = []
430: 
431:     # ---- Stage 1: Generate independent chains ----
432:     for _ in range(n_samples):
433:         out = gen_chain.invoke({"question": question})
434:         candidates.append(out)
435:         time.sleep(1)
436: 
437:     # Prepare reasoning block for meta prompt
438:     formatted = ""
439:     for idx, c in enumerate(candidates, 1):
440:         formatted += (
441:             f"\n[{idx}] Reasoning:\n{c.reasoning_chain}\nFinal Answer: {c.answer}\n"
442:         )
443: 
444:     # ---- Stage 2: Meta-synthesis ----
445:     meta_output = meta_chain.invoke(
446:         {"question": question, "all_chains": formatted}
447:     )
448: 
449:     return meta_output, candidates
450: 
451: 
452: # ---------------------------------------------------------
453: # 8. Run Few-shot Multi-Chain Reasoning
454: # ---------------------------------------------------------
455: question = (
456:     "Identify the capital city of the largest country in South America, "
457:     "and then state which continent that capital city is located on."
458: )
459: 
460: meta_output, all_outputs = multi_chain_reasoning(question, n_samples=3)
461: 
462: 
463: # ---------------------------------------------------------
464: # 9. Display results
465: # ---------------------------------------------------------
466: print("\n===== FINAL META-SYNTHESIZED ANSWER =====")
467: print(meta_output.content)
468: 
469: print("\n===== ALL GENERATED CANDIDATE CHAINS =====")
470: for i, out in enumerate(all_outputs, 1):
471:     print(f"\n--- Candidate {i} ---")
472:     print(out.reasoning_chain)
473:     print("Answer:", out.answer)
474: ```
475: 
476: Here the output is
477: 
478: ```
479: 
480: ===== FINAL META-SYNTHESIZED ANSWER =====
481: {
482:   "meta_reasoning": "All three reasoning chains correctly identify Brazil as the largest country in South America, Bras√≠lia as its capital, and South America as the continent where Bras√≠lia is located. There are no factual errors or inconsistencies in the reasoning steps across the chains. The only minor difference lies in the phrasing of the final answer. Chains [2] and [3] provide a concise answer ('Bras√≠lia, South America'), while Chain [1] provides a more complete sentence that explicitly answers both parts of the question ('The capital city of the largest country in South America is Bras√≠lia, and it is located on the continent of South America.'). For a 'free-form answer' that fully addresses the prompt 'Identify... and then state...', Chain [1]'s final answer is slightly more complete and directly responsive to both clauses of the question. Therefore, the improved, unified chain will consolidate the consistent correct reasoning, and the final answer will adopt the more comprehensive phrasing from Chain [1].",
483:   "answer": "The capital city of the largest country in South America is Bras√≠lia, and it is located on the continent of South America."
484: }
485: 
486: ===== ALL GENERATED CANDIDATE CHAINS =====
487: 
488: --- Candidate 1 ---
489: First, identify the largest country in South America. Brazil is the largest country in South America by both land area and population. Next, identify the capital city of Brazil, which is Bras√≠lia. Finally, state the continent where Bras√≠lia is located. Bras√≠lia is located within Brazil, which is on the continent of South America.
490: Answer: The capital city of the largest country in South America is Bras√≠lia, and it is located on the continent of South America.
491: 
492: --- Candidate 2 ---
493: First, identify the largest country in South America. The largest country in South America by both area and population is Brazil. Next, identify the capital city of Brazil. The capital city of Brazil is Bras√≠lia. Finally, state which continent Bras√≠lia is located on. Bras√≠lia is located on the continent of South America.
494: Answer: Bras√≠lia, South America.
495: 
496: --- Candidate 3 ---
497: First, identify the largest country in South America. Brazil is the largest country in South America by both land area and population. Next, identify the capital city of Brazil, which is Bras√≠lia. Finally, state the continent where Bras√≠lia is located. Bras√≠lia is located on the continent of South America.
498: Answer: Bras√≠lia, South America
499: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Plan_and_Solve_Prompting.md">
  1: # **Plan and Solve Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Plan-and-Solve Prompting is a break down prompting technique that guides a model to approach complex problems in two deliberate stages:
  8: 
  9: 1. First create a plan ‚Äî understand the problem, extract important information, and outline the steps required to solve it.
 10: 2. Then execute the plan ‚Äî compute each step carefully and derive the final answer.
 11: 
 12: ![Plan and Solve prompting](images/2-plan-solve-prompt.jpg)
 13: 
 14: Figure from [Plan and Solve prompting](https://arxiv.org/abs/2305.04091) paper. 
 15: 
 16: ## **Prompt Template**
 17: Here is the prompt template for plan and solve prompting.
 18: 
 19: ```
 20: You are an expert step-by-step reasoning assistant using plan and solve prompting following the instruction.
 21: Let‚Äôs first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan. 
 22: 
 23: Then, let‚Äôs carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer."
 24: 
 25: Question:
 26: {question}
 27: 
 28: Answer: 
 29: 
 30: Important:
 31: - variables must list each extracted variable and its numeric value.
 32: - plan must contain a numbered plan of steps to compute the answer.
 33: - calculation must show the step-by-step execution of the plan with arithmetic.
 34: - final_answer must contain ONLY the final numeric answer (no units, no explanation).
 35: ```
 36: 
 37: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 38: 
 39: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 40: - ‚ú® Weekly GenAI updates
 41: - üìÑ Weekly LLM, Agents and RAG research paper updates
 42: - üìù 1 fresh blog post on an interesting topic every week
 43: 
 44: 
 45: ## **Zero-Shot Implementation**
 46: Now let's see the implementation of zero-shot plan and solve promtping technique using LangChain v1.0
 47: 
 48: ```python
 49: # pip install langchain langchain-google-genai pydantic
 50: 
 51: import os
 52: from langchain.chat_models import init_chat_model
 53: from langchain_core.prompts import ChatPromptTemplate
 54: from langchain_core.output_parsers import PydanticOutputParser
 55: from pydantic import BaseModel, Field
 56: 
 57: # 1. Set your Gemini API key
 58: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
 59: 
 60: # 2. Define structured output for Plan-and-Solve
 61: class PlanSolveResponse(BaseModel):
 62:     variables: str = Field(..., description="Extracted relevant variables and their numerals")
 63:     plan: str = Field(..., description="A complete step-by-step plan to solve the problem")
 64:     calculation: str = Field(..., description="Execution of the plan with intermediate calculations")
 65:     final_answer: str = Field(..., description="Final numeric answer only")
 66: 
 67: # 3. Create parser
 68: parser = PydanticOutputParser(pydantic_object=PlanSolveResponse)
 69: 
 70: # 4. Initialize Gemini model (gemini-2.5-flash)
 71: model = init_chat_model(
 72:     "gemini-2.5-flash",
 73:     model_provider="google_genai",
 74:     temperature=0
 75: )
 76: 
 77: # 5. Zero-Shot Plan-and-Solve Prompt Template
 78: prompt_template = ChatPromptTemplate.from_template(
 79:     """
 80: You are an expert step-by-step reasoning assistant using plan and solve prompting following the instruction.
 81: Let‚Äôs first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan. 
 82: Then, let‚Äôs carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), 
 83: solve the problem step by step, and show the answer."
 84: 
 85: 
 86: Question:
 87: {question}
 88: 
 89: Answer: 
 90: 
 91: Provide your solution in the following JSON format exactly:
 92: {format_instructions}
 93: 
 94: Important:
 95: - variables must list each extracted variable and its numeric value.
 96: - plan must contain a numbered plan of steps to compute the answer.
 97: - calculation must show the step-by-step execution of the plan with arithmetic.
 98: - final_answer must contain ONLY the final numeric answer (no units, no explanation).
 99: """
100: )
101: 
102: # 6. Insert parser instructions
103: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
104: 
105: # 7. Build LCEL chain
106: chain = prompt | model | parser
107: 
108: # 8. Invoke the chain using your train-speed Plan-and-Solve problem
109: question = """
110: A train travels at an average speed of 60 mph for the first 3 hours of a journey and then at an average speed
111: of 40 mph for the remaining 2 hours. What is the average speed of the train for the entire journey?
112: 
113: Answer Choices: (A) 52 mph (B) 50 mph (C) 48 mph (D) 46 mph (E) 45 mph
114: """
115: 
116: result = chain.invoke({"question": question})
117: 
118: # 9. Output
119: print("\n--- Variables ---\n", result.variables)
120: print("\n--- Plan ---\n", result.plan)
121: print("\n--- Calculation ---\n", result.calculation)
122: print("\n--- Final Answer ---\n", result.final_answer)
123: 
124: ```
125: 
126: Here the output is
127: ```
128: --- Variables ---
129:  Speed for the first part of the journey (S1) = 60 mph, Time for the first part of the journey (T1) = 3 hours, Speed for the second part of the journey (S2) = 40 mph, Time for the second part of the journey (T2) = 2 hours
130: 
131: --- Plan ---
132:  1. Calculate the distance covered in the first part of the journey (D1) using the formula D1 = S1 √ó T1. 2. Calculate the distance covered in the second part of the journey (D2) using the formula D2 = S2 √ó T2. 3. Calculate the total distance covered (D_total) by adding D1 and D2. 4. Calculate the total time taken for the journey (T_total) by adding T1 and T2. 5. Calculate the average speed for the entire journey (S_average) using the formula S_average = D_total / T_total.
133: 
134: --- Calculation ---
135:  1. Distance for the first part (D1) = 60 mph √ó 3 hours = 180 miles.
136: 2. Distance for the second part (D2) = 40 mph √ó 2 hours = 80 miles.
137: 3. Total distance (D_total) = D1 + D2 = 180 miles + 80 miles = 260 miles.
138: 4. Total time (T_total) = T1 + T2 = 3 hours + 2 hours = 5 hours.
139: 5. Average speed (S_average) = D_total / T_total = 260 miles / 5 hours = 52 mph.
140: 
141: --- Final Answer ---
142:  52
143: ```
144: 
145: 
146: ## **Few-Shot Implementation**
147: Now let's see the implementation of few-shot plan and solve promtping technique using LangChain v1.0
148: 
149: ```python
150: # !pip install langchain langchain-google-genai pydantic
151: 
152: import os
153: from google.colab import userdata
154: from langchain.chat_models import init_chat_model
155: from langchain_core.prompts import ChatPromptTemplate
156: from langchain_core.output_parsers import PydanticOutputParser
157: from pydantic import BaseModel, Field
158: 
159: # 1. Set your API key
160: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
161: 
162: # 2. Define Pydantic schema for Plan-and-Solve output
163: class PlanSolveResponse(BaseModel):
164:     variables: str = Field(..., description="Extracted variables with their numerical values")
165:     plan: str = Field(..., description="A complete numbered plan for solving the problem")
166:     calculation: str = Field(..., description="Execution of the plan with intermediate steps")
167:     final_answer: str = Field(..., description="Final numeric answer only")
168: 
169: # 3. Create parser
170: parser = PydanticOutputParser(pydantic_object=PlanSolveResponse)
171: 
172: # 4. Initialize Gemini model
173: model = init_chat_model(
174:     "gemini-2.5-flash",
175:     model_provider="google_genai",
176:     temperature=0
177: )
178: 
179: # 5. Few-shot Plan-and-Solve example (train problem)
180: few_shot_example = """
181: Goal: Solve the problem using Plan-and-Solve prompting.
182: 
183: Problem:
184: A train travels at an average speed of 60 mph for the first 3 hours
185: and then at 40 mph for the next 2 hours. What is the average speed
186: for the entire journey? Answer Choices: (A) 52 mph (B) 50 mph (C) 48 mph (D) 46 mph (E) 45 mph
187: 
188: 1. Variables:
189: - S1 = 60 mph
190: - T1 = 3 hours
191: - S2 = 40 mph
192: - T2 = 2 hours
193: 
194: 2. Plan:
195: 1. Compute D1 = S1 √ó T1
196: 2. Compute D2 = S2 √ó T2
197: 3. Compute total distance = D1 + D2
198: 4. Compute total time = T1 + T2
199: 5. Compute average speed = total distance √∑ total time
200: 
201: 3. Calculation:
202: - D1 = 60 √ó 3 = 180 miles
203: - D2 = 40 √ó 2 = 80 miles
204: - Total distance = 180 + 80 = 260
205: - Total time = 3 + 2 = 5
206: - Average speed = 260 √∑ 5 = 52
207: 
208: Final Answer: 52
209: """
210: 
211: # 6. Few-shot Plan-and-Solve prompt template
212: prompt_template = ChatPromptTemplate.from_template(
213:     """
214: You are an expert reasoning assistant.
215: 
216: Below is an example problem solved using **Plan-and-Solve prompting**:
217: {few_shot_example}
218: 
219: Now apply the same Plan-and-Solve structure to solve the following problem.
220: 
221: You must start your reasoning with this exact trigger sentence:
222: 
223: "Let‚Äôs first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan. Then, let‚Äôs carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer."
224: 
225: Question: {question}
226: 
227: Provide the answer in the following JSON format:
228: {format_instructions}
229: """
230: )
231: 
232: # 7. Inject example + parser instructions
233: prompt = prompt_template.partial(
234:     few_shot_example=few_shot_example,
235:     format_instructions=parser.get_format_instructions()
236: )
237: 
238: # 8. Build LCEL chain
239: chain = prompt | model | parser
240: 
241: # 9. Target problem (chemist dilution question)
242: question = (
243:     "A chemist has 40 liters of a solution that is 30% acid. "
244:     "How many liters of pure water must be added to dilute the solution "
245:     "so that the final mixture is 10% acid? "
246:     "Answer Choices: (A) 60 liters (B) 70 liters (C) 80 liters "
247:     "(D) 90 liters (E) 100 liters"
248: )
249: 
250: # 10. Run the chain
251: result = chain.invoke({"question": question})
252: 
253: # 11. Display output sections
254: print("\n--- Variables ---\n", result.variables)
255: print("\n--- Plan ---\n", result.plan)
256: print("\n--- Calculation ---\n", result.calculation)
257: print("\n--- Final Answer ---\n", result.final_answer)
258: 
259: ```
260: 
261: Here the output is
262: ```
263: --- Variables ---
264:  V_initial = 40 liters (initial volume of solution)
265: C_initial = 30% = 0.30 (initial acid concentration)
266: C_final = 10% = 0.10 (final acid concentration)
267: W_added = ? (liters of pure water to be added)
268: 
269: --- Plan ---
270:  1. Calculate the initial amount of acid in the solution (Amount_acid = V_initial √ó C_initial).
271: 2. Recognize that adding pure water does not change the amount of acid in the solution.
272: 3. Define the final total volume of the solution (V_final = V_initial + W_added).
273: 4. Set up an equation using the final acid concentration: Amount_acid = C_final √ó V_final.
274: 5. Substitute the expression for V_final into the equation: Amount_acid = C_final √ó (V_initial + W_added).
275: 6. Solve the equation for W_added.
276: 
277: --- Calculation ---
278:  1. Calculate initial amount of acid:
279:    Amount_acid = V_initial √ó C_initial = 40 liters √ó 0.30 = 12 liters
280: 2. The amount of acid in the final solution remains 12 liters.
281: 3. Let V_final be the total volume after adding water.
282: 4. Set up the equation for the final concentration:
283:    Amount_acid = C_final √ó V_final
284:    12 = 0.10 √ó V_final
285: 5. Solve for V_final:
286:    V_final = 12 / 0.10 = 120 liters
287: 6. Calculate the amount of water added:
288:    W_added = V_final - V_initial
289:    W_added = 120 liters - 40 liters = 80 liters
290: 
291: --- Final Answer ---
292:  80
293: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Program_of_Thoughts_Prompting.md">
  1: # **Program of Thoughts Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Program of Thoughts (PoT) Prompting is a beak down prompting technique in which a Large Language Model (LLM) solves mathematical or symbolic problems by generating executable Python code rather than explaining its reasoning in natural language.
  8: 
  9: This method separates thinking from calculating:
 10: 
 11: - The LLM performs the reasoning by writing a clear, semantically meaningful program.
 12: - The interpreter performs the computation, ensuring perfect numerical accuracy.
 13: 
 14: PoT is especially effective for tasks involving arithmetic, geometry, algebra, symbolic manipulation, or multi-step calculations because code execution is reliable, deterministic, and precise.
 15: 
 16: ![Program of Thoughts prompting](images/3-program-prompt.jpg)
 17: 
 18: Figure from [Program of Thoughts prompting](https://arxiv.org/abs/2211.12588) paper. 
 19: 
 20: 
 21: ## **Prompt Template**
 22: 
 23: Here is the prompt template for program of thoughts promtping
 24: 
 25: ```
 26: You are an expert numerical reasoning assistant.
 27: 
 28: You must solve the problem using **Program-of-Thoughts (PoT)** prompting.
 29: 
 30: Your output MUST be ONLY Python code:
 31: 
 32: - Use step-by-step reasoning expressed as variable assignments.
 33: - Do NOT include comments.
 34: - Do NOT include print statements.
 35: - Use clear variable names.
 36: - The last line MUST be: ans = <final value>
 37: - The code MUST run in a Python interpreter.
 38: 
 39: Do NOT output natural language.  
 40: Do NOT add explanations.  
 41: ONLY return Python code.
 42: 
 43: Problem:
 44: {question}
 45: ```
 46: 
 47: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 48: 
 49: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 50: - ‚ú® Weekly GenAI updates
 51: - üìÑ Weekly LLM, Agents and RAG research paper updates
 52: - üìù 1 fresh blog post on an interesting topic every week
 53: 
 54: ## **Zero-Shot Implementation** 
 55: 
 56: Now let's see the implementation of zero-shot program of thoughts promtping technique using LangChain v1.0
 57: 
 58: ```python
 59: # !pip install langchain langchain-google-genai pydantic
 60: 
 61: import os
 62: from langchain.chat_models import init_chat_model
 63: from langchain_core.prompts import ChatPromptTemplate
 64: from langchain_core.output_parsers import PydanticOutputParser
 65: from pydantic import BaseModel, Field
 66: from langchain_experimental.utilities import PythonREPL
 67: 
 68: # 1. Set your Gemini API key
 69: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
 70: 
 71: # 2. Define PoT structured output
 72: class PoTResponse(BaseModel):
 73:     program: str = Field(..., description="Python code that computes the answer. Must assign final result to 'ans'.")
 74: 
 75: # 3. Parser
 76: parser = PydanticOutputParser(pydantic_object=PoTResponse)
 77: 
 78: # 4. Initialize Gemini model
 79: model = init_chat_model(
 80:     "gemini-2.5-flash",
 81:     model_provider="google_genai",
 82:     temperature=0
 83: )
 84: 
 85: # 5. Python Interpreter Tool (LangChain)
 86: python_repl = PythonREPL()
 87: 
 88: # 6. Zero-Shot PoT Prompt Template
 89: prompt_template = ChatPromptTemplate.from_template(
 90:     """
 91: You are an expert numerical reasoning assistant.
 92: 
 93: You must solve the problem using **Program-of-Thoughts (PoT)** prompting.
 94: 
 95: Your output MUST be ONLY Python code:
 96: 
 97: - Use step-by-step reasoning expressed as variable assignments.
 98: - Do NOT include comments.
 99: - Do NOT include print statements.
100: - Use clear variable names.
101: - The last line MUST be: ans = <final value>
102: - The code MUST run in a Python interpreter.
103: 
104: Do NOT output natural language.  
105: Do NOT add explanations.  
106: ONLY return Python code.
107: 
108: Problem:
109: {question}
110: 
111: Provide the solution in this JSON format:
112: {format_instructions}
113: """
114: )
115: 
116: # 7. Insert parser instructions
117: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
118: 
119: # 8. Build chain
120: chain = prompt | model | parser
121: 
122: # 9. Problem
123: question = """
124: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and
125: bakes muffins for her friends every day with four. She sells the remainder at the 
126: farmers' market daily for $2 per fresh duck egg. How much in dollars does she make 
127: every day at the farmers' market?
128: """
129: 
130: # 10. Invoke LLM ‚Üí get Python program
131: result = chain.invoke({"question": question})
132: 
133: print("\n--- Program Generated by LLM ---\n")
134: print(result.program)
135: 
136: # 11. Execute using LangChain Python Interpreter Tool
137: execution_output = python_repl.run(result.program)
138: 
139: # 12. Retrieve 'ans' from REPL environment
140: final_answer = python_repl.locals.get("ans", None)
141: 
142: print("\n--- Final Answer (from Python interpreter) ---\n")
143: print(final_answer)
144: 
145: ```
146: 
147: Here the output is
148: ```
149: --- Program Generated by LLM ---
150: 
151: eggs_laid_per_day = 16
152: eggs_eaten_for_breakfast = 3
153: eggs_used_for_muffins = 4
154: price_per_egg = 2
155: 
156: eggs_remaining_for_sale = eggs_laid_per_day - eggs_eaten_for_breakfast - eggs_used_for_muffins
157: daily_earnings = eggs_remaining_for_sale * price_per_egg
158: 
159: ans = daily_earnings
160: 
161: --- Final Answer (from Python interpreter) ---
162: 
163: 18
164: ```
165: 
166: 
167: ## **Few-Shot Implementation** 
168: 
169: Now let's see the implementation of few-shot program of thoughts promtping technique using LangChain v1.0
170: 
171: ```python
172: !pip install langchain langchain-google-genai pydantic langchain-experimental
173: 
174: import os
175: from google.colab import userdata
176: from langchain.chat_models import init_chat_model
177: from langchain_core.prompts import ChatPromptTemplate
178: from langchain_core.output_parsers import PydanticOutputParser
179: from pydantic import BaseModel, Field
180: from langchain_experimental.utilities import PythonREPL   # UPDATED IMPORT
181: 
182: # 1. Set API key
183: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
184: 
185: # 2. Structured PoT Response
186: class PoTResponse(BaseModel):
187:     program: str = Field(..., description="Python code that computes the answer, must assign to 'ans'.")
188: 
189: # 3. Parser
190: parser = PydanticOutputParser(pydantic_object=PoTResponse)
191: 
192: # 4. Initialize Gemini model
193: model = init_chat_model(
194:     "gemini-2.5-flash",
195:     model_provider="google_genai",
196:     temperature=0
197: )
198: 
199: # 5. Python Interpreter Tool (Experimental REPL)
200: python_repl = PythonREPL()
201: 
202: # 6. FEW-SHOT EXAMPLE (Only the first one kept)
203: few_shot_examples = """
204: Question: Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins with four. She sells the remainder at $2 per egg. How much does she make daily?
205: # Python code, return ans
206: total_eggs = 16
207: eaten = 3
208: baked = 4
209: sold = total_eggs - eaten - baked
210: price = 2
211: ans = sold * price
212: """
213: 
214: # 7. Few-Shot Prompt Template
215: prompt_template = ChatPromptTemplate.from_template(
216:     """
217: You are an expert numerical reasoning assistant.
218: 
219: Below is an example demonstrating **Program of Thoughts (PoT) prompting**.
220: The solution is expressed entirely as Python code with the final value stored in `ans`.
221: 
222: {few_shot_examples}
223: 
224: Now solve the following problem using the SAME format:
225: 
226: Problem:
227: {question}
228: 
229: Output Instructions:
230: - Output ONLY executable Python code.
231: - Use clear variable names.
232: - No comments.
233: - No print statements.
234: - Last line MUST be: ans = <final value>
235: 
236: Return your output in this JSON format:
237: {format_instructions}
238: """
239: )
240: 
241: # 8. Insert few-shot example + parser instructions
242: prompt = prompt_template.partial(
243:     few_shot_examples=few_shot_examples,
244:     format_instructions=parser.get_format_instructions()
245: )
246: 
247: # 9. Build chain
248: chain = prompt | model | parser
249: 
250: # 10. Current Problem
251: question = """
252: A cylindrical water storage tank has a height of 5 meters and a radius of 2 meters.
253: The cost of water is $0.50 per cubic meter.. Calculate the total cost to fill the tank
254: completely. Use 3.14159 for œÄ.
255: """
256: 
257: # 11. Generate PoT Program
258: result = chain.invoke({"question": question})
259: 
260: print("\n--- Program Generated by LLM ---\n")
261: print(result.program)
262: 
263: # 12. Execute the generated Python program using REPL
264: execution_output = python_repl.run(result.program)
265: 
266: # 13. Retrieve answer
267: final_answer = python_repl.locals.get("ans", None)
268: 
269: print("\n--- Final Answer (from Python interpreter) ---\n")
270: print(final_answer)
271: 
272: ```
273: 
274: Here the output is
275: 
276: ```
277: --- Program Generated by LLM ---
278: 
279: height = 5
280: radius = 2
281: pi = 3.14159
282: cost_per_cubic_meter = 0.50
283: volume = pi * (radius ** 2) * height
284: total_cost = volume * cost_per_cubic_meter
285: ans = total_cost
286: 
287: --- Final Answer (from Python interpreter) ---
288: 
289: 31.4159
290: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Rephrase_and_Respond_Prompting.md">
  1: # **Rephrase and Respond Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Rephrase-and-Respond Prompting is a technique in which the model improves the clarity of a user‚Äôs question by first rewriting (rephrasing) the question in a clearer, more explicit form, and then answering that clarified version.  
  8: 
  9: The model:
 10: 
 11: 1. Rephrases the question to make all implicit information explicit
 12: 2. Expands the intent, adds needed details, and clarifies categories or constraints
 13: 3. Responds to the improved version of the question
 14: 
 15: By strengthening the question before solving it, the model reduces errors caused by misinterpretation, vague phrasing, or missing details.
 16: 
 17: ![Rephrase and Respond prompting](images/1-rephrase-respond-prompt.jpg)
 18: 
 19: Figure from [Rephrase and Respond prompting](https://arxiv.org/abs/2311.04205) paper.
 20: 
 21: ##  **Prompt Template**
 22: 
 23: Here is the prompt template for rephrase and respond prompting.
 24: 
 25: ```
 26: You are an expert reasoning assistant.
 27: 
 28: For the user question below, perform BOTH steps in a single reasoning flow:
 29: 
 30: 1. Rephrase and expand the question  
 31:    - Remove ambiguity  
 32:    - State the hidden intention clearly  
 33:    - Make the required reasoning explicit  
 34: 
 35: 2. Respond to the rephrased question  
 36:    - Follow the clarified interpretation  
 37:    - Provide a correct and well-reasoned answer  
 38: 
 39: User Question:
 40: {question}
 41: ```
 42: 
 43: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 44: 
 45: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 46: - ‚ú® Weekly GenAI updates
 47: - üìÑ Weekly LLM, Agents and RAG research paper updates
 48: - üìù 1 fresh blog post on an interesting topic every week
 49: 
 50: ## **Implementation**
 51: 
 52: Now let's see the implementation of rephrase and respond promtping technique using LangChain v1.0
 53: 
 54: ```python
 55: # !pip install langchain langchain-google-genai pydantic
 56: 
 57: import os
 58: from google.colab import userdata
 59: from langchain.chat_models import init_chat_model
 60: from langchain_core.prompts import ChatPromptTemplate
 61: from langchain_core.output_parsers import PydanticOutputParser
 62: from pydantic import BaseModel, Field
 63: 
 64: 
 65: # ----------------------------------------------------------
 66: # 1. Set Gemini API Key
 67: # ----------------------------------------------------------
 68: 
 69: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 70: 
 71: 
 72: # ----------------------------------------------------------
 73: # 2. Structured Output for Rephrase-and-Respond
 74: # ----------------------------------------------------------
 75: 
 76: class RaRResult(BaseModel):
 77:     rephrased_question: str = Field(..., description="The rephrased and expanded question")
 78:     response: str = Field(..., description="Final answer produced after rephrasing")
 79: 
 80: 
 81: rar_parser = PydanticOutputParser(pydantic_object=RaRResult)
 82: 
 83: 
 84: # ----------------------------------------------------------
 85: # 3. Initialize Gemini model
 86: # ----------------------------------------------------------
 87: 
 88: model = init_chat_model(
 89:     "gemini-2.5-flash",
 90:     model_provider="google_genai",
 91:     temperature=0
 92: )
 93: 
 94: 
 95: # ----------------------------------------------------------
 96: # 4. Single-Prompt Rephrase-and-Respond Template
 97: # ----------------------------------------------------------
 98: 
 99: rar_prompt_template = ChatPromptTemplate.from_template(
100:     """
101: You are an expert reasoning assistant.
102: 
103: For the user question below, perform BOTH steps in a single reasoning flow:
104: 
105: 1. Rephrase and expand the question  
106:    - Remove ambiguity  
107:    - State the hidden intention clearly  
108:    - Make the required reasoning explicit  
109: 
110: 2. Respond to the rephrased question  
111:    - Follow the clarified interpretation  
112:    - Provide a correct and well-reasoned answer  
113: 
114: User Question:
115: {question}
116: 
117: Provide your output in this JSON format:
118: {format_instructions}
119: """
120: )
121: 
122: rar_prompt = rar_prompt_template.partial(
123:     format_instructions=rar_parser.get_format_instructions()
124: )
125: 
126: 
127: # ----------------------------------------------------------
128: # 5. Build the LCEL Chain ‚Äî Only One LLM Call
129: # ----------------------------------------------------------
130: 
131: rar_chain = rar_prompt | model | rar_parser
132: 
133: 
134: # ----------------------------------------------------------
135: # 6. Run RaR on the Example Question
136: # ----------------------------------------------------------
137: 
138: question = "Identify the odd one out: Apple, Banana, Car, Orange."
139: 
140: result = rar_chain.invoke({"question": question})
141: 
142: print("\n--- REPHRASED QUESTION ---\n")
143: print(result.rephrased_question)
144: 
145: print("\n--- FINAL RESPONSE ---\n")
146: print(result.response)
147: ```
148: 
149: Here the output is
150: ```
151: --- REPHRASED QUESTION ---
152: 
153: Given the list of items 'Apple', 'Banana', 'Car', and 'Orange', identify which item is the 'odd one out' by determining a common semantic category that applies to three of the items, and then explicitly stating why the remaining item does not fit into that established category. The reasoning for the categorization must be clear.
154: 
155: --- FINAL RESPONSE ---
156: 
157: The odd one out is 'Car'.
158: 
159: **Reasoning:**
160: *   'Apple' is a type of fruit.
161: *   'Banana' is a type of fruit.
162: *   'Orange' is a type of fruit.
163: *   'Car' is a type of vehicle or mode of transportation.
164: 
165: Therefore, 'Apple', 'Banana', and 'Orange' all belong to the category of 'fruits', while 'Car' belongs to a completely different category, making it the odd one out.
166: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Self_Ask_Prompting.md">
  1: # **Self Ask Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Self-Ask prompting is an advanced prompting technique where the LLM learns how to break down a complex question into smaller follow-up questions by looking at one or more example demonstrations provided in the prompt.
  8: 
  9: In Self-Ask prompting, the model:
 10: 
 11: 1. Checks whether follow-up questions are needed.
 12: 2. Asks itself a sub-question.
 13: 3. Answers it.
 14: 4. Asks another follow-up question.
 15: 5. Continues until it has enough information.
 16: 6. Outputs: ‚ÄúSo the final answer is: ‚Ä¶‚Äù
 17: 
 18: This structured decomposition improves reasoning, especially for multi-step or compositional problems.
 19: 
 20: 
 21: ![Self Ask prompting](images/2-self-ask-prompt.jpg)
 22: 
 23: Figure from [Self Ask prompting](https://arxiv.org/abs/2210.03350) paper. 
 24: 
 25: 
 26: ## **Prompt Template**
 27: 
 28: Here is the prompt template for few-shot chain of thoughts prompting.
 29: 
 30: ```
 31: Here is an example problem solved using self-ask prompting:
 32: {few_shot_example}
 33: 
 34: Now solve the following question using a similar self-ask prompting approach:
 35: 
 36: Question: {question}
 37: ```
 38: 
 39: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 40: 
 41: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 42: - ‚ú® Weekly GenAI updates
 43: - üìÑ Weekly LLM, Agents and RAG research paper updates
 44: - üìù 1 fresh blog post on an interesting topic every week
 45: 
 46: ## **Implementation**
 47: 
 48: Now let's see the implementation of self ask promtping technique using LangChain v1.0
 49: 
 50: ```python
 51: !pip install langchain langchain-google-genai pydantic
 52: 
 53: import os
 54: from google.colab import userdata
 55: from langchain.chat_models import init_chat_model
 56: from langchain_core.prompts import ChatPromptTemplate
 57: from langchain_core.output_parsers import PydanticOutputParser
 58: from pydantic import BaseModel, Field
 59: 
 60: # 1. Set your API key
 61: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 62: 
 63: # 2. Define Pydantic schema
 64: class SelfAskResponse(BaseModel):
 65:     reasoning_chain: str = Field(..., description="Complete self-ask transcript (follow-ups + intermediate answers)")
 66:     answer: str = Field(..., description="Final answer only in MM/DD/YYYY format")
 67: 
 68: # 3. Create parser
 69: parser = PydanticOutputParser(pydantic_object=SelfAskResponse)
 70: 
 71: # 4. Initialize Gemini model
 72: model = init_chat_model(
 73:     "gemini-2.5-flash",
 74:     model_provider="google_genai",
 75:     temperature=0
 76: )
 77: 
 78: # 5. Few-shot Self-Ask example (1-shot)
 79: few_shot_example = """
 80: Q: The historical event was originally planned for 11/05/1852, but due to unexpected weather, it was moved forward by two days to today. What is the date 8 days from today in MM/DD/YYYY?
 81: Are follow up questions needed here: Yes.
 82: Follow up: What is today's date?
 83: Intermediate answer: Moving an event forward by two days from 11/05/1852 means today's date is 11/03/1852.
 84: Follow up: What date is 8 days from today?
 85: Intermediate answer: 8 days from 11/03/1852 is 11/11/1852.
 86: So the final answer is: 11/11/1852.
 87: """
 88: 
 89: # 6. Prompt template matching your exact requested pattern
 90: prompt_template = ChatPromptTemplate.from_template(
 91:     """
 92: You are a step-by-step reasoning assistant.
 93: 
 94: Here is an example problem solved using self-ask prompting:
 95: {few_shot_example}
 96: 
 97: Now solve the following question using a similar self-ask prompting approach:
 98: 
 99: Question: {question}
100: 
101: Provide your solution in the following JSON format:
102: {format_instructions}
103: """
104: )
105: 
106: # 7. Inject reference example + parser formatting into the prompt
107: prompt = prompt_template.partial(
108:     few_shot_example=few_shot_example,
109:     format_instructions=parser.get_format_instructions()
110: )
111: 
112: # 8. Build the LCEL chain
113: chain = prompt | model | parser
114: 
115: # 9. Target Question (given earlier)
116: question = (
117:     "A construction project started on 09/15/2024. The first phase took 12 days. "
118:     "The second phase was originally scheduled for 20 days, but was shortened by 3 days. "
119:     "What is the completion date of the second phase in MM/DD/YYYY?"
120: )
121: 
122: # 10. Run the chain
123: result = chain.invoke({"question": question})
124: 
125: # 11. Display result
126: print("\n--- Reasoning Chain (self-ask transcript) ---\n", result.reasoning_chain)
127: print("\n--- Final Answer ---\n", result.answer)
128: 
129: 
130: ```
131: Here the output is
132: ```
133: --- Reasoning Chain (self-ask transcript) ---
134:  Are follow up questions needed here: Yes.
135: Follow up: What is the completion date of the first phase?
136: Intermediate answer: The first phase started on 09/15/2024 and took 12 days. Adding 12 days to 09/15/2024 gives 09/27/2024. So, the first phase completed on 09/27/2024.
137: Follow up: What is the actual duration of the second phase?
138: Intermediate answer: The second phase was originally scheduled for 20 days but was shortened by 3 days. So, the actual duration is 20 - 3 = 17 days.
139: Follow up: What is the completion date of the second phase?
140: Intermediate answer: The second phase starts immediately after the first phase completes, which is 09/27/2024. It takes 17 days. Adding 17 days to 09/27/2024:
141: September has 30 days. From 09/27/2024, there are 3 days left in September (09/28, 09/29, 09/30).
142: 17 days - 3 days = 14 days remaining.
143: These 14 days will be in October. So, the date is 10/14/2024.
144: So the final answer is: 10/14/2024.
145: 
146: --- Final Answer ---
147:  10/14/2024
148: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Self_Consistency_Prompting.md">
  1: # **Self Consistency Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Self-Consistency Prompting is a decoding strategy that improves multi-step reasoning by *letting a model explore multiple different reasoning paths* and then picking the answer that appears most consistently across those paths. Instead of trusting a single chain-of-thought (the usual greedy decode), self-consistency asks the model to sample many possible chains-of-thought and then take a majority vote over the final answers.
  8: 
  9: Instead of asking the AI to guess the answer once (where it might make a silly mistake), you ask it to solve the same problem multiple times using different reasoning paths. Then, you look at all the answers and pick the one that appears most frequently (a "majority vote").
 10: 
 11: ![Self Consistency prompting](images/1-self-consistency-prompt.jpg)
 12: 
 13: Figure from [Self Consistency prompting](https://arxiv.org/abs/2203.11171) paper. 
 14: 
 15: ## **Prompt Template**
 16: 
 17: Here is the prompt template for self consistency prompting.
 18: 
 19: ```
 20: You are a step-by-step reasoning assistant.
 21: 
 22: Use deliberate, step-by-step reasoning.
 23: 
 24: Question: {question}
 25: 
 26: Instruction:
 27: - Think through the problem step by step.
 28: - Produce a full chain of thought.
 29: - Then give ONLY the final numeric answer.
 30: 
 31: Important:
 32: - reasoning_chain must contain multiple reasoning steps.
 33: - answer must contain ONLY the final numeric answer.
 34: ```
 35: 
 36: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 37: 
 38: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 39: - ‚ú® Weekly GenAI updates
 40: - üìÑ Weekly LLM, Agents and RAG research paper updates
 41: - üìù 1 fresh blog post on an interesting topic every week
 42: 
 43: ## **Zero-Shot Implementation**
 44: 
 45: Now let's see the implementation of zero-shot self consistency promtping technique using LangChain v1.0
 46: 
 47: ```python
 48: # !pip install langchain langchain-google-genai pydantic
 49: 
 50: import os
 51: import time
 52: from google.colab import userdata
 53: from langchain.chat_models import init_chat_model
 54: from langchain_core.prompts import ChatPromptTemplate
 55: from langchain_core.output_parsers import PydanticOutputParser
 56: from pydantic import BaseModel, Field
 57: from collections import Counter
 58: 
 59: # ---------------------------------------------------------
 60: # 1. Set your Gemini API key
 61: # ---------------------------------------------------------
 62: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 63: 
 64: 
 65: # ---------------------------------------------------------
 66: # 2. Define structured output model
 67: # ---------------------------------------------------------
 68: class SCResponse(BaseModel):
 69:     reasoning_chain: str = Field(..., description="Full reasoning steps")
 70:     answer: str = Field(..., description="Final numeric answer only")
 71: 
 72: 
 73: # ---------------------------------------------------------
 74: # 3. Create parser
 75: # ---------------------------------------------------------
 76: parser = PydanticOutputParser(pydantic_object=SCResponse)
 77: 
 78: 
 79: # ---------------------------------------------------------
 80: # 4. Initialize Gemini model with sampling enabled
 81: # ---------------------------------------------------------
 82: model = init_chat_model(
 83:     "gemini-2.5-flash",
 84:     model_provider="google_genai",
 85:     temperature=0.8,
 86:     top_k=40,
 87: )
 88: 
 89: 
 90: # ---------------------------------------------------------
 91: # 5. Zero-shot Self-Consistency Prompt
 92: # ---------------------------------------------------------
 93: prompt_template = ChatPromptTemplate.from_template(
 94:     """
 95: You are a step-by-step reasoning assistant.
 96: 
 97: Use deliberate, step-by-step reasoning.
 98: 
 99: Question: {question}
100: 
101: Instruction:
102: - Think through the problem step by step.
103: - Produce a full chain of thought.
104: - Then give ONLY the final numeric answer.
105: 
106: Return your output in this JSON format:
107: {format_instructions}
108: 
109: Important:
110: - reasoning_chain must contain multiple reasoning steps.
111: - answer must contain ONLY the final numeric answer.
112: """
113: )
114: 
115: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
116: 
117: 
118: # ---------------------------------------------------------
119: # 6. Build LCEL chain
120: # ---------------------------------------------------------
121: chain = prompt | model | parser
122: 
123: 
124: # ---------------------------------------------------------
125: # 7. Self-Consistency Sampling (with sleep + 5 samples)
126: # ---------------------------------------------------------
127: def self_consistency(question: str, samples: int = 5):
128:     answers = []
129:     all_outputs = []
130: 
131:     for i in range(samples):
132:         result = chain.invoke({"question": question})
133:         answers.append(result.answer)
134:         all_outputs.append(result)
135: 
136:         time.sleep(1)   # <-- prevents rate-limits
137: 
138:     final_answer = Counter(answers).most_common(1)[0][0]
139:     return final_answer, all_outputs
140: 
141: 
142: # ---------------------------------------------------------
143: # 8. Run on your example
144: # ---------------------------------------------------------
145: question = (
146:     "When I was 6 years old, my sister was half my age. Now I am 70 years old. How old is my sister?"
147: )
148: 
149: final_answer, outputs = self_consistency(question, samples=5)
150: 
151: 
152: # ---------------------------------------------------------
153: # 9. Display results
154: # ---------------------------------------------------------
155: print("\n===== SELF CONSISTENCY OUTPUT =====")
156: print("Final Aggregated Answer:", final_answer)
157: 
158: print("\n===== ALL SAMPLED REASONING PATHS =====")
159: for i, out in enumerate(outputs, 1):
160:     print(f"\n--- Sample {i} ---")
161:     print(out.reasoning_chain)
162:     print("Answer:", out.answer)
163: ```
164: 
165: Here the output is
166: ```
167: ===== SELF CONSISTENCY OUTPUT =====
168: Final Aggregated Answer: 67
169: 
170: ===== ALL SAMPLED REASONING PATHS =====
171: 
172: --- Sample 1 ---
173: First, I need to determine the age of the sister when the person was 6 years old. The problem states that when the person was 6, their sister was half their age. So, the sister's age was 6 / 2 = 3 years old. Next, I need to calculate the age difference between the person and their sister. When the person was 6 and the sister was 3, the age difference was 6 - 3 = 3 years. This age difference remains constant throughout their lives. Finally, I will apply this age difference to the person's current age. The person is now 70 years old. Since the sister is always 3 years younger, her current age is 70 - 3 = 67 years old.
174: Answer: 67
175: 
176: --- Sample 2 ---
177: First, I need to determine the age difference between the person and their sister. When the person was 6 years old, their sister was half their age, which means the sister was 6 / 2 = 3 years old. The age difference between them is 6 - 3 = 3 years. This age difference remains constant throughout their lives. Now, the person is 70 years old. To find the sister's current age, I subtract the constant age difference from the person's current age: 70 - 3 = 67 years old.
178: Answer: 67
179: 
180: --- Sample 3 ---
181: Step 1: Determine the sister's age when the person was 6 years old. The problem states the sister was half the person's age. So, sister's age = 6 / 2 = 3 years old.
182: Step 2: Calculate the age difference between the person and their sister. Age difference = Person's age - Sister's age = 6 - 3 = 3 years. This age difference remains constant throughout their lives.
183: Step 3: Apply the constant age difference to the person's current age. The person is now 70 years old. Since the sister is always 3 years younger, her current age will be 70 - 3 = 67 years old.
184: Answer: 67
185: 
186: --- Sample 4 ---
187: First, I need to determine the sister's age when the speaker was 6 years old. The problem states that at that time, the sister was half the speaker's age. So, 6 years / 2 = 3 years old. Next, I need to find the constant age difference between the speaker and the sister. Since the speaker was 6 and the sister was 3, the age difference is 6 - 3 = 3 years. This age difference remains constant throughout their lives. Finally, I apply this age difference to the speaker's current age. The speaker is now 70 years old. Therefore, the sister's age will be 70 - 3 = 67 years old.
188: Answer: 67
189: 
190: --- Sample 5 ---
191: Step 1: Determine the sister's age when the person was 6 years old. The problem states that when the person was 6, the sister was half their age. So, sister's age = 6 / 2 = 3 years old. 
192: Step 2: Calculate the age difference between the person and their sister. Age difference = Person's age - Sister's age = 6 - 3 = 3 years. 
193: Step 3: Understand that the age difference between two people remains constant over time. If the person is 3 years older than their sister, this difference will always be 3 years, regardless of how many years pass. 
194: Step 4: Apply the constant age difference to the person's current age. The person is now 70 years old. Since the sister is 3 years younger, her current age will be 70 - 3 = 67 years old.
195: 
196: Answer: 67
197: ```
198: 
199: 
200: ## **Few-Shot Implementation**
201: 
202: Now let's see the implementation of few-shot self consistency promtping technique using LangChain v1.0
203: 
204: ```python
205: # pip install langchain langchain-google-genai pydantic
206: 
207: import os
208: import time
209: from google.colab import userdata
210: from collections import Counter
211: from pydantic import BaseModel, Field
212: from langchain.chat_models import init_chat_model
213: from langchain_core.prompts import ChatPromptTemplate
214: from langchain_core.output_parsers import PydanticOutputParser
215: 
216: 
217: # ---------------------------------------------------------
218: # 1. Set Gemini API key
219: # ---------------------------------------------------------
220: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
221: 
222: 
223: # ---------------------------------------------------------
224: # 2. Define structured output schema
225: # ---------------------------------------------------------
226: class SCResponse(BaseModel):
227:     reasoning_chain: str = Field(..., description="Full step-by-step reasoning")
228:     answer: str = Field(..., description="Final numeric answer only")
229: 
230: 
231: parser = PydanticOutputParser(pydantic_object=SCResponse)
232: 
233: 
234: # ---------------------------------------------------------
235: # 3. Initialize Gemini model with sampling enabled
236: # ---------------------------------------------------------
237: model = init_chat_model(
238:     "gemini-2.5-flash",
239:     model_provider="google_genai",
240:     temperature=0.8,
241:     top_k=40,
242: )
243: 
244: 
245: # ---------------------------------------------------------
246: # 4. Few-shot example (your earlier example)
247: # ---------------------------------------------------------
248: few_shot_example = """
249: Example Problem:
250: When I was 6 years old, my sister was half my age. Now I am 70 years old. How old is my sister?
251: 
252: Example Chain-of-Thought:
253: When I was 6, my sister was half my age, meaning she was 3. So she is always 3 years younger than me.
254: Now I am 70, so she must be 70 - 3 = 67.
255: 
256: Example Final Answer:
257: 67
258: """
259: 
260: 
261: # ---------------------------------------------------------
262: # 5. Create Few-shot Prompt Template
263: # ---------------------------------------------------------
264: prompt_template = ChatPromptTemplate.from_template(
265:     """
266: You are a step-by-step reasoning assistant.
267: 
268: Below is a worked example:
269: {few_shot_example}
270: 
271: Now use a similar style of reasoning to answer the new question.
272: 
273: New Question:
274: {question}
275: 
276: Instructions:
277: - Provide a full chain-of-thought reasoning.
278: - Then give ONLY the final numeric answer.
279: - Respond in this JSON format:
280: {format_instructions}
281: 
282: Important:
283: - reasoning_chain must contain multiple reasoning steps.
284: - answer must contain ONLY the final numeric answer.
285: """
286: )
287: 
288: prompt = prompt_template.partial(
289:     format_instructions=parser.get_format_instructions(),
290:     few_shot_example=few_shot_example
291: )
292: 
293: 
294: # ---------------------------------------------------------
295: # 6. Build LCEL chain
296: # ---------------------------------------------------------
297: chain = prompt | model | parser
298: 
299: 
300: # ---------------------------------------------------------
301: # 7. Self-consistency Sampling (n_samples = 3)
302: # ---------------------------------------------------------
303: def self_consistency(question: str, samples: int = 3):
304:     answers = []
305:     outputs = []
306: 
307:     for _ in range(samples):
308:         result = chain.invoke({"question": question})
309:         outputs.append(result)
310:         answers.append(result.answer)
311: 
312:         time.sleep(1)   # Avoid rate-limit issues
313: 
314:     final_answer = Counter(answers).most_common(1)[0][0]
315:     return final_answer, outputs
316: 
317: 
318: # ---------------------------------------------------------
319: # 8. Run Few-shot Self-Consistency
320: # ---------------------------------------------------------
321: question = "If it takes 1 hour to dry 3 shirts outside on a sunny line, how long does it take to dry 9 shirts?"
322: 
323: final_answer, samples = self_consistency(question, samples=3)
324: 
325: 
326: # ---------------------------------------------------------
327: # 9. Display results
328: # ---------------------------------------------------------
329: print("\n===== FINAL AGGREGATED ANSWER =====")
330: print(final_answer)
331: 
332: print("\n===== ALL SAMPLED REASONING PATHS =====")
333: for i, out in enumerate(samples, 1):
334:     print(f"\n--- Sample {i} ---")
335:     print(out.reasoning_chain)
336:     print("Answer:", out.answer)
337: ```
338: 
339: Here the output is
340: ```
341: ===== FINAL AGGREGATED ANSWER =====
342: 1
343: 
344: ===== ALL SAMPLED REASONING PATHS =====
345: 
346: --- Sample 1 ---
347: The key factor in drying shirts outside on a sunny line is the time it takes for the sun and air to dry the fabric. All shirts placed on the line at the same time will dry simultaneously. If it takes 1 hour for 3 shirts to dry, it means that each individual shirt dries in 1 hour. Therefore, if you place 9 shirts on the line at the same time (assuming sufficient space and sun), they will all dry simultaneously, and the total time required will still be 1 hour.
348: Answer: 1
349: 
350: --- Sample 2 ---
351: The problem states that it takes 1 hour to dry 3 shirts outside on a sunny line. When shirts are hung on a line, they dry simultaneously, assuming they are all exposed to the same conditions (sun, wind). The drying time is determined by the environmental factors, not by the number of items drying at the same time, as long as there is enough space. Therefore, if 3 shirts dry in 1 hour, each individual shirt takes 1 hour to dry. If you hang 9 shirts on the line at the same time, they will all be drying concurrently under the same conditions. Consequently, it will still take 1 hour for all 9 shirts to dry.
352: Answer: 1
353: 
354: --- Sample 3 ---
355: The problem states that it takes 1 hour to dry 3 shirts. When drying shirts on a line outside, the shirts dry simultaneously, not sequentially. This means that if you put 3 shirts out, they all dry within that 1 hour. If you put 9 shirts out, assuming there is enough space on the line for all of them to be exposed to the sun and air at the same time, they will all be drying at the same rate. Therefore, the total time required for all 9 shirts to dry will still be the same amount of time it takes for one shirt (or any number of shirts placed simultaneously) to dry under those conditions.
356: 
357: Answer: 1
358: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Self_Refine_Prompting.md">
  1: # **Self Refine Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Self-Refine Prompting is an iterative reasoning technique in which a model improves its own output through a repeated cycle of generation ‚Üí feedback ‚Üí refinement.
  8: 
  9: Instead of producing a single answer in one attempt, the model first drafts an initial solution, then evaluates it, identifies flaws or opportunities for improvement, and finally produces a refined version. This loop can repeat several times until a high-quality final output is reached.
 10: 
 11: Just as a human writer drafts a paragraph, rereads it, notices issues, and revises it, the model engages in self-reflection to improve accuracy, clarity, and quality.
 12: 
 13: 
 14: ![Self Refine Prompting](images/1-self-refine-prompt.jpg)
 15: 
 16: Figure from [Self Refine Prompting](https://arxiv.org/abs/2303.17651) paper. 
 17: 
 18: 
 19: ## **Prompt Template**
 20: 
 21: Here is the initial draft prompt template for self refine prompting.
 22: 
 23: ```
 24: You are an expert Python developer.
 25: 
 26: Write the first draft solution to the task below, without feedback or refinement.
 27: Focus only on producing an initial attempt.
 28: 
 29: Task:
 30: {task}
 31: ```
 32: Here is the feedback prompt template for self refine prompting.
 33: 
 34: ```
 35: You are an expert code reviewer.
 36: 
 37: Given the initial draft below, generate **specific and actionable feedback**.
 38: Your feedback MUST identify:
 39: - What is missing
 40: - What is incorrect
 41: - What can be improved
 42: - Why the improvement is important
 43: 
 44: Do NOT rewrite the answer. Only critique it.
 45: 
 46: Task:
 47: {task}
 48: 
 49: Initial Draft:
 50: {draft}
 51: ```
 52: 
 53: Here is the refinement prompt template for self refine prompting.
 54: 
 55: ```
 56: You are an expert Python developer.
 57: 
 58: Refine the initial draft by applying the feedback.
 59: Your refined version MUST:
 60: - Correct errors
 61: - Address missing logic
 62: - Improve quality, clarity, and reliability
 63: - Follow best Python practices
 64: 
 65: Task:
 66: {task}
 67: 
 68: Initial Draft:
 69: {draft}
 70: 
 71: Feedback:
 72: {feedback}
 73: 
 74: Now produce the improved answer.
 75: ```
 76: 
 77: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 78: 
 79: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 80: - ‚ú® Weekly GenAI updates
 81: - üìÑ Weekly LLM, Agents and RAG research paper updates
 82: - üìù 1 fresh blog post on an interesting topic every week
 83: 
 84: ## **Implementation**
 85: 
 86: Now let's see the implementation of self refine promtping technique (without multi-loop iteration) using LangChain v1.0
 87: 
 88: ```python
 89: # pip install langchain langchain-google-genai pydantic
 90: 
 91: import os
 92: from google.colab import userdata
 93: from langchain.chat_models import init_chat_model
 94: from langchain_core.prompts import ChatPromptTemplate
 95: from langchain_core.output_parsers import PydanticOutputParser
 96: from pydantic import BaseModel, Field
 97: 
 98: 
 99: # 1. Set your Gemini API key
100: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
101: 
102: 
103: # ----------------------------------------------------------
104: # 2. Define Structured Output Models for Self-Refine
105: # ----------------------------------------------------------
106: 
107: class InitialDraft(BaseModel):
108:     draft: str = Field(..., description="The model's initial attempt at the solution")
109: 
110: 
111: class Feedback(BaseModel):
112:     feedback: str = Field(..., description="Actionable and specific feedback describing issues and improvements")
113: 
114: 
115: class RefinedOutput(BaseModel):
116:     refined_answer: str = Field(..., description="Improved solution incorporating the feedback")
117: 
118: 
119: initial_parser = PydanticOutputParser(pydantic_object=InitialDraft)
120: feedback_parser = PydanticOutputParser(pydantic_object=Feedback)
121: refine_parser = PydanticOutputParser(pydantic_object=RefinedOutput)
122: 
123: 
124: # ----------------------------------------------------------
125: # 3. Initialize Gemini model (gemini-2.5-flash)
126: # ----------------------------------------------------------
127: 
128: model = init_chat_model(
129:     "gemini-2.5-flash",
130:     model_provider="google_genai",
131:     temperature=0
132: )
133: 
134: 
135: # ----------------------------------------------------------
136: # 4. Prompt Templates for INITIAL ‚Üí FEEDBACK ‚Üí REFINE
137: # ----------------------------------------------------------
138: 
139: # 4.1 Initial Draft Prompt
140: initial_prompt_template = ChatPromptTemplate.from_template(
141:     """
142: You are an expert Python developer.
143: 
144: Write the first draft solution to the task below, without feedback or refinement.
145: Focus only on producing an initial attempt.
146: 
147: Task:
148: {task}
149: 
150: Provide the output in this JSON format:
151: {format_instructions}
152: """
153: )
154: 
155: initial_prompt = initial_prompt_template.partial(
156:     format_instructions=initial_parser.get_format_instructions()
157: )
158: 
159: 
160: # 4.2 Feedback Prompt
161: feedback_prompt_template = ChatPromptTemplate.from_template(
162:     """
163: You are an expert code reviewer.
164: 
165: Given the initial draft below, generate **specific and actionable feedback**.
166: Your feedback MUST identify:
167: - What is missing
168: - What is incorrect
169: - What can be improved
170: - Why the improvement is important
171: 
172: Do NOT rewrite the answer. Only critique it.
173: 
174: Task:
175: {task}
176: 
177: Initial Draft:
178: {draft}
179: 
180: Provide your feedback in this JSON format:
181: {format_instructions}
182: """
183: )
184: 
185: feedback_prompt = feedback_prompt_template.partial(
186:     format_instructions=feedback_parser.get_format_instructions()
187: )
188: 
189: 
190: # 4.3 Refinement Prompt
191: refine_prompt_template = ChatPromptTemplate.from_template(
192:     """
193: You are an expert Python developer.
194: 
195: Refine the initial draft by applying the feedback.
196: Your refined version MUST:
197: - Correct errors
198: - Address missing logic
199: - Improve quality, clarity, and reliability
200: - Follow best Python practices
201: 
202: Task:
203: {task}
204: 
205: Initial Draft:
206: {draft}
207: 
208: Feedback:
209: {feedback}
210: 
211: Now produce the improved answer.
212: 
213: Provide the refined output in this JSON format:
214: {format_instructions}
215: """
216: )
217: 
218: refine_prompt = refine_prompt_template.partial(
219:     format_instructions=refine_parser.get_format_instructions()
220: )
221: 
222: 
223: # ----------------------------------------------------------
224: # 5. Build LCEL Chains
225: # ----------------------------------------------------------
226: 
227: initial_chain = initial_prompt | model | initial_parser
228: feedback_chain = feedback_prompt | model | feedback_parser
229: refine_chain = refine_prompt | model | refine_parser
230: 
231: 
232: # ----------------------------------------------------------
233: # 6. Run Self-Refine on the User's Example Task
234: # ----------------------------------------------------------
235: 
236: task = "Write a Python function calculate_average that takes a list of numbers and returns the average."
237: 
238: # Phase 1 ‚Äî Initial Draft
239: initial_result = initial_chain.invoke({"task": task})
240: print("\n--- INITIAL DRAFT ---\n")
241: print(initial_result.draft)
242: 
243: # Phase 2 ‚Äî Feedback
244: feedback_result = feedback_chain.invoke({
245:     "task": task,
246:     "draft": initial_result.draft
247: })
248: print("\n--- FEEDBACK ---\n")
249: print(feedback_result.feedback)
250: 
251: # Phase 3 ‚Äî Refinement
252: refined_result = refine_chain.invoke({
253:     "task": task,
254:     "draft": initial_result.draft,
255:     "feedback": feedback_result.feedback
256: })
257: print("\n--- REFINED SOLUTION ---\n")
258: print(refined_result.refined_answer)
259: ```
260: 
261: Here the output is
262: 
263: ```
264: 
265: --- INITIAL DRAFT ---
266: 
267: def calculate_average(numbers):
268:     """
269:     Calculates the average of a list of numbers.
270: 
271:     Args:
272:         numbers (list): A list of numbers (integers or floats).
273: 
274:     Returns:
275:         float: The average of the numbers in the list.
276: 
277:     Raises:
278:         ValueError: If the input list is empty.
279:     """
280:     if not numbers:
281:         raise ValueError("Input list cannot be empty.")
282:     
283:     total_sum = sum(numbers)
284:     count = len(numbers)
285:     return total_sum / count
286: 
287: --- FEEDBACK ---
288: 
289: ### What is missing:
290: 1.  **Return Type Hint:** The function signature is missing a return type hint (`-> float`), which is specified in the docstring.
291: 
292: ### What is incorrect:
293: 1.  The core logic for calculating the average and handling an empty list is correct.
294: 
295: ### What can be improved:
296: 1.  **Add Return Type Hint:** Explicitly add `-> float` to the function signature.
297: 2.  **Input Validation for Element Types:** While the docstring specifies 'A list of numbers (integers or floats)', the current implementation relies on `sum()` to raise a `TypeError` if non-numeric elements are present. This could be improved by adding explicit validation for the types of elements within the `numbers` list.
298: 3.  **More Specific Error Handling for Non-Numeric Types:** Instead of letting `sum()` raise a generic `TypeError`, the function could catch this or perform checks to raise a more specific `TypeError` or `ValueError` if the list contains non-numeric items.
299: 
300: ### Why the improvement is important:
301: 1.  **Return Type Hint:** Adding `-> float` to the signature improves code readability, maintainability, and enables static analysis tools (like MyPy) to catch potential type-related bugs at development time, making the function's contract clearer and more robust.
302: 2.  **Input Validation for Element Types:** Explicitly validating that all elements in the `numbers` list are indeed numbers (integers or floats) ensures the function adheres strictly to its documented input contract. This prevents unexpected runtime errors from internal Python functions (`sum()` in this case) and allows the function to provide more controlled and user-friendly error messages.
303: 3.  **More Specific Error Handling:** Providing a custom error message when non-numeric types are encountered makes debugging easier for the caller. Instead of a generic `TypeError` from `sum()`, a message like "Input list must contain only numbers" would clearly indicate the problem, improving the user experience and the robustness of the function.
304: 
305: --- REFINED SOLUTION ---
306: 
307: def calculate_average(numbers: list) -> float:
308:     """
309:     Calculates the average of a list of numbers.
310: 
311:     Args:
312:         numbers (list): A list of numbers (integers or floats).
313: 
314:     Returns:
315:         float: The average of the numbers in the list.
316: 
317:     Raises:
318:         ValueError: If the input list is empty.
319:         TypeError: If the input list contains non-numeric elements.
320:     """
321:     if not numbers:
322:         raise ValueError("Input list cannot be empty.")
323: 
324:     # Validate that all elements in the list are numbers (int or float)
325:     for num in numbers:
326:         if not isinstance(num, (int, float)):
327:             raise TypeError("All elements in the input list must be numbers (int or float).")
328:     
329:     total_sum = sum(numbers)
330:     count = len(numbers)
331:     return total_sum / count
332: ```
333: 
334: 
335: ## **Implemntation (Multi-loop)**
336: 
337: Now let's see the implementation of self refine promtping technique with multi-loop iteration using LangChain v1.0
338: 
339: ```python
340: # pip install langchain langchain-google-genai pydantic
341: 
342: import os
343: import time
344: from google.colab import userdata
345: from langchain.chat_models import init_chat_model
346: from langchain_core.prompts import ChatPromptTemplate
347: from langchain_core.output_parsers import PydanticOutputParser
348: from pydantic import BaseModel, Field
349: 
350: # 1. Set your Gemini API key
351: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
352: 
353: 
354: # ----------------------------------------------------------
355: # 2. Define Structured Output Models
356: # ----------------------------------------------------------
357: 
358: class InitialDraft(BaseModel):
359:     draft: str = Field(..., description="The model's initial attempt at the solution")
360: 
361: class Feedback(BaseModel):
362:     feedback: str = Field(..., description="Specific, actionable feedback. If no issues, must include the phrase 'no issues'.")
363: 
364: class RefinedOutput(BaseModel):
365:     refined_answer: str = Field(..., description="Improved answer incorporating the feedback")
366: 
367: 
368: initial_parser = PydanticOutputParser(pydantic_object=InitialDraft)
369: feedback_parser = PydanticOutputParser(pydantic_object=Feedback)
370: refine_parser = PydanticOutputParser(pydantic_object=RefinedOutput)
371: 
372: 
373: # ----------------------------------------------------------
374: # 3. Initialize Gemini model
375: # ----------------------------------------------------------
376: 
377: model = init_chat_model(
378:     "gemini-2.5-flash",
379:     model_provider="google_genai",
380:     temperature=0
381: )
382: 
383: 
384: # ----------------------------------------------------------
385: # 4. Prompt Templates
386: # ----------------------------------------------------------
387: 
388: # 4.1 Initial Draft Prompt
389: initial_prompt_template = ChatPromptTemplate.from_template(
390:     """
391: You are an expert Python developer.
392: 
393: Write the FIRST DRAFT solution to the task below.
394: Do NOT critique or refine it yet.
395: 
396: Task:
397: {task}
398: 
399: Output format:
400: {format_instructions}
401: """
402: )
403: 
404: initial_prompt = initial_prompt_template.partial(
405:     format_instructions=initial_parser.get_format_instructions()
406: )
407: 
408: 
409: # 4.2 Feedback Prompt
410: feedback_prompt_template = ChatPromptTemplate.from_template(
411:     """
412: You are an expert code reviewer.
413: 
414: Carefully analyze the initial or refined draft.
415: Provide feedback that is:
416: 
417: - Specific
418: - Actionable
419: - Mentioning what to fix and why
420: 
421: If the answer is already correct, complete, and high-quality,
422: write feedback that **explicitly contains the phrase "no issues"**.
423: 
424: Task:
425: {task}
426: 
427: Draft Under Review:
428: {draft}
429: 
430: Output format:
431: {format_instructions}
432: """
433: )
434: 
435: feedback_prompt = feedback_prompt_template.partial(
436:     format_instructions=feedback_parser.get_format_instructions()
437: )
438: 
439: 
440: # 4.3 Refinement Prompt
441: refine_prompt_template = ChatPromptTemplate.from_template(
442:     """
443: You are an expert Python developer.
444: 
445: Refine the draft by applying the feedback.
446: Improve correctness, clarity, robustness, and Python best practices.
447: 
448: Task:
449: {task}
450: 
451: Draft:
452: {draft}
453: 
454: Feedback:
455: {feedback}
456: 
457: Output format:
458: {format_instructions}
459: """
460: )
461: 
462: refine_prompt = refine_prompt_template.partial(
463:     format_instructions=refine_parser.get_format_instructions()
464: )
465: 
466: 
467: # ----------------------------------------------------------
468: # 5. Build LCEL Chains
469: # ----------------------------------------------------------
470: 
471: initial_chain = initial_prompt | model | initial_parser
472: feedback_chain = feedback_prompt | model | feedback_parser
473: refine_chain = refine_prompt | model | refine_parser
474: 
475: 
476: # ----------------------------------------------------------
477: # 6. Multi-Iteration Self-Refine Loop (Stop When ‚Äúno issues‚Äù)
478: # ----------------------------------------------------------
479: 
480: task = "Write a Python function calculate_average that takes a list of numbers and returns the average."
481: 
482: MAX_ITER = 3    # upper limit for safety
483: 
484: # Phase 1 ‚Äî Generate initial draft
485: draft_result = initial_chain.invoke({"task": task})
486: current_draft = draft_result.draft
487: 
488: print("\n=== INITIAL DRAFT ===\n")
489: print(current_draft)
490: 
491: # Phase 2 ‚Äî Iterative refine loop
492: for iteration in range(MAX_ITER):
493:     print(f"\n=== FEEDBACK ROUND {iteration} ===\n")
494: 
495:     # Generate feedback
496:     fb_result = feedback_chain.invoke({
497:         "task": task,
498:         "draft": current_draft
499:     })
500: 
501:     feedback = fb_result.feedback
502:     print(feedback)
503: 
504:     # Stop condition: feedback contains "no issues"
505:     if "no issues" in feedback.lower():
506:         print("\nStopping refinement: feedback reports 'no issues'.")
507:         break
508: 
509: 		time.sleep(1)
510:     # Apply refinement
511:     refine_result = refine_chain.invoke({
512:         "task": task,
513:         "draft": current_draft,
514:         "feedback": feedback
515:     })
516: 
517:     current_draft = refine_result.refined_answer
518: 
519:     print(f"\n=== REFINED DRAFT {iteration} ===\n")
520:     print(current_draft)
521: 
522: 
523: print("\n\n=== FINAL OUTPUT AFTER SELF-REFINE ===\n")
524: print(current_draft)
525: ```
526: 
527: Here the output is
528: ```
529: === INITIAL DRAFT ===
530: 
531: def calculate_average(numbers):
532:     if not numbers:
533:         return 0
534:     total = sum(numbers)
535:     count = len(numbers)
536:     return total / count
537: 
538: === FEEDBACK ROUND 0 ===
539: 
540: The provided `calculate_average` function is well-written, efficient, and correctly implements the task. It handles the edge case of an empty list gracefully by returning 0, which is a reasonable convention for an average of an empty set. The use of built-in `sum()` and `len()` functions is Pythonic and efficient. There are no issues.
541: 
542: Stopping refinement: feedback reports 'no issues'.
543: 
544: 
545: === FINAL OUTPUT AFTER SELF-REFINE ===
546: 
547: def calculate_average(numbers):
548:     if not numbers:
549:         return 0
550:     total = sum(numbers)
551:     count = len(numbers)
552:     return total / count
553: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Step_Back_Prompting.md">
  1: # **Step Back Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates. 
  4: 
  5: ## **Overview**
  6: 
  7: Step-Back Prompting is a reasoning technique that improves problem-solving by encouraging the model to *temporarily step back* from the specific question and reflect on the more general principle that governs the solution.
  8: 
  9: Instead of jumping directly into computations, the model is first guided to identify the high-level concept or first-principle law relevant to the task. Once this abstraction is established, the model uses that principle to reason clearly and arrive at the final answer.
 10: 
 11: This two-stage process, *Abstraction ‚Üí Reasoning* helps the model avoid errors caused by focusing too narrowly on surface details. By anchoring the solution to a general principle, Step-Back Prompting improves accuracy, structure, and conceptual grounding.
 12: 
 13: ![Step Back prompting](images/2-step-back-prompt.jpg)
 14: 
 15: Figure from [Step Back prompting](https://arxiv.org/abs/2311.04205) paper.
 16: 
 17: 
 18: ##  **Prompt Template**
 19: 
 20: Here is the step back abstraction prompt template for step back prompting.
 21: 
 22: ```
 23: You are an expert in abstraction.
 24: 
 25: Given the original question below:
 26: 
 27: Original Question:
 28: {question}
 29: 
 30: Perform TWO tasks:
 31: 1. Generate a high-level step-back question that captures the general principle needed.
 32: 2. Answer that step-back question by giving the underlying principle or formula.
 33: ```
 34: 
 35: Here is the final reasoning prompt template for step back prompting.
 36: ```
 37: You are an expert problem solver.
 38: 
 39: Use the abstract principle retrieved earlier to answer the original question.
 40: 
 41: Original Question:
 42: {question}
 43: 
 44: Step-Back Principle:
 45: {abstraction}
 46: 
 47: Now solve the original question step by step.
 48: ```
 49: 
 50: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 51: 
 52: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 53: - ‚ú® Weekly GenAI updates
 54: - üìÑ Weekly LLM, Agents and RAG research paper updates
 55: - üìù 1 fresh blog post on an interesting topic every week
 56: 
 57: 
 58: ## **Implementation**
 59: 
 60: Now let's see the implementation of step back promtping technique using LangChain v1.0
 61: 
 62: ```python
 63: # !pip install langchain langchain-google-genai pydantic
 64: 
 65: import os
 66: from google.colab import userdata
 67: from langchain.chat_models import init_chat_model
 68: from langchain_core.prompts import ChatPromptTemplate
 69: from langchain_core.output_parsers import PydanticOutputParser
 70: from pydantic import BaseModel, Field
 71: 
 72: 
 73: # ----------------------------------------------------------
 74: # 1. Set Gemini API Key
 75: # ----------------------------------------------------------
 76: 
 77: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 78: 
 79: 
 80: # ----------------------------------------------------------
 81: # 2. Define Structured Output Models
 82: # ----------------------------------------------------------
 83: 
 84: class Abstraction(BaseModel):
 85:     stepback_question: str = Field(..., description="The abstract step-back question")
 86:     stepback_answer: str = Field(..., description="The high-level principle that answers the step-back question")
 87: 
 88: 
 89: class FinalAnswer(BaseModel):
 90:     final_answer: str = Field(..., description="The final solution using the abstract principle")
 91: 
 92: 
 93: abstraction_parser = PydanticOutputParser(pydantic_object=Abstraction)
 94: final_answer_parser = PydanticOutputParser(pydantic_object=FinalAnswer)
 95: 
 96: 
 97: # ----------------------------------------------------------
 98: # 3. Initialize Gemini model
 99: # ----------------------------------------------------------
100: 
101: model = init_chat_model(
102:     "gemini-2.5-flash",
103:     model_provider="google_genai",
104:     temperature=0
105: )
106: 
107: 
108: # ----------------------------------------------------------
109: # 4. Prompt Templates (ONLY TWO CALLS)
110: # ----------------------------------------------------------
111: 
112: # --- Call 1: Step-Back Abstraction ---
113: abstraction_prompt_template = ChatPromptTemplate.from_template(
114:     """
115: You are an expert in abstraction.
116: 
117: Given the original question below:
118: 
119: Original Question:
120: {question}
121: 
122: Perform TWO tasks:
123: 1. Generate a high-level **step-back question** that captures the general principle needed.
124: 2. Answer that step-back question by giving the **underlying principle or formula**.
125: 
126: Return BOTH in this JSON format:
127: {format_instructions}
128: """
129: )
130: 
131: abstraction_prompt = abstraction_prompt_template.partial(
132:     format_instructions=abstraction_parser.get_format_instructions()
133: )
134: 
135: 
136: # --- Call 2: Final Reasoning ---
137: final_reasoning_prompt_template = ChatPromptTemplate.from_template(
138:     """
139: You are an expert problem solver.
140: 
141: Use the abstract principle retrieved earlier to answer the original question.
142: 
143: Original Question:
144: {question}
145: 
146: Step-Back Principle:
147: {abstraction}
148: 
149: Now solve the original question step by step.
150: 
151: Return the final answer in this JSON format:
152: {format_instructions}
153: """
154: )
155: 
156: final_reasoning_prompt = final_reasoning_prompt_template.partial(
157:     format_instructions=final_answer_parser.get_format_instructions()
158: )
159: 
160: 
161: # ----------------------------------------------------------
162: # 5. Build LCEL Chains (Only Two Calls)
163: # ----------------------------------------------------------
164: 
165: abstraction_chain = abstraction_prompt | model | abstraction_parser
166: final_answer_chain = final_reasoning_prompt | model | final_answer_parser
167: 
168: 
169: # ----------------------------------------------------------
170: # 6. Run Step-Back Prompting on Your Example
171: # ----------------------------------------------------------
172: 
173: question = "A train travels at 60 miles per hour. How far will it travel in 3 hours?"
174: 
175: 
176: # Call 1 ‚Äî Abstraction
177: abs_result = abstraction_chain.invoke({"question": question})
178: print("\n--- STEP-BACK ABSTRACTION ---\n")
179: print("Step-Back Question:", abs_result.stepback_question)
180: print("Step-Back Answer:", abs_result.stepback_answer)
181: 
182: 
183: # Call 2 ‚Äî Reasoning
184: final_result = final_answer_chain.invoke({
185:     "question": question,
186:     "abstraction": abs_result.stepback_answer
187: })
188: print("\n--- FINAL ANSWER ---\n")
189: print(final_result.final_answer)
190: ```
191: 
192: Here the output is
193: ```
194: --- STEP-BACK ABSTRACTION ---
195: 
196: Step-Back Question: How is the total distance covered related to the constant rate of travel and the duration of that travel?
197: Step-Back Answer: The total distance traveled is calculated by multiplying the constant rate of travel (speed) by the time spent traveling. This relationship is commonly expressed by the formula: Distance = Rate √ó Time (D = R √ó T).
198: 
199: --- FINAL ANSWER ---
200: 
201: To find the distance, we use the formula Distance = Rate √ó Time. Given the rate (speed) is 60 miles per hour and the time is 3 hours, we multiply 60 mph by 3 hours. Distance = 60 miles/hour √ó 3 hours = 180 miles. Therefore, the train will travel 180 miles in 3 hours.
202: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Tabular_Chain_of_Thought_Prompting.md">
  1: # **Tabular Chain of Thought Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Tabular Prompting (Tab-CoT) is a prompting technique where the model is guided to show its reasoning in the form of a *table*, instead of plain step-by-step text.
  8: 
  9: Just like Zero-Shot CoT makes the model ‚Äúthink step by step,‚Äù Zero-Shot Tab-CoT makes the model think in a structured table format with clear columns such as:
 10: 
 11: | step | subquestion | process | result |
 12: 
 13: This table format forces the model to reason in a highly organized and structured way, which often leads to:
 14: 
 15: - More accurate reasoning
 16: - Less confusion in multi-step problems
 17: - Clearer intermediate results
 18: - Better handling of numbers and logic
 19: 
 20: ![Tabular Chain of Thought prompting](images/6-tcot-prompt.jpg)
 21: 
 22: Figure from [Tabular Chain of Thought prompting](https://arxiv.org/abs/2305.17812) paper. 
 23: 
 24: ## **Prompt Template**
 25: 
 26: Here is the prompt template for tabular chain of thoughts prompting.
 27: 
 28: ```
 29: You are a reasoning assistant that uses Tabular Chain-of-Thought (Tab-CoT).
 30: 
 31: You must generate your reasoning in a table format using the header:
 32: 
 33: |step|subquestion|process|result|
 34: 
 35: For every step:
 36: - Fill each column
 37: - Show clean calculations in the "process" column
 38: - Show only the intermediate numeric answer in "result"
 39: 
 40: After generating the full reasoning table, provide the final answer.
 41: 
 42: Question: {question}
 43: ```
 44: 
 45: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 46: 
 47: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 48: - ‚ú® Weekly GenAI updates
 49: - üìÑ Weekly LLM, Agents and RAG research paper updates
 50: - üìù 1 fresh blog post on an interesting topic every week
 51: 
 52: ## **Implementation**
 53: 
 54: Now let's see the implementation of tabular chain of thoughts promtping technique using LangChain v1.0
 55: 
 56: ```python
 57: # !pip install langchain langchain-google-genai pydantic
 58: 
 59: import os
 60: from google.colab import userdata
 61: from langchain.chat_models import init_chat_model
 62: from langchain_core.prompts import ChatPromptTemplate
 63: from langchain_core.output_parsers import PydanticOutputParser
 64: from pydantic import BaseModel, Field
 65: 
 66: # --------------------------------------------------------
 67: # 1. Set your API Key
 68: # --------------------------------------------------------
 69: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 70: 
 71: # --------------------------------------------------------
 72: # 2. Define the Pydantic schema for structured output
 73: # --------------------------------------------------------
 74: class TabCoTResponse(BaseModel):
 75:     reasoning_table: str = Field(..., description="Generated Tabular Chain-of-Thought reasoning table")
 76:     answer: str = Field(..., description="Final numeric answer only")
 77: 
 78: # --------------------------------------------------------
 79: # 3. Create the parser
 80: # --------------------------------------------------------
 81: parser = PydanticOutputParser(pydantic_object=TabCoTResponse)
 82: 
 83: # --------------------------------------------------------
 84: # 4. Initialize the model (Gemini-2.5-flash)
 85: # --------------------------------------------------------
 86: model = init_chat_model(
 87:     "gemini-2.5-flash",
 88:     model_provider="google_genai",
 89:     temperature=0
 90: )
 91: 
 92: # --------------------------------------------------------
 93: # 5. Prompt Template for Zero-Shot Tabular CoT
 94: # --------------------------------------------------------
 95: prompt_template = ChatPromptTemplate.from_template(
 96:     """
 97: You are a reasoning assistant that uses **Tabular Chain-of-Thought (Tab-CoT)**.
 98: 
 99: You must generate your reasoning in a table format using the header:
100: 
101: |step|subquestion|process|result|
102: 
103: For every step:
104: - Fill each column
105: - Show clean calculations in the "process" column
106: - Show only the intermediate numeric answer in "result"
107: 
108: After generating the full reasoning table, provide the final answer.
109: 
110: Question: {question}
111: 
112: Provide the output in the following JSON format:
113: {format_instructions}
114: """
115: )
116: 
117: # Insert parser format instructions
118: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
119: 
120: # --------------------------------------------------------
121: # 6. Build the LCEL chain
122: # --------------------------------------------------------
123: chain = prompt | model | parser
124: 
125: # --------------------------------------------------------
126: # 7. Example problem (YOUR GIVEN EXAMPLE)
127: # --------------------------------------------------------
128: question = (
129:     "A librarian is shelving books. A shelf for fiction novels can hold 15 books, "
130:     "and a shelf for non-fiction can hold 12 books. If the library needs to shelve "
131:     "90 fiction novels and 72 non-fiction books, how many total shelves will the librarian need?"
132: )
133: 
134: # --------------------------------------------------------
135: # 8. Invoke the chain
136: # --------------------------------------------------------
137: result = chain.invoke({"question": question})
138: 
139: # --------------------------------------------------------
140: # 9. Display results
141: # --------------------------------------------------------
142: print("\n--- Tabular Reasoning Table ---\n")
143: print(result.reasoning_table)
144: 
145: print("\n--- Final Answer ---\n")
146: print(result.answer)
147: ```
148: 
149: Here the output is
150: ```
151: --- Tabular Reasoning Table ---
152: 
153: |step|subquestion|process|result|
154: |---|---|---|---|
155: |1|How many shelves are needed for fiction novels?|90 novels / 15 novels/shelf|6|
156: |2|How many shelves are needed for non-fiction books?|72 books / 12 books/shelf|6|
157: |3|What is the total number of shelves needed?|6 (fiction shelves) + 6 (non-fiction shelves)|12|
158: 
159: --- Final Answer ---
160: 
161: 12
162: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Thread_of_Thoughts_Prompting.md">
  1: # **Thread of Thoughts Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Thread-of-Thoughts (ThoT) prompting is a technique designed to help LLMs handle chaotic or cluttered contexts ‚Äî especially when the input contains many irrelevant passages, mixed information, or retrieved evidence scattered across locations. ThoT prompting is triggered using the phrase, *‚ÄúWalk me through this context in manageable parts step by step, summarizing and analyzing as we go.‚Äù*
  8: 
  9: While Chain-of-Thought (CoT) helps reasoning by ‚Äúthinking step by step,‚Äù ThoT prompting goes further:
 10: 
 11: - ThoT breaks long/chaotic context into manageable parts.
 12: 
 13: - Summarizes each part.
 14: 
 15: - Identifies the relevant pieces.
 16: 
 17: - Then synthesizes the final answer.
 18: 
 19: It is extremely useful in *retrieval-augmented generation*, *multi-turn dialogue*, or *any situation* where a lot of irrelevant text is mixed with relevant information.
 20: 
 21: ![Thread of Thoughts prompting](images/5-tot-prompt.jpg)
 22: 
 23: Figure from [Thread of Thoughts prompting](https://arxiv.org/abs/2311.08734) paper. 
 24: 
 25: ## **Prompt Template**
 26: 
 27: Here is the prompt template for thread of thoughts prompting.
 28: 
 29: ```
 30: You are an assistant that performs Thread-of-Thoughts reasoning:
 31: 
 32: Context: 
 33: {retrieved_passages}
 34: 
 35: Question: {question}
 36: 
 37: Trigger for Thread-of-Thoughts:
 38: Walk me through this context in manageable parts step by step, summarizing and analyzing as we go.
 39: ```
 40: 
 41: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 42: 
 43: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 44: - ‚ú® Weekly GenAI updates
 45: - üìÑ Weekly LLM, Agents and RAG research paper updates
 46: - üìù 1 fresh blog post on an interesting topic every week
 47: 
 48: ## **Implementation**
 49: 
 50: Now let's see the implementation of thread of thoughts promtping technique using LangChain v1.0
 51: 
 52: ```python
 53: # !pip install langchain langchain-google-genai pydantic
 54: 
 55: import os
 56: from google.colab import userdata
 57: from langchain.chat_models import init_chat_model
 58: from langchain_core.prompts import ChatPromptTemplate
 59: from langchain_core.output_parsers import PydanticOutputParser
 60: from pydantic import BaseModel, Field
 61: 
 62: # 1. Set your API key
 63: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
 64: 
 65: # 2. Define a Pydantic schema for structured ThoT output
 66: class ThoTResponse(BaseModel):
 67:     thread_of_thought: str = Field(..., description="Segment-by-segment analysis with summaries")
 68:     answer: str = Field(..., description="Final answer extracted after analysis")
 69: 
 70: # 3. Create parser for the structured output
 71: parser = PydanticOutputParser(pydantic_object=ThoTResponse)
 72: 
 73: # 4. Initialize the LLM (gemini-2.5-flash)
 74: model = init_chat_model(
 75:     "gemini-2.5-flash",
 76:     model_provider="google_genai",
 77:     temperature=0
 78: )
 79: 
 80: # 5. Thread-of-Thoughts prompt template (using your example)
 81: prompt_template = ChatPromptTemplate.from_template(
 82:     """
 83: You are an assistant that performs Thread-of-Thoughts reasoning:
 84: 
 85: Context: 
 86: {retrieved_passages}
 87: 
 88: Question: {question}
 89: 
 90: Trigger for Thread-of-Thoughts:
 91: Walk me through this context in manageable parts step by step, summarizing and analyzing as we go.
 92: 
 93: Provide the output using this JSON format:
 94: {format_instructions}
 95: """
 96: )
 97: 
 98: # 6. Inject format instructions into prompt
 99: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
100: 
101: # 7. LCEL chain: prompt ‚Üí model ‚Üí parser
102: chain = prompt | model | parser
103: 
104: # 8. Example data (your provided retrieval example)
105: retrieved_passages = """
106: Passage 1: Talks about book vending machines.
107: Passage 2: Reclam's founder created the publishing house in Leipzig.
108: Passage 3: Mentions a random street address.
109: Passage 4: Reclam's publishing house was located in Leipzig.
110: Passage 5: Talks about another unrelated company.
111: """
112: 
113: question = "Where was Reclam founded?"
114: 
115: # 9. Invoke the chain
116: result = chain.invoke({
117:     "retrieved_passages": retrieved_passages,
118:     "question": question
119: })
120: 
121: # 10. Display the result
122: print("\n--- Thread of Thoughts ---\n", result.thread_of_thought)
123: print("\n--- Final Answer ---\n", result.answer)
124: ```
125: Here the output is
126: ```
127: --- Thread of Thoughts ---
128:  Let's break down the provided passages to find the answer to where Reclam was founded.
129: 
130: *   **Passage 1 Summary:** This passage discusses book vending machines.
131: *   **Passage 1 Analysis:** This passage does not contain any information relevant to Reclam's founding location.
132: 
133: *   **Passage 2 Summary:** This passage states that Reclam's founder created the publishing house in Leipzig.
134: *   **Passage 2 Analysis:** This passage directly answers the question, indicating that Reclam was founded in Leipzig.
135: 
136: *   **Passage 3 Summary:** This passage mentions a random street address.
137: *   **Passage 3 Analysis:** This passage is irrelevant to the question about Reclam's founding location.
138: 
139: *   **Passage 4 Summary:** This passage states that Reclam's publishing house was located in Leipzig.
140: *   **Passage 4 Analysis:** This passage corroborates the information from Passage 2, confirming Leipzig as the location of Reclam's publishing house.
141: 
142: *   **Passage 5 Summary:** This passage talks about another unrelated company.
143: *   **Passage 5 Analysis:** This passage is irrelevant to the question.
144: 
145: **Conclusion:** Based on Passage 2, which explicitly states that Reclam's founder created the publishing house in Leipzig, and Passage 4, which confirms its location in Leipzig, the founding location is clearly identified.
146: 
147: --- Final Answer ---
148:  Leipzig
149: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Universal_Self_Consistency_Prompting.md">
  1: # **Universal Self Consistency Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Self-consistency prompting is a technique in which a large language model (LLM) is asked to generate multiple reasoning chains (via chain-of-thought prompting) for the same input, and then the final answer is chosen by majority vote (i.e., the answer that appears most frequently across the sampled outputs). The idea is that if many independent reasoning paths converge on the same answer, that answer is more likely to be correct.
  8: 
  9: 
 10: Universal self-consistency prompting builds on self-consistency but removes its main limitation (that the final answer must be in a form that supports majority/exact-match voting). In USC, one again samples multiple outputs from the LLM, but then instead of simply voting on the same answer string, the LLM is prompted to select (or rank) among the candidate outputs which is the ‚Äúmost consistent‚Äù with the set of responses (or best according to some consistency criterion). This allows it to be applied to free-form generation tasks (summarization, open-ended Q&A, code generation) where answers are not identical strings and majority voting fails.
 11: 
 12: 
 13: ![Universal Self Consistency prompting](images/2-universal-self-prompt.jpg)
 14: 
 15: Figure from [Universal Self Consistency prompting](https://arxiv.org/abs/2311.17311) paper. 
 16: 
 17: ## **Prompt Template**
 18: 
 19: Here is the generation prompt template for universal self consistency prompting.
 20: 
 21: ```
 22: You are a detailed step-by-step reasoning assistant.
 23: 
 24: Question: {question}
 25: 
 26: Instruction:
 27: - Think step by step.
 28: - Produce a clear chain of thought.
 29: - Then produce ONLY the final numeric answer.
 30: ```
 31: Here is the selection prompt template for universal self consistency prompting.
 32: 
 33: ```
 34: You are an evaluator assistant. You are given multiple candidate answers to the same question.
 35: Your job is to read ALL responses and select the one that is the most consistent, reasonable, and logically sound.
 36: 
 37: Question:
 38: {question}
 39: 
 40: Candidate Responses:
 41: {all_responses}
 42: 
 43: Instruction:
 44: - Carefully compare the reasoning steps.
 45: - Select the single best response.
 46: - Provide ONLY the index number of the best response.
 47: - DO NOT explain your choice.
 48: 
 49: Return output in plain text containing ONLY the index number (1, 2, or 3)
 50: ```
 51: 
 52: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 53: 
 54: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 55: - ‚ú® Weekly GenAI updates
 56: - üìÑ Weekly LLM, Agents and RAG research paper updates
 57: - üìù 1 fresh blog post on an interesting topic every week
 58: 
 59: ## **Zero-Shot Implementation**
 60: 
 61: Now let's see the implementation of zero-shot universal self consistency promtping technique using LangChain v1.0
 62: 
 63: ```python
 64: # ---------------------------------------------------------
 65: # Zero-Shot Universal Self-Consistency Prompting (USC)
 66: # ---------------------------------------------------------
 67: 
 68: # pip install langchain langchain-google-genai pydantic
 69: 
 70: import os
 71: import time
 72: from google.colab import userdata
 73: from langchain.chat_models import init_chat_model
 74: from langchain_core.prompts import ChatPromptTemplate
 75: from langchain_core.output_parsers import PydanticOutputParser
 76: from pydantic import BaseModel, Field
 77: 
 78: # ---------------------------------------------------------
 79: # 1. Set your Gemini API key
 80: # ---------------------------------------------------------
 81: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
 82: 
 83: 
 84: # ---------------------------------------------------------
 85: # 2. Define structured output model for candidate responses
 86: # ---------------------------------------------------------
 87: class USCResponse(BaseModel):
 88:     reasoning_chain: str = Field(..., description="Full reasoning steps")
 89:     answer: str = Field(..., description="Final numeric answer only")
 90: 
 91: 
 92: parser = PydanticOutputParser(pydantic_object=USCResponse)
 93: 
 94: 
 95: # ---------------------------------------------------------
 96: # 3. Initialize Gemini model with sampling enabled
 97: # ---------------------------------------------------------
 98: model = init_chat_model(
 99:     "gemini-2.5-flash",
100:     model_provider="google_genai",
101:     temperature=0.8,
102:     top_k=40,
103: )
104: 
105: 
106: # ---------------------------------------------------------
107: # 4. Zero-shot generation prompt (same as SC sampling stage)
108: # ---------------------------------------------------------
109: generation_prompt_template = ChatPromptTemplate.from_template(
110:     """
111: You are a detailed step-by-step reasoning assistant.
112: 
113: Question: {question}
114: 
115: Instruction:
116: - Think step by step.
117: - Produce a clear chain of thought.
118: - Then produce ONLY the final numeric answer.
119: 
120: Return output in this JSON format:
121: {format_instructions}
122: """
123: )
124: 
125: generation_prompt = generation_prompt_template.partial(
126:     format_instructions=parser.get_format_instructions()
127: )
128: 
129: gen_chain = generation_prompt | model | parser
130: 
131: 
132: # ---------------------------------------------------------
133: # 5. Universal Self-Consistency Selection Prompt
134: # ---------------------------------------------------------
135: selection_prompt = ChatPromptTemplate.from_template(
136:     """
137: You are an evaluator assistant.
138: 
139: You are given multiple candidate answers to the same question.
140: Your job is to read ALL responses and select the one that is
141: the most consistent, reasonable, and logically sound.
142: 
143: Question:
144: {question}
145: 
146: Candidate Responses:
147: {all_responses}
148: 
149: Instruction:
150: - Carefully compare the reasoning steps.
151: - Select the single best response.
152: - Provide ONLY the index number of the best response.
153: - DO NOT explain your choice.
154: 
155: Return output in plain text containing ONLY the index number (1, 2, or 3).
156: """
157: )
158: 
159: selection_chain = selection_prompt | model
160: 
161: 
162: # ---------------------------------------------------------
163: # 6. Universal Self-Consistency function
164: # ---------------------------------------------------------
165: def universal_self_consistency(question: str, n_samples: int = 3):
166:     candidates = []
167: 
168:     # --- Stage 1: Generate candidate responses ---
169:     for i in range(n_samples):
170:         result = gen_chain.invoke({"question": question})
171:         candidates.append(result)
172:         time.sleep(1)
173: 
174:     # Prepare text block for evaluation prompt
175:     formatted_candidates = ""
176:     for idx, c in enumerate(candidates, 1):
177:         formatted_candidates += (
178:             f"\n[{idx}] Reasoning:\n{c.reasoning_chain}\nAnswer: {c.answer}\n"
179:         )
180: 
181:     # --- Stage 2: Ask LLM to select best candidate ---
182:     chosen_idx = selection_chain.invoke(
183:         {
184:             "question": question,
185:             "all_responses": formatted_candidates,
186:         }
187:     )
188: 
189:     chosen_idx = int(chosen_idx.content.strip())
190: 
191:     return candidates[chosen_idx - 1], candidates
192: 
193: 
194: # ---------------------------------------------------------
195: # 7. Run Universal Self-Consistency on the example
196: # ---------------------------------------------------------
197: question = (
198:     "What are three advantages of electric vehicles over gasoline vehicles?"
199: )
200: 
201: best_output, all_candidates = universal_self_consistency(question, n_samples=3)
202: 
203: 
204: # ---------------------------------------------------------
205: # 8. Display results
206: # ---------------------------------------------------------
207: print("\n===== UNIVERSAL SELF CONSISTENCY OUTPUT =====")
208: print("Chosen Final Answer:", best_output.answer)
209: 
210: print("\n===== ALL GENERATED CANDIDATES =====")
211: for i, out in enumerate(all_candidates, 1):
212:     print(f"\n--- Candidate {i} ---")
213:     print(out.reasoning_chain)
214:     print("Answer:", out.answer)
215: ```
216: 
217: Here the output is
218: ```
219: 
220: ===== UNIVERSAL SELF CONSISTENCY OUTPUT =====
221: Chosen Final Answer: 1. Zero tailpipe emissions, contributing to cleaner air and reduced greenhouse gases. 
222: 2. Lower running costs due to cheaper 'fuel' (electricity) and significantly reduced maintenance requirements. 
223: 3. Superior driving experience with instant torque for quick acceleration and quieter, smoother operation.
224: 
225: ===== ALL GENERATED CANDIDATES =====
226: 
227: --- Candidate 1 ---
228: The user asked for three advantages of electric vehicles (EVs) over gasoline vehicles. I brainstormed several potential advantages, including environmental benefits, lower running costs, performance, and maintenance. From these, I selected three distinct and significant advantages:
229: 
230: 1.  **Environmental Impact:** EVs produce zero tailpipe emissions, which significantly reduces local air pollution and greenhouse gas emissions compared to gasoline vehicles.
231: 2.  **Lower Running Costs:** EVs generally have lower 'fuel' costs (electricity can be cheaper per mile than gasoline) and significantly reduced maintenance needs due to fewer moving parts (no oil changes, spark plugs, complex transmissions, etc.).
232: 3.  **Performance and Driving Experience:** EVs offer instant torque, leading to quicker acceleration, and operate much more quietly and smoothly than gasoline cars, providing a superior driving experience.
233: 
234: Answer: 1. Zero tailpipe emissions, contributing to cleaner air and reduced greenhouse gases. 2. Lower running costs due to cheaper 'fuel' (electricity) and significantly reduced maintenance requirements. 3. Superior driving experience with instant torque for quick acceleration and quieter, smoother operation.
235: 
236: --- Candidate 2 ---
237: The user asked for three advantages of electric vehicles over gasoline vehicles. I will identify three distinct benefits:
238: 1.  **Environmental Impact:** Electric vehicles produce zero tailpipe emissions, leading to cleaner air, especially in urban areas, and a reduction in smog-forming pollutants. While electricity generation might have emissions, the vehicle itself is clean.
239: 2.  **Lower Running Costs:** Electricity is generally cheaper per mile than gasoline. Additionally, EVs have fewer moving parts than internal combustion engine vehicles, leading to lower maintenance requirements (no oil changes, spark plugs, fuel filters, etc.).
240: 3.  **Performance and Driving Experience:** Electric motors provide instant torque, resulting in rapid and smooth acceleration. EVs are also significantly quieter than gasoline cars, offering a more serene driving experience.
241: 
242: Answer: 1. Zero tailpipe emissions
243: 2. Lower running costs (cheaper fuel and less maintenance)
244: 3. Instant torque and quieter operation
245: 
246: --- Candidate 3 ---
247: The user is asking for three advantages of electric vehicles (EVs) over gasoline vehicles. I need to identify three distinct and significant benefits. I will consider economic, environmental, and performance aspects.
248: 
249: 1.  **Environmental Benefits**: EVs produce zero tailpipe emissions, which significantly reduces local air pollution (smog, particulate matter) compared to gasoline vehicles. This is a major advantage for public health and environmental quality, especially in urban areas.
250: 2.  **Lower Running Costs**: EVs typically have lower 'fuel' costs (electricity vs. gasoline) per mile, especially when charging at home. Additionally, EVs have fewer moving parts than gasoline engines (no oil changes, spark plugs, complex transmissions, etc.), which generally leads to lower maintenance costs over the vehicle's lifespan.
251: 3.  **Enhanced Driving Experience**: EVs offer instant torque from a standstill, resulting in quicker acceleration and a more responsive driving feel. They are also significantly quieter than gasoline vehicles due to the absence of an internal combustion engine, contributing to a smoother and more peaceful ride.
252: 
253: These three points cover environmental, economic, and performance advantages, which are key differentiating factors.
254: 
255: Answer: 1. Lower running costs (fuel and maintenance).
256: 2. Environmental benefits (zero tailpipe emissions).
257: 3. Enhanced driving experience (instant torque, quieter operation).
258: ```
259: 
260: 
261: ## **Few-Shot Implementation**
262: 
263: Now let's see the implementation of few-shot universal self consistency promtping technique using LangChain v1.0
264: 
265: ```python
266: # ---------------------------------------------------------
267: # Few-Shot Universal Self-Consistency Prompting (USC)
268: # ---------------------------------------------------------
269: 
270: # pip install langchain langchain-google-genai pydantic
271: 
272: import os
273: import time
274: from pydantic import BaseModel, Field
275: from google.colab import userdata
276: from langchain.chat_models import init_chat_model
277: from langchain_core.prompts import ChatPromptTemplate
278: from langchain_core.output_parsers import PydanticOutputParser
279: 
280: 
281: # ---------------------------------------------------------
282: # 1. Set Gemini API key
283: # ---------------------------------------------------------
284: os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
285: 
286: 
287: # ---------------------------------------------------------
288: # 2. Define structured output schema
289: # ---------------------------------------------------------
290: class USCResponse(BaseModel):
291:     reasoning_chain: str = Field(..., description="Full chain-of-thought reasoning")
292:     answer: str = Field(..., description="Final concise answer")
293: 
294: 
295: parser = PydanticOutputParser(pydantic_object=USCResponse)
296: 
297: 
298: # ---------------------------------------------------------
299: # 3. Initialize Gemini model with sampling enabled
300: # ---------------------------------------------------------
301: model = init_chat_model(
302:     "gemini-2.5-flash",
303:     model_provider="google_genai",
304:     temperature=0.8,
305:     top_k=40,
306: )
307: 
308: 
309: # ---------------------------------------------------------
310: # 4. Few-shot example (replaced with EV example)
311: # ---------------------------------------------------------
312: few_shot_example = """
313: Example Problem:
314: What are three advantages of electric vehicles over gasoline vehicles?
315: 
316: Example Chain-of-Thought:
317: Electric vehicles (EVs) offer several benefits compared to gasoline vehicles. 
318: First, EVs have lower operating costs because electricity is cheaper than gasoline. 
319: Second, they produce zero tailpipe emissions, which helps reduce air pollution. 
320: Third, EVs have fewer moving parts, which reduces maintenance requirements.
321: 
322: Example Final Answer:
323: Lower operating cost; zero tailpipe emissions; fewer moving parts.
324: """
325: 
326: 
327: # ---------------------------------------------------------
328: # 5. Few-shot generation prompt
329: # ---------------------------------------------------------
330: generation_prompt_template = ChatPromptTemplate.from_template(
331:     """
332: You are a detailed step-by-step reasoning assistant.
333: 
334: Below is a worked example:
335: {few_shot_example}
336: 
337: Now use the same style of reasoning to answer the new question.
338: 
339: New Question:
340: {question}
341: 
342: Instructions:
343: - Provide a full chain-of-thought reasoning.
344: - Then give a concise final answer summarizing the key rules.
345: - Respond in this JSON format:
346: {format_instructions}
347: """
348: )
349: 
350: generation_prompt = generation_prompt_template.partial(
351:     few_shot_example=few_shot_example,
352:     format_instructions=parser.get_format_instructions()
353: )
354: 
355: gen_chain = generation_prompt | model | parser
356: 
357: 
358: # ---------------------------------------------------------
359: # 6. Universal Self-Consistency Selection Prompt
360: # ---------------------------------------------------------
361: selection_prompt = ChatPromptTemplate.from_template(
362:     """
363: You are an evaluator assistant.
364: 
365: You are given multiple candidate responses to the same question.
366: Your task is to read ALL the responses and select the one that is
367: the most consistent, complete, and logically sound.
368: 
369: Question:
370: {question}
371: 
372: Candidate Responses:
373: {all_responses}
374: 
375: Instructions:
376: - Compare the reasoning across responses.
377: - Choose the single best response.
378: - Return ONLY the index number (1, 2, or 3).
379: - Do NOT explain your choice.
380: 
381: Return output in plain text containing ONLY the index number.
382: """
383: )
384: 
385: selection_chain = selection_prompt | model
386: 
387: 
388: # ---------------------------------------------------------
389: # 7. Universal Self-Consistency function (n_samples=3)
390: # ---------------------------------------------------------
391: def universal_self_consistency(question: str, n_samples: int = 3):
392:     candidates = []
393: 
394:     # --- Stage 1: Generate candidate answers ---
395:     for _ in range(n_samples):
396:         out = gen_chain.invoke({"question": question})
397:         candidates.append(out)
398:         time.sleep(1)
399: 
400:     # Create formatted text block for evaluation
401:     formatted = ""
402:     for idx, c in enumerate(candidates, 1):
403:         formatted += f"\n[{idx}] Reasoning:\n{c.reasoning_chain}\nAnswer: {c.answer}\n"
404: 
405:     # --- Stage 2: LLM selects best answer ---
406:     chosen_idx = selection_chain.invoke(
407:         {"question": question, "all_responses": formatted}
408:     )
409:     chosen_idx = int(chosen_idx.content.strip())
410: 
411:     return candidates[chosen_idx - 1], candidates
412: 
413: 
414: # ---------------------------------------------------------
415: # 8. Run Few-shot Universal Self-Consistency
416: # ---------------------------------------------------------
417: question = "What are the most important rules for creating a strong password?"
418: 
419: best_output, all_outputs = universal_self_consistency(question, n_samples=3)
420: 
421: 
422: # ---------------------------------------------------------
423: # 9. Display results
424: # ---------------------------------------------------------
425: print("\n===== FINAL CHOSEN ANSWER =====")
426: print(best_output.answer)
427: 
428: print("\n===== ALL GENERATED CANDIDATES =====")
429: for i, out in enumerate(all_outputs, 1):
430:     print(f"\n--- Candidate {i} ---")
431:     print(out.reasoning_chain)
432:     print("Answer:", out.answer)
433: 
434: ```
435: 
436: Here the output is
437: ```
438: 
439: ===== FINAL CHOSEN ANSWER =====
440: Length (12+ characters); Mix of character types (uppercase, lowercase, numbers, symbols); Avoid easily guessable information; Unique for each account.
441: 
442: ===== ALL GENERATED CANDIDATES =====
443: 
444: --- Candidate 1 ---
445: Creating a strong password is crucial for online security. The most important rules revolve around making it difficult for others to guess or for automated tools to crack. 
446: First, the password should be sufficiently long, ideally 12 characters or more, as longer passwords significantly increase the number of possible combinations and thus the time it takes to crack them. 
447: Second, it must incorporate a variety of character types, including a mix of uppercase letters, lowercase letters, numbers, and special symbols (e.g., !, @, #, $). This diversity prevents simple dictionary attacks and brute-force attempts that target specific character sets. 
448: Third, it is vital to avoid using easily guessable information such as personal details (birthdays, names, pet names), sequential patterns (e.g., '123456', 'qwerty'), or common dictionary words, as these are frequently targeted. 
449: Fourth, each password should be unique for every account. Reusing passwords means that if one account is compromised, all other accounts using the same password become vulnerable. 
450: Finally, consider using a password manager to generate and store complex, unique passwords, as this helps enforce all these rules consistently.
451: 
452: Answer: Use a long password (12+ characters); include a mix of uppercase, lowercase, numbers, and symbols; avoid personal information and common words; use unique passwords for each account.
453: 
454: --- Candidate 2 ---
455: Creating a strong password is a fundamental aspect of digital security. First, the most crucial rule is to make the password long; experts generally recommend a minimum of 12-16 characters, as length significantly increases the computational effort required to crack it. Second, a strong password must incorporate a variety of character types, including uppercase letters, lowercase letters, numbers, and special symbols, which adds to its complexity and unpredictability. Third, it is vital to use a unique password for every different online account; reusing passwords means that if one service is breached, all other accounts using the same password become vulnerable. Fourth, users should avoid easily guessable information, such as personal details (like names, birthdays, or pet names), common words found in dictionaries, or simple sequential patterns (like '123456' or 'qwerty'). Finally, consider using a password manager to generate and store truly random and complex passwords, ensuring they are both strong and unique without needing to be memorized.
456: 
457: Answer: Use a long password (12+ characters); include a mix of uppercase, lowercase, numbers, and symbols; ensure it's unique for each account; and avoid personal information, common words, or simple patterns.
458: 
459: --- Candidate 3 ---
460: Creating a strong password is essential for digital security. First, the most crucial aspect is **length**. A strong password should be long, ideally 12 characters or more, as this significantly increases the number of possible combinations and makes it much harder for attackers to crack through brute-force methods. Second, it's vital to incorporate a **mix of character types**. This means using a combination of uppercase letters, lowercase letters, numbers, and special symbols (like !, @, #, $, %, etc.). This variety adds complexity and prevents simple dictionary or pattern-based attacks. Third, avoid using **easily guessable information**. This includes personal details (like names, birthdays, pet names), common words, sequential numbers (12345), or simple keyboard patterns (qwerty). Such passwords are often the first targets for attackers. Finally, ensure the password is **unique** for each account. Reusing passwords means that if one account is compromised, all other accounts using the same password become vulnerable.
461: 
462: Answer: Length (12+ characters); Mix of character types (uppercase, lowercase, numbers, symbols); Avoid easily guessable information; Unique for each account.
463: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Advanced_Prompt_Engineering_Techniques/Zero_Shot_CoT_Prompting.md">
  1: # **Zero Shot Chain of Thought Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: Zero-shot Chain-of-Thought (CoT) prompting is a technique where you instruct an LLM to think step-by-step before generating the final answer.  
  7: 
  8: Here, ‚Äúzero-shot‚Äù means the model gets no examples from you, and ‚Äúchain-of-thought‚Äù means the model shows its reasoning steps before giving the final answer. 
  9: 
 10: ![zero shot cot prompting](images/1-zs-cot-prompt.jpg)
 11: 
 12: Figure from [zero shot CoT prompting ](https://arxiv.org/abs/2205.11916) paper. 
 13: 
 14: ## **Prompt Temtplate**
 15: 
 16: Here is the prompt template for zero shot CoT prompting.
 17: 
 18: ```
 19: You are a step-by-step reasoning assistant.
 20: 
 21: Question: {question}
 22: 
 23: Answer: Let's think step by step.
 24: ```
 25: 
 26: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 27: 
 28: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 29: - ‚ú® Weekly GenAI updates
 30: - üìÑ Weekly LLM, Agents and RAG research paper updates
 31: - üìù 1 fresh blog post on an interesting topic every week
 32: 
 33: ## **Implementation**
 34: 
 35: Now let's see the implementation of zero shot CoT prompting using LangChain v1.0 library.
 36: 
 37: ```python
 38: !pip install langchain langchain-google-genai pydantic
 39: 
 40: import os
 41: from google.colab import userdata
 42: from langchain.chat_models import init_chat_model
 43: from langchain_core.prompts import ChatPromptTemplate
 44: from langchain_core.output_parsers import PydanticOutputParser
 45: from pydantic import BaseModel, Field
 46: 
 47: # 1. Set your API key
 48: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
 49: 
 50: # 2. Define the Pydantic schema for structured output
 51: class CoTResponse(BaseModel):
 52:     reasoning_chain: str = Field(..., description="Step-by-step reasoning")
 53:     answer: str = Field(..., description="Final numeric answer only")
 54: 
 55: # 3. Create the parser from the Pydantic model
 56: parser = PydanticOutputParser(pydantic_object=CoTResponse)
 57: 
 58: # 4. Initialize the chat model (gpt-4o-mini)
 59: model = init_chat_model(
 60:     "gemini-2.5-flash",
 61:     model_provider = "google_genai",
 62:     temperature=0
 63: )
 64: 
 65: # 5. Prompt template with explicit zero-shot CoT cue ("Let's think step by step.")
 66: prompt_template = ChatPromptTemplate.from_template(
 67:     """
 68: You are a step-by-step reasoning assistant.
 69: 
 70: Question: {question}
 71: 
 72: Answer: Let's think step by step.
 73: 
 74: Provide your solution in the following JSON format:
 75: {format_instructions}
 76: 
 77: """
 78: )
 79: 
 80: # 6. Inject the parser's format instructions into the template
 81: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
 82: 
 83: # 7. Build the LCEL chain (prompt ‚Üí model ‚Üí parser)
 84: chain = prompt | model | parser
 85: 
 86: # 8. Example question and invocation
 87: question = "A baker made 24 cookies. Half are chocolate chip. Half of those have sprinkles. How many chocolate-chip cookies with sprinkles?"
 88: 
 89: result = chain.invoke({"question": question})
 90: 
 91: # 9. Display the result
 92: print("\n--- Reasoning Chain ---\n", result.reasoning_chain)
 93: print("\n--- Final Answer ---\n", result.answer)
 94: 
 95: ```
 96: The output for the above code is
 97: 
 98: ```
 99: --- Reasoning Chain ---
100: 1. The baker made a total of 24 cookies.
101: 2. Half of these cookies are chocolate chip. So, we calculate 24 / 2 = 12 chocolate chip cookies.
102: 3. Half of the chocolate chip cookies have sprinkles. So, we calculate 12 / 2 = 6 chocolate chip cookies with sprinkles.
103: 
104: --- Final Answer ---
105:  6
106: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Basic_Prompt_Engineering_Techniques/Batch_Prompting.md">
  1: # **Batch Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Batch prompting is a prompting technique for large language models which involves giving the model multiple inputs (e.g., several questions or tasks) in a single prompt, instead of prompting once per input. The model generates all corresponding outputs in one go,  instead of generating output once per each input. It‚Äôs useful when you have many inputs to process (e.g., many reviews to classify, many sentences to translate, many questions to answer). By batching them, you reduce the number of separate inference calls needed, which cuts down token usage and inference time.
  8: 
  9: Batch prompting is like a teacher giving a student a whole worksheet with 10 questions at once (instead of one question at a time), the students solves all 10 questions at once.
 10: 
 11: 
 12: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 13: 
 14: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 15: - ‚ú® Weekly GenAI updates
 16: - üìÑ Weekly LLM, Agents and RAG research paper updates
 17: - üìù 1 fresh blog post on an interesting topic every week
 18: 
 19: ## **Implementation (News headlines classification)**
 20: 
 21: Here is the implementation of batch prompting for key phrases extraction.
 22: 
 23: ```python
 24: # !pip install langchain langchain-google-genai pydantic
 25: 
 26: import os
 27: from google.colab import userdata
 28: from langchain.chat_models import init_chat_model
 29: from langchain_core.prompts import ChatPromptTemplate
 30: from langchain_core.output_parsers import PydanticOutputParser
 31: from pydantic import BaseModel, Field
 32: from typing import List
 33: 
 34: # 1. Set your API key
 35: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
 36: 
 37: # 2. Define the Pydantic schema for structured output
 38: class BatchClassifyResponse(BaseModel):
 39:     predictions: List[str] = Field(..., description="Predicted labels for each headline")
 40: 
 41: # 3. Create the parser
 42: parser = PydanticOutputParser(pydantic_object=BatchClassifyResponse)
 43: 
 44: # 4. Initialize the chat model
 45: model = init_chat_model(
 46:     "gemini-2.5-flash",
 47:     model_provider="google_genai",
 48:     temperature=0
 49: )
 50: 
 51: # 5. Batch prompting template
 52: prompt_template = ChatPromptTemplate.from_template(
 53:     """
 54: You will classify multiple news headlines into one of the categories:
 55: ["Politics", "Sports", "Business", "Technology", "Entertainment", "Health", "World"]
 56: 
 57: Headlines:
 58: {headlines}
 59: 
 60: Return the predictions in order, inside this JSON format:
 61: {format_instructions}
 62: """
 63: )
 64: 
 65: # 6. Inject parser instructions
 66: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
 67: 
 68: # 7. Chain: prompt ‚Üí model ‚Üí parser
 69: chain = prompt | model | parser
 70: 
 71: # 8. Example headlines (batch)
 72: headlines = [
 73:     "Government approves new policy to boost semiconductor manufacturing.",
 74:     "Star striker leads team to victory in championship final.",
 75:     "New study reveals long-term effects of poor sleep on health."
 76: ]
 77: 
 78: # 9. Invoke batch prediction
 79: result = chain.invoke({"headlines": headlines})
 80: 
 81: # 10. Display results
 82: print("\n--- Batch Predictions ---")
 83: for i, label in enumerate(result.predictions):
 84:     print(f"{i+1}. {label}")
 85: ```
 86: Here the output is
 87: ```
 88: --- Batch Predictions ---
 89: 1. Politics
 90: 2. Sports
 91: 3. Health
 92: ```
 93: 
 94: 
 95: ## **Implementation (Key phrases extraction)**
 96: 
 97: Here is the implementation of batch prompting for key phrases extraction.
 98: 
 99: ```python
100: # !pip install langchain langchain-google-genai pydantic
101: 
102: import os
103: from google.colab import userdata
104: from langchain.chat_models import init_chat_model
105: from langchain_core.prompts import ChatPromptTemplate
106: from langchain_core.output_parsers import PydanticOutputParser
107: from pydantic import BaseModel, Field
108: from typing import List
109: 
110: # 1. Set your API key
111: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
112: 
113: # 2. Define the Pydantic schema for structured output
114: class BatchKeyPhraseResponse(BaseModel):
115:     all_key_phrases: List[List[str]] = Field(
116:         ..., 
117:         description="List of key phrase lists (one list per text input)"
118:     )
119: 
120: # 3. Create the parser
121: parser = PydanticOutputParser(pydantic_object=BatchKeyPhraseResponse)
122: 
123: # 4. Initialize the chat model
124: model = init_chat_model(
125:     "gemini-2.5-flash",
126:     model_provider="google_genai",
127:     temperature=0
128: )
129: 
130: # 5. Batch prompting template for key phrase extraction
131: prompt_template = ChatPromptTemplate.from_template(
132:     """
133: Extract the most important key phrases from each text below.
134: Key phrases must be meaningful, concise, and capture the core concepts.
135: 
136: Texts:
137: {texts}
138: 
139: Provide the output strictly in this JSON format:
140: {format_instructions}
141: """
142: )
143: 
144: # 6. Inject parser instructions
145: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
146: 
147: # 7. Build the chain
148: chain = prompt | model | parser
149: 
150: # 8. Example batch of texts
151: texts = [
152:     "Artificial intelligence is transforming healthcare by enabling faster diagnosis and advanced medical imaging.",
153:     "Climate change is accelerating due to rising greenhouse gas emissions and deforestation.",
154:     "Quantum computing promises exponential speedups for complex problem solving."
155: ]
156: 
157: # 9. Invoke
158: result = chain.invoke({"texts": texts})
159: 
160: # 10. Display results
161: print("\n--- Batch Key Phrases ---")
162: for i, phrases in enumerate(result.all_key_phrases):
163:     print(f"\nText {i+1} key phrases:")
164:     print(phrases)
165: ```
166: 
167: Here the output is
168: ```
169: --- Batch Key Phrases ---
170: 
171: Text 1 key phrases:
172: ['Artificial intelligence', 'healthcare transformation', 'faster diagnosis', 'advanced medical imaging']
173: 
174: Text 2 key phrases:
175: ['Climate change acceleration', 'greenhouse gas emissions', 'deforestation']
176: 
177: Text 3 key phrases:
178: ['Quantum computing', 'exponential speedups', 'complex problem solving']
179: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Basic_Prompt_Engineering_Techniques/Emotion_Prompting.md">
  1: # **Emotion Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Emotion prompting is a prompting technique where ‚Äî instead of using a dry or purely neutral instruction ‚Äî you add emotionally-charged phrases  to the prompt so that a large language model (LLM) responds with better outputs. It‚Äôs like asking, *‚ÄúWrite a summary of this article,‚Äù* but adding something like ‚ÄúThis is very important to my career,‚Äù. In simple words, emotion prompting means prompting with the main instruction plus an emotional appeal. 
  8: 
  9: Emotion prompting is like a teacher telling a student: *‚ÄúDo this problem ‚Äî and remember, doing well on this matters a lot for your future,‚Äù* instead of simply saying *‚ÄúDo this problem.‚Äù*
 10: 
 11: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 12: 
 13: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 14: - ‚ú® Weekly GenAI updates
 15: - üìÑ Weekly LLM, Agents and RAG research paper updates
 16: - üìù 1 fresh blog post on an interesting topic every week
 17: 
 18: ## **Implementation (News headlines classification)**
 19: 
 20: Here is the implementation of emotion prompting for news headlines classification.
 21: 
 22: ```python
 23: # !pip install langchain langchain-google-genai pydantic
 24: 
 25: import os
 26: from google.colab import userdata
 27: from langchain.chat_models import init_chat_model
 28: from langchain_core.prompts import ChatPromptTemplate
 29: from langchain_core.output_parsers import PydanticOutputParser
 30: from pydantic import BaseModel, Field
 31: 
 32: # 1. Set your API key
 33: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
 34: 
 35: # 2. Pydantic schema (single output field)
 36: class EmotionPromptClassifyResponse(BaseModel):
 37:     predicted_label: str = Field(..., description="Predicted news category")
 38: 
 39: # 3. Parser
 40: parser = PydanticOutputParser(pydantic_object=EmotionPromptClassifyResponse)
 41: 
 42: # 4. Initialize model
 43: model = init_chat_model(
 44:     "gemini-2.5-flash",
 45:     model_provider="google_genai",
 46:     temperature=0
 47: )
 48: 
 49: # 5. Emotion prompting template (concise)
 50: prompt_template = ChatPromptTemplate.from_template(
 51:     """
 52: Classify the following news headline into one of:
 53: ["Politics", "Sports", "Business", "Technology", "Entertainment", "Health", "World"]
 54: 
 55: This is very important to my career.
 56: 
 57: Headline: {headline}
 58: 
 59: Output JSON:
 60: {format_instructions}
 61: """
 62: )
 63: 
 64: # 6. Inject parser instructions
 65: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
 66: 
 67: # 7. Chain
 68: chain = prompt | model | parser
 69: 
 70: # 8. Example headline
 71: headline = "Government approves new policy to boost semiconductor manufacturing."
 72: 
 73: # 9. Invoke
 74: result = chain.invoke({"headline": headline})
 75: 
 76: # 10. Display result
 77: print("\n--- Predicted Label ---\n", result.predicted_label)
 78: ```
 79: Here the output is
 80: ```
 81: --- Predicted Label ---
 82:  Politics
 83: ```
 84: 
 85: ## **Implementation (Key phrases extraction)**
 86: 
 87: Here is the implementation of emotion prompting for key phrases extraction.
 88: 
 89: ```python
 90: # !pip install langchain langchain-google-genai pydantic
 91: 
 92: import os
 93: from google.colab import userdata
 94: from langchain.chat_models import init_chat_model
 95: from langchain_core.prompts import ChatPromptTemplate
 96: from langchain_core.output_parsers import PydanticOutputParser
 97: from pydantic import BaseModel, Field
 98: from typing import List
 99: 
100: # 1. Set your API key
101: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
102: 
103: # 2. Pydantic schema for key phrase extraction
104: class EmotionPromptKeyphrasesResponse(BaseModel):
105:     key_phrases: List[str] = Field(..., description="List of extracted key phrases")
106: 
107: # 3. Create parser
108: parser = PydanticOutputParser(pydantic_object=EmotionPromptKeyphrasesResponse)
109: 
110: # 4. Initialize model
111: model = init_chat_model(
112:     "gemini-2.5-flash",
113:     model_provider="google_genai",
114:     temperature=0
115: )
116: 
117: # 5. Emotion prompting template (concise)
118: prompt_template = ChatPromptTemplate.from_template(
119:     """
120: Extract the key phrases from the following text. Key phrases should be meaningful, concise, and capture the core concepts.
121: This is very important to my career.
122: 
123: Text: {text}
124: 
125: Output JSON:
126: {format_instructions}
127: """
128: )
129: 
130: # 6. Inject parser instructions
131: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
132: 
133: # 7. Chain
134: chain = prompt | model | parser
135: 
136: # 8. Example text
137: text = """The government has introduced a comprehensive plan to support renewable energy innovation.
138: The initiative focuses on funding solar, wind, and battery storage research programs.
139: Officials believe this effort will significantly accelerate the nation's clean energy transition."""
140: 
141: # 9. Invoke
142: result = chain.invoke({"text": text})
143: 
144: # 10. Display result
145: print("\n--- Extracted Key Phrases ---\n", result.key_phrases)
146: ```
147: 
148: Here the output is
149: ```
150: --- Extracted Key Phrases ---
151:  ['government plan', 'renewable energy innovation', 'funding research programs', 'solar, wind, and battery storage', 'clean energy transition']
152: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Basic_Prompt_Engineering_Techniques/few_shot_prompting.md">
  1: # **Few-Shot Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Few-shot prompting is a technique where you give a large language model a small number of example input-output pairs along with your instruction or question. In other words you show the model how a few instances of the task should be done, then ask it to apply the same pattern to a new instance.  In simple words, few-shot prompting = prompting with a clear instruction *and* a few example input-output pairs.
  8: 
  9: Few-shot prompting is like a teacher first shows a student a couple of solved problems on the board ‚Äî ‚Äúthis is how you do it‚Äù ‚Äî and then gives a new problem for the student to solve on their own. The student uses the pattern from the examples to work out the new problem.
 10: 
 11: 
 12: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 13: 
 14: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 15: - ‚ú® Weekly GenAI updates
 16: - üìÑ Weekly LLM, Agents and RAG research paper updates
 17: - üìù 1 fresh blog post on an interesting topic every week
 18: 
 19: ## **Implementation (News headlines classification)**
 20: 
 21: Here is the implementation of few-shot prompting for news headlines classification.
 22: 
 23: ```python
 24: # !pip install langchain langchain-google-genai pydantic
 25: 
 26: import os
 27: from google.colab import userdata
 28: from langchain.chat_models import init_chat_model
 29: from langchain_core.prompts import ChatPromptTemplate
 30: from langchain_core.output_parsers import PydanticOutputParser
 31: from pydantic import BaseModel, Field
 32: 
 33: # 1. Set your API key
 34: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
 35: 
 36: # 2. Define the Pydantic schema for structured output
 37: class FewShotClassifyResponse(BaseModel):
 38:     predicted_label: str = Field(..., description="Predicted news category")
 39: 
 40: # 3. Create the parser
 41: parser = PydanticOutputParser(pydantic_object=FewShotClassifyResponse)
 42: 
 43: # 4. Initialize the chat model
 44: model = init_chat_model(
 45:     "gemini-2.5-flash",
 46:     model_provider="google_genai",
 47:     temperature=0
 48: )
 49: 
 50: # 5. Few-shot prompt template (includes examples)
 51: prompt_template = ChatPromptTemplate.from_template(
 52:     """
 53: Classify the news headline into one of:
 54: ["Politics", "Sports", "Business", "Technology", "Entertainment", "Health", "World"]
 55: 
 56: Here are some examples:
 57: 
 58: Example 1:
 59: Headline: "Prime Minister meets foreign delegates to discuss trade agreements."
 60: Label: Politics
 61: 
 62: Example 2:
 63: Headline: "Tech company introduces new AI-powered smartphone."
 64: Label: Technology
 65: 
 66: Example 3:
 67: Headline: "Stock markets fall amid global economic slowdown."
 68: Label: Business
 69: 
 70: Now classify the following:
 71: 
 72: Headline: {headline}
 73: 
 74: Provide your output in this JSON format:
 75: {format_instructions}
 76: """
 77: )
 78: 
 79: # 6. Inject parser formatting instructions
 80: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
 81: 
 82: # 7. Build the chain
 83: chain = prompt | model | parser
 84: 
 85: # 8. Example headline
 86: headline = "Government approves new policy to boost semiconductor manufacturing."
 87: 
 88: # 9. Invoke the chain
 89: result = chain.invoke({"headline": headline})
 90: 
 91: # 10. Display result
 92: print("\n--- Predicted Label ---\n", result.predicted_label)
 93: ```
 94: 
 95: Here the output is
 96: ```
 97: --- Predicted Label ---
 98:  Politics
 99: ```
100: 
101: 
102: ## **Implementation (Key phrases extraction)**
103: 
104: Here is the implementation of few-shot prompting for key phrases extraction.
105: 
106: ```python
107: # !pip install langchain langchain-google-genai pydantic
108: 
109: import os
110: from google.colab import userdata
111: from langchain.chat_models import init_chat_model
112: from langchain_core.prompts import ChatPromptTemplate
113: from langchain_core.output_parsers import PydanticOutputParser
114: from pydantic import BaseModel, Field
115: from typing import List
116: 
117: # 1. Set your API key
118: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
119: 
120: # 2. Define the Pydantic schema for structured output
121: class KeyPhraseResponse(BaseModel):
122:     key_phrases: List[str] = Field(..., description="List of extracted key phrases")
123: 
124: # 3. Create parser
125: parser = PydanticOutputParser(pydantic_object=KeyPhraseResponse)
126: 
127: # 4. Initialize model
128: model = init_chat_model(
129:     "gemini-2.5-flash",
130:     model_provider="google_genai",
131:     temperature=0
132: )
133: 
134: # 5. Few-shot prompt with examples
135: prompt_template = ChatPromptTemplate.from_template(
136:     """
137: Extract the most important key phrases from the text. 
138: Key phrases should be meaningful, concise, and capture core concepts.
139: 
140: Here are some examples:
141: 
142: Example 1:
143: Text: "Climate change is accelerating due to rising greenhouse gas emissions."
144: Key Phrases: ["climate change", "greenhouse gas emissions"]
145: 
146: Example 2:
147: Text: "Machine learning models require large datasets for effective training."
148: Key Phrases: ["machine learning models", "large datasets", "effective training"]
149: 
150: Example 3:
151: Text: "Renewable energy sources like solar and wind are becoming more affordable."
152: Key Phrases: ["renewable energy sources", "solar", "wind", "affordable energy"]
153: 
154: Now extract key phrases from the following text:
155: 
156: Text:
157: {input_text}
158: 
159: Provide the output in this JSON format:
160: {format_instructions}
161: """
162: )
163: 
164: # 6. Inject parser instructions
165: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
166: 
167: # 7. Build LCEL chain
168: chain = prompt | model | parser
169: 
170: # 8. Example text
171: input_text = "Artificial intelligence is transforming healthcare by enabling faster diagnosis, personalized treatments, and advanced medical imaging."
172: 
173: # 9. Invoke
174: result = chain.invoke({"input_text": input_text})
175: 
176: # 10. Display results
177: print("\n--- Key Phrases ---\n", result.key_phrases)
178: ```
179: Here the output is
180: ```
181: --- Key Phrases ---
182:  ['artificial intelligence', 'healthcare transformation', 'faster diagnosis', 'personalized treatments', 'advanced medical imaging']
183: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Basic_Prompt_Engineering_Techniques/Role_Prompting.md">
  1: # **Role Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Role prompting is a technique where the large language model is instructed to take on a specific *role, identity, or persona* before performing a task. Instead of giving only a task instruction, you tell the model who it should act as, such as *‚ÄúAct as a cybersecurity expert and explain‚Ä¶‚Äù* or *‚ÄúYou are a professional journalist. Summarize‚Ä¶‚Äù*. By adopting the assigned role, the model adjusts its tone, depth, and reasoning to match that persona.
  8: 
  9: It is like asking a student to ‚Äúpretend you are a doctor‚Äù before explaining a medical concept. The student now answers not just from general knowledge but through the lens of that specialized role. In simple words, role prompting guides the model‚Äôs behavior by assigning it a specific identity or expertise.
 10: 
 11: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 12: 
 13: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 14: - ‚ú® Weekly GenAI updates
 15: - üìÑ Weekly LLM, Agents and RAG research paper updates
 16: - üìù 1 fresh blog post on an interesting topic every week
 17: 
 18: ## **Implementation (News headlines classification)**
 19: 
 20: Here is the implementation of role prompting for news headlines classification.
 21: 
 22: ```python
 23: # !pip install langchain langchain-google-genai pydantic
 24: 
 25: import os
 26: from google.colab import userdata
 27: from langchain.chat_models import init_chat_model
 28: from langchain_core.prompts import ChatPromptTemplate
 29: from langchain_core.output_parsers import PydanticOutputParser
 30: from pydantic import BaseModel, Field
 31: 
 32: # 1. Set your API key
 33: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
 34: 
 35: # 2. Define the Pydantic schema for structured output
 36: class ZeroShotClassifyResponse(BaseModel):
 37:     predicted_label: str = Field(..., description="Predicted news category")
 38: 
 39: # 3. Create the parser
 40: parser = PydanticOutputParser(pydantic_object=ZeroShotClassifyResponse)
 41: 
 42: # 4. Initialize the chat model
 43: model = init_chat_model(
 44:     "gemini-2.5-flash",
 45:     model_provider="google_genai",
 46:     temperature=0
 47: )
 48: 
 49: # 5. Zero-shot prompt template (no examples)
 50: prompt_template = ChatPromptTemplate.from_template(
 51:     """
 52: You are a professional news editor with years of experience in global journalism. Your job is to accurately classify news headlines into their correct category.
 53: Classify the news headline into one of the categories:
 54: ["Politics", "Sports", "Business", "Technology", "Entertainment", "Health", "World"]
 55: 
 56: Headline: {headline}
 57: 
 58: Provide your output in this JSON format:
 59: {format_instructions}
 60: """
 61: )
 62: 
 63: # 6. Inject parser instructions
 64: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
 65: 
 66: # 7. Chain: prompt ‚Üí model ‚Üí parser
 67: chain = prompt | model | parser
 68: 
 69: # 8. Example headline
 70: headline = "Government approves new policy to boost semiconductor manufacturing."
 71: 
 72: # 9. Invoke
 73: result = chain.invoke({"headline": headline})
 74: 
 75: # 10. Display result
 76: print("\n--- Predicted Label ---\n", result.predicted_label)
 77: ```
 78: 
 79: Here the output is
 80: ```
 81: --- Predicted Label ---
 82:  Politics
 83: ```
 84: 
 85: 
 86: ## **Implementation (Key phrases extraction)**
 87: 
 88: Here is the implementation of role prompting for key phrases extraction.
 89: 
 90: ```python
 91: # !pip install langchain langchain-google-genai pydantic
 92: 
 93: import os
 94: from google.colab import userdata
 95: from langchain.chat_models import init_chat_model
 96: from langchain_core.prompts import ChatPromptTemplate
 97: from langchain_core.output_parsers import PydanticOutputParser
 98: from pydantic import BaseModel, Field
 99: from typing import List
100: 
101: # 1. Set your API key
102: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
103: 
104: # 2. Define the Pydantic schema for structured output
105: class KeyPhraseResponse(BaseModel):
106:     key_phrases: List[str] = Field(..., description="List of extracted key phrases")
107: 
108: # 3. Create the parser
109: parser = PydanticOutputParser(pydantic_object=KeyPhraseResponse)
110: 
111: # 4. Initialize the chat model
112: model = init_chat_model(
113:     "gemini-2.5-flash",
114:     model_provider="google_genai",
115:     temperature=0
116: )
117: 
118: # 5. Role prompting template for key phrase extraction
119: prompt_template = ChatPromptTemplate.from_template(
120:     """
121: You are a professional linguistic analyst specializing in information extraction.
122: Your task is to extract the most important key phrases from the given text.
123: 
124: Key phrases should be:
125: - concise
126: - meaningful
127: - representative of the core ideas
128: 
129: Text:
130: {input_text}
131: 
132: Provide the output strictly in this JSON format:
133: {format_instructions}
134: """
135: )
136: 
137: # 6. Inject parser instructions
138: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
139: 
140: # 7. Build LCEL chain
141: chain = prompt | model | parser
142: 
143: # 8. Example text
144: input_text = (
145:     "Artificial intelligence is transforming healthcare by enabling faster diagnosis, "
146:     "personalized treatments, and advanced medical imaging."
147: )
148: 
149: # 9. Invoke
150: result = chain.invoke({"input_text": input_text})
151: 
152: # 10. Display results
153: print("\n--- Key Phrases ---\n", result.key_phrases)
154: ```
155: 
156: Here the output is
157: ```
158: --- Key Phrases ---
159:  ['Artificial intelligence', 'transforming healthcare', 'faster diagnosis', 'personalized treatments', 'advanced medical imaging']
160: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/Basic_Prompt_Engineering_Techniques/Zero_Shot_Prompting.md">
  1: # **Zero Shot Prompting**
  2: 
  3: Authored by Kalyan KS. You can follow me on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for latest LLM, RAG and Agent updates.
  4: 
  5: ## **Overview**
  6: 
  7: Zero-shot prompting is the simplest prompting technique where a large language model is given only an instruction or question (no examples) and is expected to complete the task using its general pre-trained knowledge.  It is like asking, *‚ÄúTranslate this sentence into French‚Äù* or *‚ÄúClassify this review as positive, negative, or neutral‚Äù* without showing any sample translations or labeled reviews. In simple words, zero-shot prompting is prompting with a clear instruction or question without any examples. 
  8: 
  9: Zero-shot prompting is like a teacher giving a student a problem to solve without showing them a practice example on the board first. The student must rely solely on their general knowledge and what they have learned previously to arrive at the answer.
 10: 
 11: ## **Stay Updated with Generative AI, LLMs, Agents and RAG**
 12: 
 13: Join üöÄ [**AIxFunda** free newsletter](https://aixfunda.substack.com/) to get *latest updates* and *interesting tutorials* related to Generative AI, LLMs, Agents and RAG. 
 14: - ‚ú® Weekly GenAI updates
 15: - üìÑ Weekly LLM, Agents and RAG research paper updates
 16: - üìù 1 fresh blog post on an interesting topic every week
 17: 
 18: ## **Implementation (News headlines classification)**
 19: 
 20: Here is the implementation of zero-shot prompting for news headlines classification.
 21: 
 22: ```python
 23: # !pip install langchain langchain-google-genai pydantic
 24: 
 25: import os
 26: from google.colab import userdata
 27: from langchain.chat_models import init_chat_model
 28: from langchain_core.prompts import ChatPromptTemplate
 29: from langchain_core.output_parsers import PydanticOutputParser
 30: from pydantic import BaseModel, Field
 31: 
 32: # 1. Set your API key
 33: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
 34: 
 35: # 2. Define the Pydantic schema for structured output
 36: class ZeroShotClassifyResponse(BaseModel):
 37:     predicted_label: str = Field(..., description="Predicted news category")
 38: 
 39: # 3. Create the parser
 40: parser = PydanticOutputParser(pydantic_object=ZeroShotClassifyResponse)
 41: 
 42: # 4. Initialize the chat model
 43: model = init_chat_model(
 44:     "gemini-2.5-flash",
 45:     model_provider="google_genai",
 46:     temperature=0
 47: )
 48: 
 49: # 5. Zero-shot prompt template (no examples)
 50: prompt_template = ChatPromptTemplate.from_template(
 51:     """
 52: Classify the news headline into one of the categories:
 53: ["Politics", "Sports", "Business", "Technology", "Entertainment", "Health", "World"]
 54: 
 55: Headline: {headline}
 56: 
 57: Provide your output in this JSON format:
 58: {format_instructions}
 59: """
 60: )
 61: 
 62: # 6. Inject parser instructions
 63: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
 64: 
 65: # 7. Chain: prompt ‚Üí model ‚Üí parser
 66: chain = prompt | model | parser
 67: 
 68: # 8. Example headline
 69: headline = "Government approves new policy to boost semiconductor manufacturing."
 70: 
 71: # 9. Invoke
 72: result = chain.invoke({"headline": headline})
 73: 
 74: # 10. Display result
 75: print("\n--- Predicted Label ---\n", result.predicted_label)
 76: ```
 77: Here the output is
 78: ```
 79: --- Predicted Label ---
 80:  Politics
 81:  ```
 82: 
 83: ## **Implementation (Key phrases extraction)**
 84: 
 85: Here is the implementation of zero-shot prompting for key phrases extraction.
 86: 
 87: ```python
 88: # !pip install langchain langchain-google-genai pydantic
 89: 
 90: import os
 91: from google.colab import userdata
 92: from langchain.chat_models import init_chat_model
 93: from langchain_core.prompts import ChatPromptTemplate
 94: from langchain_core.output_parsers import PydanticOutputParser
 95: from pydantic import BaseModel, Field
 96: from typing import List
 97: 
 98: # 1. Set your API key
 99: os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
100: 
101: # 2. Define the Pydantic schema for structured output
102: class KeyPhraseResponse(BaseModel):
103:     key_phrases: List[str] = Field(..., description="List of extracted key phrases")
104: 
105: # 3. Create the parser
106: parser = PydanticOutputParser(pydantic_object=KeyPhraseResponse)
107: 
108: # 4. Initialize the chat model
109: model = init_chat_model(
110:     "gemini-2.5-flash",
111:     model_provider="google_genai",
112:     temperature=0
113: )
114: 
115: # 5. Zero-shot prompt template for key phrase extraction
116: prompt_template = ChatPromptTemplate.from_template(
117:     """
118: Extract the most important key phrases from the text. 
119: Key phrases should be meaningful, concise, and capture the core concepts.
120: 
121: Text:
122: {input_text}
123: 
124: Provide the output in this JSON format:
125: {format_instructions}
126: """
127: )
128: 
129: # 6. Inject parser instructions
130: prompt = prompt_template.partial(format_instructions=parser.get_format_instructions())
131: 
132: # 7. Build LCEL chain
133: chain = prompt | model | parser
134: 
135: # 8. Example text
136: input_text = "Artificial intelligence is transforming healthcare by enabling faster diagnosis, personalized treatments, and advanced medical imaging."
137: 
138: # 9. Invoke
139: result = chain.invoke({"input_text": input_text})
140: 
141: # 10. Display results
142: print("\n--- Key Phrases ---\n", result.key_phrases)
143: print("\n--- Reason ---\n", result.short_reason)
144: ```
145: 
146: Here the output is
147: ```
148: --- Key Phrases ---
149:  ['Artificial intelligence', 'transforming healthcare', 'faster diagnosis', 'personalized treatments', 'advanced medical imaging']
150: ```
</file>

<file path="__LOCAL-REPO/__exemplar/__import/prompt-engineering-techniques/README.md">
 1: ## Related Repositories
 2: - üöÄ[LLM Interview Questions and Answers](https://github.com/KalyanKS-NLP/LLM-Interview-Questions-and-Answers-Hub)  - 100+ LLM Interview questions with answers.
 3: - üë®üèª‚Äçüíª[LLM Engineer Toolkit](https://github.com/KalyanKS-NLP/llm-engineer-toolkit) - Categories wise collection of 120+ LLM, RAG and Agent related libraries. 
 4: - ü©∏[LLM, RAG and Agents Survey Papers Collection](https://github.com/KalyanKS-NLP/LLM-Survey-Papers-Collection) - Category wise collection of 200+ survey papers.
 5: 
 6: | # | Category | Technique | Description | Learn |
 7: |---|----------|-----------|-------------|--------|
 8: | 1 | Basic    | Zero-shot Prompting | Prompting without examples | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Basic_Prompt_Engineering_Techniques/Zero_Shot_Prompting.md)  |
 9: | 2 | Basic    | Emotion Prompting | Prompting with emotional cues | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Basic_Prompt_Engineering_Techniques/Emotion_Prompting.md) |
10: | 3 | Basic    | Role Prompting | Model acts by adopting a specified role | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Basic_Prompt_Engineering_Techniques/Role_Prompting.md) |
11: | 4 | Basic    | Batch Prompting | Model processes multiple inputs in one prompt | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Basic_Prompt_Engineering_Techniques/Batch_Prompting.md) | 
12: | 5 | Basic    | Few-Shot Prompting | Prompting with examples | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Basic_Prompt_Engineering_Techniques/few_shot_prompting.md) |
13: | 6 | Advanced <br> - Reasoning  | Zero-Shot CoT Prompting | Step-by-step reasoning without examples | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Zero_Shot_CoT_Prompting.md) | 
14: | 7 | Advanced <br> - Reasoning | Chain of Draft (CoD) Prompting | Step-by-step reasoning in a compact form without examples | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Chain_of_Draft_Prompting.md) |
15: | 8 | Advanced <br> - Reasoning | Meta Prompting | Structured blueprint + step-by-step reasoning | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Meta_Prompting.md) | 
16: | 9 | Advanced <br> - Reasoning  | Analogical Prompting | Self-generated relevant examples + step-by-step reasoning | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Analogical_Prompting.md) | 
17: | 10 | Advanced <br> - Reasoning | Thread of Thoughts Prompting | Breaks chaotic context into parts, summarizes, filters, then answers | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Thread_of_Thoughts_Prompting.md) |
18: | 11 | Advanced <br> - Reasoning | Tabular CoT Prompting | Step-by-step reasoning in tabular format | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Tabular_Chain_of_Thought_Prompting.md) | 
19: | 12 | Advanced <br> - Reasoning | Few-Shot CoT Prompting   | Example step-by-step solutions + model imitates reasoning | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Few_Shot_Chain-of_Thought_Prompting.md) | 
20: | 13 | Advanced <br> - Reasoning | Self-Ask Prompting | Model generates sub-questions + solves them sequentially | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Self_Ask_Prompting.md) | 
21: | 14 | Advanced <br> - Reasoning | Contrastive CoT Prompting | Correct + incorrect reasoning examples to guide thinking | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Contrastive_CoT_Prompting.md) |
22: | 15 | Advanced <br> - Reasoning | Chain of Symbol Prompting | Symbolic reasoning examples + symbolic step-by-step solution | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Chain_of_Symbol_Prompting.md) | 
23: | 16 | Advanced <br> - Break Down | Least to Most Prompting | Decompose complex problem into sub-questions + solve them one by one | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Least_to_Most_Prompting.md) |
24: | 17 | Advanced <br> - Break Down | Plan and Solve Prompting | Model plans first, then solves step-by-step | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Plan_and_Solve_Prompting.md) |
25: | 18 | Advanced <br> - Break Down | Program of Thoughts Prompting | Generate code (or symbolic steps) + run it for precise reasoning | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Program_of_Thoughts_Prompting.md) |
26: | 19 | Advanced <br> - Break Down | Faithful CoT Prompting | Natural-language plan + executable code ensuring logic-derived answers | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Faithful_Chain_of_Thought_Prompting.md) |
27: | 20 | Advanced <br> - Break Down | Meta Cognitive Prompting | Model reflect on its reasoning strategy before answering | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Meta_Cognitive_Prompting.md) |
28: | 21 | Advanced <br> - Ensemble | Self Consistency Prompting | Generate many reasoning paths + pick the most common answer | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Self_Consistency_Prompting.md) |
29: | 22 | Advanced <br> - Ensemble | Universal Self Consistency Prompting | Sample multiple outputs + LLM picks the most coherent one | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Universal_Self_Consistency_Prompting.md) | 
30: | 23 | Advanced <br> - Ensemble | Multi Chain Reasoning Prompting | Multiple reasoning paths + meta-synthesis into a superior final answer | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Multi_Chain_Reasoning_Prompting.md) |
31: | 24 | Advanced <br> - Critique | Self Refine Prompting | Model critiques its own response and iteratively improves it | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Self_Refine_Prompting.md) |
32: | 25 | Advanced <br> - Critique | Chain of Verification | Draft an answer, verify it independently, then correct it | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Chain_of_Verification_Prompting.md) |
33: | 26 | Advanced <br> - Multilingual | Chain of Translation Prompting | Translate first, then perform the task for clearer reasoning | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Chain_of_Translation_Prompting.md) |
34: | 27 | Advanced <br> - Multilingual | Cross Lingual Prompting | Translate for meaning first, then step-step reason in a stronger language | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Cross_Lingual_Prompting.md) |
35: | 28 | Advanced <br> - Multi-Step | Rephrase and Respond Prompting | Rewrite the question clearly, then answer the clarified version | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Rephrase_and_Respond_Prompting.md) |
36: | 29 | Advanced <br> - Multi-Step | Step Back Prompting | Identify the general principle first, then solve the specific problem | [Link](https://github.com/KalyanKS-NLP/Prompt-Engineering-Techniques-Hub/blob/main/Advanced_Prompt_Engineering_Techniques/Step_Back_Prompting.md) |
</file>

</files>
