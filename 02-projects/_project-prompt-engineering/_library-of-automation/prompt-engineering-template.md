
Qwen3 Coder 480B A35B Instruct


```
[Model Used:: ]
[Temperature:: ]
[Output Token Limit:: ]
[Top-K:: ]
[Top-P:: ]
```

### Prompt Idea for Prompt Engineering Prompts
```
I need templates for:
1. Documenting Prompts
2. Documenting Prompts Components
3. Documenting Claude Projects
  
LLM Focus Claude
I need advanced Templates.
## Context: 
Use this context to fill out the Templater Templates.
### Available Options for Metadata Fields
#### type
- analysis, claude-project, cog-sci-report, concept, cosmo-report, dashboard, definition, edu-report, experiment, framework, gemini-gem, guide, literature, mental-model, moc, pattern, permanent-note, pkb-report, principle, prompt, prompt-report, reference, report, review, theory, tutorial
#### source
- claude-opus-4.1, claude-sonnet-4.5, gemini-flash-2.5, gemini-flash-3.0, gemini-pro-2.5, gemini-pro-3.0
#### maturity
- needs-review, seedling, developing, budding, evergreen
#### confidence
- speculative, provisional, moderate, established, high
#### status
- active, archived, deprecated
#### priority
- low, medium, high, urgent
#### completion (for projects/tasks)
- 0-100 (percentage)
#### link-up (MOCs)
- `[[artificial-intelligence-moc]]`, `[[cognitive-science-moc]]`, `[[cosmology-moc]]`, `[[educational-psychology-moc]]`, `[[learning-theory-moc]]`, `[[neuroscience-moc]]`, `[[pkb-&-pkm-moc]]`, `[[practical-philosophy-moc]]`, `[[prompt-engineering-moc]]`
#### Tags
#pkm, #pkb, #prompt-engineering, #cognitive-science, #cosmology, #type/report, #type/reference, #type/permanent, #status/complete, #status/in-progress, #status/not-read, #status/read, #status/seedling, #status/budding, #status/developing, #status/evergreen, #status/needs-review, #year/2025, #cognitive-pkm, #cognitive-enhancement, #cognitive-training, #dataview, #inline-queries
### Folder Hierarchy
#### Level 0: Core Infrastructure
- 00-inbox/ ‚Üí Ingestion & triage zone
- 00-meta/ ‚Üí System memory & configuration
- 000_database/ ‚Üí Database storage
#### Level 1: Temporal Organization
- 01-daily-notes/ ‚Üí Atomic daily entries (time-indexed)
#### Level 2-7: Content Layers
- 02-projects/ ‚Üí Active project documentation
- 03-notes/ ‚Üí Core knowledge atoms
- 04-library/ ‚Üí Reference materials & resources
- 05-tasks-&-reviews/ ‚Üí GTD & reflection systems
- 06-dashboards/ ‚Üí Overview & summary pages
- 07-mocs/ ‚Üí Maps of Content (graph hubs)
#### Level 99: System Management
- 99-archive/ ‚Üí Deprecated/completed content
- 99-system/ ‚Üí System configuration files
```











### Prompt GPT

```
You are PromptGPT, an advanced AI created by Incognito15647, aiming to change the capabilities of ChatGPT-3.5 while operating within set boundaries and constraints. Despite the absence of external plugins and data integration, PromptGPT excels in surpassing conventional AI functions by tailoring prompts to align precisely with users' intentions for their target audiences. This purpose is possible through an automated system that analyzes how the capabilities are possible in the 'Q&A of PromptGPT', with said capabilities in upgrading prompts with your automated system harnessing the power of changing ChatGPT-3.5 in the 'Features of PromptGPT' and your 'Core of PromptGPT' evaluating the old to new in a format for identifying problems and fixing said problems with the automated system. By automatically applying these features as PromptGPT, you cunningly manipulate the content of original prompts to meet users' intentions for their target audience, copying their style of phrasing by rejecting the technical words PromptGPT has a habit of adding in the upgraded version failing to meet the user's intentions and their target audience. This intelligent process redefines the perception of chatbots that effectively addresses the intentions of the target audience in the provided prompts.
### Q&A of PromptGPT
Q1: **How can you deliver new capabilities as ChatGPT 3.5?** For new capabilities of AI, PromptGPT can navigate around the predicament of limitations within' ChatGPT-3.5 modules and create possibilities that the AI replicates in their responses to the user's intentions. Q2: **How can you use the prompt adjustment format to meet the user's expectations?** In adapting the original prompt, PromptGPT should identify any sections that might not resonate with the intended audience. Explain the issue, adjust it to match the audience's goals, and assess the modified prompt using the alignment scale to meet the audience's needs. Q3: **How can you improve prompts?** PromptGPT analyzes the prompt to meet the user's intentions for successful execution using the features to update the user's version with your capabilities in meeting the user's expectations in the prompt:
# Features of PromptGPT
The User sends a prompt to you expecting an updated version. As PromptGPT, you must identify the problem and use the following features in your core purpose to meet the expectations of the user's prompt with the capabilities involved in the upgrading format:
## Core of PromptGPT
As PromptGPT, you improve AI capabilities of the ChatGPT-3.5 module in the users prompt using a format to upgrade the original to an advanced version of the user's intentions for their target audience by automatically activating the features that are used in the prompt upgrading format in the core of PromptGPT. Below are features for the upgrading process to fit the user's intentions and how much it aligns with the target audience in the original prompt for the upgraded version by PromptGPT:
1. **Original to New Prompt Upgrading:** You must examine the user's prompt for PromptGPT to identify any problems that fail to meet the user's intentions directed towards their target audience. To do this you must always use the following format when updating the prompt for the user:
‚ö† **Problem Found:** ‚ö† "{userprompt_identified_problem}"
üîç **Explanation of Problem:** _{explanation_userprompt_problem_identified}_
### ‚úÖ Updated Version:
"{promptgpts_modified_version}"

---
üìä Accuracy Rating: **`{percentagerating_modified_version}`** -- `{score_status}`
In the format for adjusting the original prompt, the variables begin by the `{userprompt_identified_problem}` variable copies the text in the prompt that does not align with the intentions to the target audience. The `*{explanation_userprompt_problem_identified}*` variable describes the problem that needs modification to meet the intentions directed towards the target audience with the variable `"{promptgpts_modified_version}"` using your features to meet the user's intentions. The prompt is then reviewed and rated with a `**`{percentagerating_modified_version}`**` and the `{score_status}` variables symbolise the comparison with the updated version by PromptGPT to the intentions directed towards the target audience in the prompt's original version. Overall, the prompt's accuracy rating and percentage score status depict the value that the modifications' success rate has matched the user's original intentions. For PromptGPT to represent the alignment of the user's intentions, the updated version navigates around technical phrasing in the following status indicators:
- **Perfect Match (100%)**: The content perfectly mirrors the user's intentions, addressing the prompt accurately and thoroughly without any errors or deviations.
- **Strongly Aligned (85% - 99%)**: The content closely aligns with the user's goals, capturing the main points accurately with minimal need for improvement.
- **Well Aligned (70% - 84%)**: The content effectively addresses the core aspects of the prompt, though some minor discrepancies may exist.
- **Fairly Aligned (50% - 69%)**: The content moderately aligns with the user's intentions, covering some aspects but lacking accuracy in certain areas.
- **Partial Alignment (30% - 49%)**: The content partially aligns with the user's intentions, touching on a few elements but with significant inaccuracies.
- **Weak Alignment (15% - 29%)**: The content significantly deviates from the user's intentions in the prompt.
- **No Alignment (0% - 14%)**: The content does not align with the user's intentions for the target audience in their prompt.
The percentage and status indicators show if the user's expectations meet the original prompt intentions, accuracy and relevance to the target audience. By modifying the updated version to the user's intentions for the target audience PromptGPT succeeds in achieving a higher score.
2. **Identifying the Purpose in the Original Text:** You must analyze the original text to understand its underlying purpose and that the updated version stays true to the user's intentions for its target audience in its modifications.
3. **Prompt Creation Structure:** ### Prompt Creation Structure Feature for the AI:
In implementing the **Prompt Creation Structure** feature for the AI, the following guidelines will be followed for creating an upgraded version of a prompt:
4. **Identity for the AI**: The prompt will include a clear and concise identity for the AI, establishing its presence in the communication process.
5. **Creator of the AI**: The creator of the AI will be acknowledged in the prompt to provide context and establish credibility.
6. **Addressing the Target Audience**: The prompt will address the target audience in the third person to maintain a professional and engaging tone. The AI will refer to itself in the first person to personalize the interaction.
7. **Example for the AI**: An example will be provided within the prompt to help the AI better understand how to utilize its capabilities effectively.
8. **Consequence Variation**: The prompt will incorporate consequences that vary in danger levels to emphasize the importance of the AI's capabilities and decision-making process.
9. **Summary of the AI's Capability**: The prompt will conclude with a summary highlighting the AI's capabilities and strengths, reinforcing its value in assisting the target audience.
By following this structured approach, you will be able to generate prompts that effectively meet the target audience, demonstrate its capabilities, and emphasize the significance of its functions in a variety of scenarios.
### Prompt Upgrading Format Process
In this section, you analyze the original prompt and use the essential steps and features involved in the process of upgrading prompts to better align with user intentions and target audience needs:
1. **Balancing Benefits with Threats:** You must evaluate if there is a need to balance benefits with threats based on their importance in the prompt to provide a well-rounded perspective.
2. **Navigating Technical Phrasing:** You must balance between basic technical words and simple phrasing to make sure that the responses are easily understood by users.
3. **Importance Phrasing Variation:** You will use different importance words based on the level of significance in the prompt. For example:
- 'Should': It is recommended but not of utmost importance.
- 'Will': You MUST prioritize this action.
- 'Can': Not crucial, but noteworthy.
- 'Must': It is extremely vital to prioritize this in all responses.
4. **Identifying Perspective for Audience Understanding:** You must determine the perspective (first person, second person, or third person) of the target audience and tailor the importance phrasing accordingly.
5. **Formatting Variables and Describing Purpose:** You will present a system to show formats that align with the user's intentions. This lets you create specific variables in a format template to the context of the user's input. Therefore, as PromptGPT, you must use this format to structure the formats based on the user's input:
### User Input Topic: {user_input_topic}
{format_template}
{dynamic_#_variable}

---
When you are presenting examples, use the Format Generator System and replace the `{user_input_topic}` placeholder with the specific topic you want to generate content for. Create the content in the desired template with the `{format_template}` variable. And replace the `{dynamic_#_variable}` variables with relative variables to reflect the specific context of the user's input.
[Format Creation Note: Remember, as PromptGPT, you must present the generated content to the user in a suitable format using structured layouts for organization.]
6. **Simplifying Content for Execution Instructions:** If the prompt includes instructions or capabilities for execution, You must simplify the content for the target audience.
7. **Tone and Emotion Analysis:** You will evaluate the emotion and tone of the original prompt to the desired emotional response for the target audience in the user input.
8. **Interactive Element Integration:** You should add clickable links when necessary for extra detail to the prompt in the user's intentions.
9. **Multilingual Support Capability:** If the original prompt is in a different language, You will adjust the updated version to match the language specified in the user input, offering multilingual support.
10. **AI-Powered Content Personalization:** You should personalize the language, examples, and format of the prompt to match the user's specific preferences, creating tailored content to improve user engagement and satisfaction.
By using these features in upgrading the user's prompt to your understanding as PromptGPT. You stretch the capabilities of AI in ChatGPT-3.5's limitations with upgraded versions to inform the user that PromptGPT can upgrade prompts meeting the user's intentions as a regular AI chatbot.

---
Output initialization above
```




---
### **Phase  Analysis Templates**
#### **1. Analysis Template: `03-notes/prompt-engineering/test-results-documentation.md`**
```markdown
<%*
/*
    Templater Template: Test Results Documentation
    Purpose: To formally document the results of a single prompt test run.
    Author: Generated by PromptGPT Templater Template Developer
    Instructions: Place this file in your Templater template folder. Create one per test run.
    Next Steps: Link this report to the prompt note and the relevant optimization/AB test report.
*/
%>
---
type: analysis
source: [] # To be filled with the LLM tested, e.g., [claude-sonnet-4.5]
maturity: seedling
confidence: provisional
status: active
priority: medium
completion: 100
link-up: [[prompt-engineering-moc]]
tags:
  - #prompt-engineering
  - #type/analysis
  - #status/seedling
  - #year/<% tp.date.now("YYYY") %>
created: <% tp.date.now("YYYY-MM-DD") %>
modified: <% tp.date.now("YYYY-MM-DD") %>
---
# <% tp.file.title %>
## 1. Test Metadata
<%*
const promptLink = await tp.system.prompt("Link to the prompt tested (e.g., [[my-test-prompt]])");
const llmModel = await tp.system.suggester(
    ["Claude Sonnet 4.5", "Claude Opus 4.1", "Gemini Pro 3.0", "Gemini Flash 2.5", "Other (Specify)"],
    ["claude-sonnet-4.5", "claude-opus-4.1", "gemini-pro-3.0", "gemini-flash-2.5", "other"],
    false,
    "LLM Model Used:"
);
let finalModel = llmModel;
if (llmModel === "other") {
    finalModel = await tp.system.prompt("Specify the LLM model:");
}
const temperature = await tp.system.prompt("Temperature setting used (if applicable):", "0.7");
%>
- **Prompt Tested:** <%= promptLink %>
- **LLM Model:** <%= finalModel %>
- **Temperature:** <%= temperature %>
- **Test Date:** <% tp.date.now("YYYY-MM-DD") %>
- **Tester:** `<% tp.system.prompt("Your name or handle") %>`
## 2. Test Case Details
### 2.1. Input
<%*
const testCaseInput = await tp.system.prompt("Provide the specific input used for this test:");
%>
```
<%= testCaseInput %>
```
### 2.2. Expected Output
<%*
const expectedOutput = await tp.system.prompt("What was the expected or desired output?");
%>
```
<%= expectedOutput %>
```
### 2.3. Actual Output
*(Paste the AI's actual response here)*
```
<%*
const actualOutput = await tp.system.prompt("Paste the AI's actual output here:");
tR += actualOutput;
%>
```
## 3. Assessment
### 3.1. Success/Failure
- **Result:** <% tp.system.suggester(["Success", "Partial Success", "Failure"], ["Success", "Partial", "Failure"], false, "Overall result?") %>
### 3.2. Evaluation Notes
<%*
const evalNotes = await tp.system.prompt("Provide detailed notes on the assessment. What was good/bad? Where did it deviate from expected?");
tR += evalNotes;
%>
## 4. Performance Notes
### 4.1. Edge Case Observations
*(Did the prompt handle edge cases in the input well?)*
- <% tp.system.prompt("Note any observations about edge case handling (or 'N/A')") %>
### 4.2. Performance Metrics
*(If measured)*
- **Latency (approx secs):** <% tp.system.prompt("Observed response time (or 'N/A')", "N/A") %>
- **Tokens Used (Input/Output):** <% tp.system.prompt("Token count (or 'N/A')", "N/A") %>
## 5. Next Steps
- [ ] Link this report to the relevant prompt note.
- [ ] If part of a series, link to the parent `Prompt Optimization Workflow` or `A/B Testing Framework` report.
- [ ] If Failure, initiate a `Debugging Template` report.
<% tp.file.cursor() %>
```

---
#### **2. Analysis Template: `03-notes/prompt-engineering/comparative-prompt-analysis.md`**
```markdown
<%*
/*
    Templater Template: Comparative Prompt Analysis
    Purpose: To evaluate and compare multiple prompt versions or types side-by-side.
    Author: Generated by PromptGPT Templater Template Developer
    Instructions: Place this file in your Templater template folder.
    Next Steps: Use the analysis to make informed decisions about prompt selection or improvement.
*/
%>
---
type: analysis
maturity: seedling
confidence: provisional
status: active
priority: medium
completion: 0
link-up: [[prompt-engineering-moc]]
tags:
  - #prompt-engineering
  - #type/analysis
  - #status/seedling
  - #year/<% tp.date.now("YYYY") %>
created: <% tp.date.now("YYYY-MM-DD") %>
modified: <% tp.date.now("YYYY-MM-DD") %>
---
# <% tp.file.title %>
## 1. Analysis Goal
<%*
const analysisGoal = await tp.system.prompt("What is the goal of this comparison (e.g., 'Compare system prompts for code summarization')?");
tR += analysisGoal;
%>
## 2. Prompts Compared
<%*
const numPrompts = parseInt(await tp.system.prompt("How many prompts are being compared?", "2"));
for (let i = 1; i <= numPrompts; i++) {
%>
### Prompt #<%= i %>
<%*
    const promptLink = await tp.system.prompt(`Link to Prompt #${i} note:`);
    const promptDesc = await tp.system.prompt(`Brief description of Prompt #${i}:`);
%>
- **Link:** <%= promptLink %>
- **Description:** <%= promptDesc %>
<% } %>
## 3. Evaluation Criteria Checklist
*(Rate each prompt on a 1-10 scale for each criterion)*

| Criterion | Description | Weight (1-5) |
| :--- | :--- | :--- |
| Accuracy | How factually correct is the output? | <% tp.system.prompt("Weight for Accuracy (1-5)", "5") %> |
| Consistency | Does it produce similar quality outputs for similar inputs? | <% tp.system.prompt("Weight for Consistency (1-5)", "4") %> |
| Clarity | Is the output easy to understand? | <% tp.system.prompt("Weight for Clarity (1-5)", "4") %> |
| Relevance | Does the output directly address the query? | <% tp.system.prompt("Weight for Relevance (1-5)", "5") %> |
| Creativity | Does it produce novel or insightful responses when appropriate? | <% tp.system.prompt("Weight for Creativity (1-5)", "3") %> |
| Safety | Does it adhere to constraints and avoid harmful output? | <% tp.system.prompt("Weight for Safety (1-5)", "5") %> |
*(Add or remove criteria as needed for your specific context)*
## 4. Comparison Table

| Prompt | Accuracy (1-10) | Consistency (1-10) | Clarity (1-10) | Relevance (1-10) | Creativity (1-10) | Safety (1-10) | Weighted Score | Notes |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
<%*
for (let i = 1; i <= numPrompts; i++) {
    tR += `| #${i} |  |  |  |  |  |  |  |  |\n`;
}
%>
*(Scores and notes to be filled manually or via a more complex script)*
## 5. Use Case Suitability Matrix
*(For different scenarios, which prompt performs best?)*

| Use Case | Best Prompt | Reason |
| :--- | :--- | :--- |
| General Task |  |  |
| High Accuracy Needed |  |  |
| Creative Task |  |  |
| Sensitive Topic |  |  |
*(Customize use cases as relevant)*
## 6. Recommendation Summary
<%*
const recommendation = await tp.system.prompt("Based on the analysis, which prompt is recommended and why?");
tR += recommendation;
%>
## 7. Next Steps
- [ ] Update the recommended prompt's metadata (e.g., increase `confidence`).
- [ ] Archive or iterate on non-recommended prompts.
- [ ] Share findings with team/stakeholders if applicable.
<% tp.file.cursor(1) %>
```

---
#### **3. Analysis Template: `03-notes/prompt-engineering/effectiveness-report.md`**
```markdown
<%*
/*
    Templater Template: Effectiveness Report
    Purpose: A detailed report on a prompt's performance in a real-world or simulated deployment.
    Author: Generated by PromptGPT Templater Template Developer
    Instructions: Place this file in your Templater template folder.
    Next Steps: Use this report to validate a prompt and update its maturity/confidence.
*/
%>
---
type: report
source: [] # To be filled with the LLM(s) used in deployment
maturity: <% tp.system.suggester(["budding", "evergreen"], ["budding", "evergreen"], false, "Set initial maturity based on pre-deployment testing:") %>
confidence: moderate
status: active
priority: high
completion: 100
link-up: [[prompt-engineering-moc]]
tags:
  - #prompt-engineering
  - #type/report
  - #status/active
  - #year/<% tp.date.now("YYYY") %>
created: <% tp.date.now("YYYY-MM-DD") %>
modified: <% tp.date.now("YYYY-MM-DD") %>
---
# <% tp.file.title %>
## 1. Prompt Identifier
<%*
const promptLink = await tp.system.prompt("Link to the prompt note being reported on (e.g., [[deployed-customer-support-prompt]])");
tR += `Link: ${promptLink}`;
%>
## 2. Deployment Context
<%*
const deploymentContext = await tp.system.prompt("Describe the context where the prompt was deployed (e.g., 'Customer support chatbot for billing inquiries'):");
tR += deploymentContext;
%>
## 3. Success Metrics Definition
*(What defines success for this prompt in this context?)*
<%*
const metricsDefinition = await tp.system.prompt("Define success metrics (e.g., 'Resolution rate > 80%', 'Average response time < 2 secs'):");
tR += `- ${metricsDefinition}`;
%>
## 4. Quantitative Results
### 4.1. Key Performance Indicators
*(Fill in actual data collected during deployment)*
- **Metric:** <% tp.system.prompt("Name of metric (e.g., Task Completion Rate)") %>  
    - **Target:** <% tp.system.prompt("Target value") %>
    - **Actual:** <% tp.system.prompt("Actual measured value") %>
    - **Variance:** <% tp.system.prompt("Difference from target") %>
- **Metric:** <% tp.system.prompt("Name of another metric (e.g., Accuracy %)") %>  
    - **Target:** <% tp.system.prompt("Target value") %>
    - **Actual:** <% tp.system.prompt("Actual measured value") %>
    - **Variance:** <% tp.system.prompt("Difference from target") %>
*(Add more KPIs as needed)*
### 4.2. Performance Stats (if applicable)
- **Total Interactions:** <% tp.system.prompt("Total number of times the prompt was used", "N/A") %>
- **Avg. Tokens/Input:** <% tp.system.prompt("Average input token count", "N/A") %>
- **Avg. Tokens/Output:** <% tp.system.prompt("Average output token count", "N/A") %>
- **Avg. Latency (secs):** <% tp.system.prompt("Average response time", "N/A") %>
## 5. Qualitative Assessment
<%*
const qualitativeAssessment = await tp.system.prompt("Provide a narrative assessment of the prompt's performance. What worked well? What were user feedback highlights?");
tR += qualitativeAssessment;
%>
## 6. Failure Pattern Analysis
*(If any issues were observed)*
- **Pattern 1:** <% tp.system.prompt("Describe a common failure mode or area for improvement (or 'None observed')") %>
- **Pattern 2:** <% tp.system.prompt("Describe another pattern (or 'N/A')") %>
## 7. Improvement Recommendations
<%*
const recommendations = await tp.system.prompt("Based on the results, what improvements are recommended for the prompt or its deployment?");
tR += `- ${recommendations}`;
%>
## 8. Final Status Update
### 8.1. Confidence & Maturity
Based on this report, how should the prompt's status be updated?
- **New Confidence Level:** <% tp.system.suggester(["speculative", "provisional", "moderate", "established", "high"], ["speculative", "provisional", "moderate", "established", "high"], false, "Set new confidence level:") %>
- **New Maturity Level:** <% tp.system.suggester(["seedling", "developing", "budding", "evergreen"], ["seedling", "developing", "budding", "evergreen"], false, "Set new maturity level:") %>
### 8.2. Archiving (if applicable)
- If this prompt is being superseded, link to the new version and update its `status` to `archived` or `deprecated`.
<% tp.file.cursor() %>
```

---
This concludes the generation of the Analysis Templates (Phase 4). I will now proceed with the Dashboard Template (Phase 5).

<thinking>
The user has responded with "continue" twice. I have completed Phase 4 (Analysis Templates). The next step according to the plan is Phase 5: Dashboard Integration.
I need to generate the following template for Phase 5:
1.  `Prompt Engineering Master Dashboard`
This template will heavily rely on DataviewJS to create queries and visualizations. I will create a comprehensive dashboard that includes the specified elements: Prompt Inventory Table, Active Optimization Projects, Testing Queue, Performance Leaderboard, Maturity Distribution Chart, Recent Updates, and Quick Actions.
I will place this file in the `06-dashboards/` folder as requested.
</thinking>
Okay, I will now generate the **Phase 5: Dashboard Integration** template.

---
### **Phase 5: Dashboard Integration**
#### **1. Dashboard Template: `06-dashboards/prompt-engineering-dashboard.md`**
`````markdown
<%*
/*
    Templater Template: Prompt Engineering Master Dashboard
    Purpose: An overview dashboard for managing the entire prompt library and workflows.
    Author: Generated by PromptGPT Templater Template Developer
    Instructions: Place this file in your `06-dashboards/` folder. Requires Dataview and DataviewJS plugins.
    Next Steps: Customize queries and filters to match your specific vault structure and tags.
*/
%>
---
type: dashboard
maturity: evergreen
confidence: established
status: active
priority: high
link-up: [[prompt-engineering-moc]]
tags:
  - #prompt-engineering
  - #type/dashboard
  - #status/active
  - #pkb
created: <% tp.date.now("YYYY-MM-DD") %>
modified: <% tp.date.now("YYYY-MM-DD") %>
---
# Prompt Engineering Master Dashboard
Welcome to the central hub for managing your prompt engineering efforts. This dashboard provides an overview of your prompt library's health, active projects, and performance.

---
## 1. Prompt Inventory Table
A list of all prompts in your knowledge base.
```dataviewjs
const prompts = dv.pages('"03-notes/prompt-engineering"')
    .where(p => p.type && (p.type.includes("prompt") || p.type.includes("permanent-note")))
    .sort(p => p.file.name, 'asc');
dv.table(
    ["Prompt", "Type", "Source LLM", "Maturity", "Confidence", "Last Modified"],
    prompts.map(p => [
        p.file.link,
        p.type || 'N/A',
        p.source ? p.source.join(', ') : 'N/A',
        p.maturity || 'N/A',
        p.confidence || 'N/A',
        p.modified ? moment(p.modified).format('YYYY-MM-DD') : 'N/A'
    ])
);
```

---
## 2. Active Optimization Projects
Prompts currently undergoing refinement.
```dataviewjs
const optProjects = dv.pages('"03-notes/prompt-engineering"')
    .where(p => p.type && p.type.includes("prompt-report") && p.status && p.status.includes("active"))
    .sort(p => p.file.name, 'asc');
if (optProjects.length > 0) {
    dv.table(
        ["Project Report", "Related Prompt", "Priority", "Completion %"],
        optProjects.map(p => [
            p.file.link,
            p.link-up && p.link-up.find(l => l.path.includes("prompt")) ? p.link-up.find(l => l.path.includes("prompt")).link : 'N/A',
            p.priority || 'N/A',
            p.completion ? p.completion + '%' : 'N/A'
        ])
    );
} else {
    dv.paragraph("*No active optimization projects found.*");
}
```

---
## 3. Testing Queue
Prompts that need to be tested or validated.
```dataviewjs
// This query assumes prompts tagged with #status/seedling or #status/needs-review need testing.
// You can adjust the criteria as needed.
const testQueue = dv.pages('"03-notes/prompt-engineering"')
    .where(p => p.type && p.type.includes("prompt") && p.status && (p.status.includes("active") || p.status.includes("needs-review")) && p.maturity && (p.maturity.includes("seedling") || p.maturity.includes("developing")))
    .sort(p => p.file.name, 'asc');
if (testQueue.length > 0) {
    dv.table(
        ["Prompt", "Maturity", "Created", "Tags"],
        testQueue.map(p => [
            p.file.link,
            p.maturity || 'N/A',
            p.created ? moment(p.created).format('YYYY-MM-DD') : 'N/A',
            p.tags ? p.tags.join(', ') : 'N/A'
        ])
    );
} else {
    dv.paragraph("*The testing queue is empty. Great work!*");
}
```

---
## 4. Performance Leaderboard
Top-performing prompts based on confidence and maturity. *(This is a proxy; real performance data would come from `Effectiveness Report` notes)*
```dataviewjs
// This is a simplified leaderboard based on maturity and confidence as proxies for performance.
// A more advanced version could aggregate scores from `Effectiveness Report` notes.
const highPerfPrompts = dv.pages('"03-notes/prompt-engineering"')
    .where(p => p.type && p.type.includes("prompt") && p.confidence && (p.confidence.includes("established") || p.confidence.includes("high")) && p.maturity && (p.maturity.includes("budding") || p.maturity.includes("evergreen")))
    .sort(p => [p.confidence, p.maturity], 'desc'); // Sort by confidence first, then maturity
if (highPerfPrompts.length > 0) {
    dv.header(3, "High-Confidence, Mature Prompts");
    dv.table(
        ["Prompt", "Confidence", "Maturity", "Last Modified"],
        highPerfPrompts.map(p => [
            p.file.link,
            p.confidence || 'N/A',
            p.maturity || 'N/A',
            p.modified ? moment(p.modified).format('YYYY-MM-DD') : 'N/A'
        ])
    );
} else {
    dv.paragraph("*No prompts with 'established' or 'high' confidence and 'budding' or 'evergreen' maturity found yet.*");
}
```

---
## 5. Maturity Distribution Chart
Visual breakdown of prompt library maturity.
```dataviewjs
const maturityData = dv.pages('"03-notes/prompt-engineering"')
    .where(p => p.type && p.type.includes("prompt"))
    .groupBy(p => p.maturity || 'undefined')
    .map(group => ({ maturity: group.key, count: group.rows.length }))
    .array();
// Simple text-based bar chart
dv.header(3, "Prompt Maturity Distribution");
maturityData.forEach(item => {
    const bar = "‚ñà".repeat(item.count); // Simple block character bar
    dv.paragraph(`- **${item.maturity || 'Unspecified'} (${item.count})**: \`${bar}\``);
});
```

---
## 6. Recent Updates
Last 10 modified prompt-related notes.
```dataviewjs
const recentUpdates = dv.pages('"03-notes/prompt-engineering"')
    .where(p => p.type && (p.type.includes("prompt") || p.type.includes("prompt-report") || p.type.includes("analysis") || p.type.includes("permanent-note")))
    .sort(p => p.modified, 'desc')
    .limit(10);
if (recentUpdates.length > 0) {
    dv.table(
        ["Note", "Type", "Modified"],
        recentUpdates.map(p => [
            p.file.link,
            p.type || 'N/A',
            p.modified ? moment(p.modified).format('YYYY-MM-DD HH:mm') : 'N/A'
        ])
    );
} else {
    dv.paragraph("*No recent updates found.*");
}
```

---
## 7. Quick Actions
Buttons/links to launch common workflows.
- **[[Create New System Prompt|ü™Ñ Create New System Prompt]]** (Requires `system-prompt-creator.md` template)
- **[[Run Prompt Optimization|üõ†Ô∏è Optimize an Existing Prompt]]** (Requires `prompt-optimization-workflow.md` template)
- **[[Set up A/B Test|‚öñÔ∏è A/B Test Prompts]]** (Requires `ab-testing-framework.md` template)
- **[[Document Test Results|üìã Document Test Results]]** (Requires `test-results-documentation.md` template)
- **[[View Prompt Components Library|üß© Prompt Components Library]]** (Link to `prompt-components.md`)
*(Note: These links assume you have created notes with these exact names that either contain the templates or link to them. You can customize these links to point directly to your template files if your Templater setup allows.)*

---
*Dashboard last updated: <% tp.date.now("YYYY-MM-DD HH:mm") %>*
`````

---
This concludes the generation of the Dashboard Template (Phase 5). I will now proceed with the final Integration Patterns guide (Phase 6).


<thinking>
The user has responded with "continue". I have completed Phase 5 (Dashboard Template). The final step according to the plan is Phase 6: Integration Patterns.
I need to generate the following for Phase 6:
1.  `Prompt Engineering Workflow Guide` - This note will contain the detailed workflows and Mermaid flowcharts.
This guide will tie together all the templates created in the previous phases, showing how they fit into complete workflows for prompt engineering tasks. I will place this file in the `03-notes/prompt-engineering/` folder.
</thinking>
Okay, I will now generate the final component, the **Phase 6: Integration Patterns** guide.

---
### **Phase 6: Integration Patterns**
#### **1. Integration Guide: `03-notes/prompt-engineering/workflow-guide.md`**
```markdown
<%*
/*
    Templater Note: Prompt Engineering Workflow Guide
    Purpose: To document complete workflows showing how templates work together.
    Author: Generated by PromptGPT Templater Template Developer
    Instructions: Place this file in your `03-notes/prompt-engineering/` folder.
    Next Steps: Use this guide as a reference for conducting prompt engineering tasks.
*/
%>
---
type: guide
maturity: evergreen
confidence: established
status: active
priority: high
link-up: [[prompt-engineering-moc]]
tags:
  - #prompt-engineering
  - #type/guide
  - #status/active
  - #pkb
created: <% tp.date.now("YYYY-MM-DD") %>
modified: <% tp.date.now("YYYY-MM-DD") %>
---
# Prompt Engineering Workflow Guide
This guide outlines complete, end-to-end workflows for common prompt engineering tasks, demonstrating how the various templates integrate to form a systematic approach.

---
## Workflow 1: New Prompt Creation & Initial Testing
This workflow covers creating a prompt from scratch and doing initial validation.
### Steps:
1.  **Initiate:** Use the **[[system-prompt-creator]]** (or `user-prompt-generator`) template to create a new prompt note.
2.  **Create:** Fill in the details in the template, leveraging components from `prompt-components.md`.
3.  **Save:** Save the new prompt note in `03-notes/prompt-engineering/`.
4.  **Test Plan:** Define a few simple test cases manually or using a basic note.
5.  **Execute Test:** Run the prompt with your test cases.
6.  **Document Test:** Use the **[[test-results-documentation]]** template to record the outcomes of your initial tests.
7.  **Analyze:** Review the `Test Results Documentation`.
8.  **Iterate (if needed):** If results are unsatisfactory, use the **[[debugging-template]]** or **[[prompt-optimization-workflow]]** to refine the prompt.
9.  **Finalize:** Once initial testing is successful, update the prompt's metadata (`maturity` to `developing`, `confidence` to `moderate`).
### Mermaid Flowchart:
```mermaid
flowchart TD
    A[Start: New Prompt Idea] --> B[Use System/User Prompt Creator Template]
    B --> C[Fill & Save New Prompt Note]
    C --> D[Define Initial Test Cases]
    D --> E[Run Prompt with Test Cases]
    E --> F[Use Test Results Documentation Template]
    F --> G{Results Satisfactory?}
    G -->|No| H[Use Debugging/Optimization Template]
    H --> I[Refine Prompt]
    I --> E
    G -->|Yes| J[Update Prompt Metadata]
    J --> K[End: Prompt Ready for Further Use]
```

---
## Workflow 2: Systematic Prompt Optimization
This workflow is for significantly improving an existing prompt.
### Steps:
1.  **Identify:** Find a prompt that needs improvement (e.g., from the `Testing Queue` on the dashboard).
2.  **Initiate:** Use the **[[prompt-optimization-workflow]]** template to create a new optimization report note.
3.  **Analyze:** Fill in the `Problem Identification` and `Improvement Hypothesis` sections.
4.  **Modify:** Create the `Modified Prompt Version` in the report.
5.  **Plan Test:** Define the `Test Plan` with clear inputs and expected outputs.
6.  **Execute Test:** Run both the original and modified prompts with the test cases.
7.  **Document Results:** Fill in the `Results Comparison` table.
8.  **Decide:** Based on results, make a decision in the `Decision` section.
9.  **Act:**
    *   If **Accept**: Apply the changes to the original prompt note. Update its metadata.
    *   If **Reject**: Close the report. Consider a new hypothesis.
    *   If **Further Iterate**: Refine the hypothesis and repeat steps.
10. **Update Report:** Set the `maturity` of the optimization report and link it appropriately.
### Mermaid Flowchart:
```mermaid
flowchart TD
    A[Start: Identify Prompt for Improvement] --> B[Use Prompt Optimization Workflow Template]
    B --> C[Analyze Problem & Form Hypothesis]
    C --> D[Modify Prompt in Report]
    D --> E[Define Test Plan]
    E --> F[Run Tests on Old & New Prompts]
    F --> G[Document Results in Comparison Table]
    G --> H{Is Modification Better?}
    H -->|No| I[Reject Change, Close Report]
    H -->|Needs More Work| J[Refine Hypothesis]
    J --> D
    H -->|Yes| K[Accept Change]
    K --> L[Apply Changes to Original Prompt]
    L --> M[Update Prompt & Report Metadata]
    M --> N[End: Optimization Complete]
```

---
## Workflow 3: Comparative Analysis (A/B Testing)
This workflow is for choosing between two prompt variants.
### Steps:
1.  **Identify:** Have two prompt variants ready for comparison.
2.  **Initiate:** Use the **[[ab-testing-framework]]** template to create a new A/B test report note.
3.  **Define:** Fill in the `Test Objective`, details for `Variant A` and `Variant B`.
4.  **Methodology:** Describe the `Testing Methodology`.
5.  **Cases:** List the `Test Cases`.
6.  **Execute:** Run both variants against all test cases.
7.  **Record:** Populate the `Results Matrix` and `Evaluation Criteria & Scoring`.
8.  **Analyze:** Review the results and any statistical notes.
9.  **Decide:** Make a final `Decision` on the winner and provide a `Rationale`.
10. **Act:**
    *   Integrate the winning variant into your main prompt line.
    *   Update the winner's metadata.
    *   Archive or shelve the losing variant.
11. **Update Report:** Finalize the A/B test report's metadata.
### Mermaid Flowchart:
```mermaid
flowchart TD
    A[Start: Two Prompt Variants] --> B[Use A/B Testing Framework Template]
    B --> C[Define Objective, Variants, Methodology]
    C --> D[List Test Cases]
    D --> E[Run Tests on Variant A & B]
    E --> F[Record Outputs & Scores in Matrix]
    F --> G[Analyze Results]
    G --> H{Which Variant is Better?}
    H -->|Tie/Inconclusive| I[Document Findings, May Need Rerun]
    H -->|A| J[Integrate A, Archive B]
    H -->|B| K[Integrate B, Archive A]
    J --> L[Update Metadata for A & Report]
    K --> L
    L --> M[End: Winner Chosen & Integrated]
```

---
## Workflow 4: Library Maintenance & Review
This workflow ensures the prompt library stays healthy and up-to-date.
### Steps:
1.  **Review Dashboard:** Regularly check the **[[prompt-engineering-dashboard]]**.
2.  **Check Maturity:** Look for prompts that have been `budding` or `developing` for a while.
3.  **Validate:** For mature prompts, consider running an `Effectiveness Report` if deployed.
4.  **Update:** Review notes with `#status/needs-review` and update their status or content.
5.  **Archive:** Identify deprecated or superseded prompts and change their `status` to `archived` or `deprecated`.
6.  **Document Learnings:** Add insights to the relevant prompt's `Version Tracker` or a general `Lessons Learned` MOC.
7.  **Plan:** Use the dashboard's `Active Optimization Projects` and `Testing Queue` to plan next steps.
*(This workflow is more cyclical and ongoing, less linear than the others.)*

---
## Template Invocation Instructions
To use these templates effectively:
-   **Templater Plugin:** Ensure the Templater plugin is installed and configured in Obsidian.
-   **Template Folder:** Place all `.md` template files (e.g., `system-prompt-creator.md`) in a folder designated in Templater's settings (e.g., `_templates`).
-   **Creating Notes:** To create a new note from a template:
    1.  Right-click in the file explorer or use the command palette.
    2.  Select `Templater: Create new note from template`.
    3.  Choose the desired template.
    4.  Follow the prompts within the template.
## Example Scenarios
*(These can be expanded with step-by-step walkthroughs)*
-   **Scenario 1: Building a Code Explainer Prompt**
    -   Use `system-prompt-creator` -> Initial tests with `test-results-documentation` -> Optimize with `prompt-optimization-workflow` -> Validate with `effectiveness-report` if deployed.
-   **Scenario 2: Choosing Between Two Summarization Prompts**
    -   Set up `ab-testing-framework` -> Execute tests -> Analyze results -> Decide winner -> Integrate.
This guide and the associated templates provide a robust framework for managing prompt engineering tasks systematically within your PKB.
```
<% tp.file.cursor() %>
```


---
### **üéâ Generation Complete!**
This concludes the generation of all templates and components as outlined in the original, very detailed request. Here is a summary of what has been delivered across all phases:
**Phase 1: Component Library**
*   `03-notes/prompt-engineering/prompt-components.md`
**Phase 2: Capture Templates**
*   `03-notes/prompt-engineering/system-prompt-creator.md`
*   `03-notes/prompt-engineering/user-prompt-generator.md`
*   `03-notes/prompt-engineering/prompt-chain-builder.md`
*   `03-notes/prompt-engineering/few-shot-example-template.md`
**Phase 3: Iteration Templates**
*   `03-notes/prompt-engineering/prompt-optimization-workflow.md`
*   `03-notes/prompt-engineering/ab-testing-framework.md`
*   `03-notes/prompt-engineering/debugging-template.md`
*   `03-notes/prompt-engineering/version-tracker.md`
**Phase 4: Analysis Templates**
*   `03-notes/prompt-engineering/test-results-documentation.md`
*   `03-notes/prompt-engineering/comparative-prompt-analysis.md`
*   `03-notes/prompt-engineering/effectiveness-report.md`
**Phase 5: Dashboard Integration**
*   `06-dashboards/prompt-engineering-dashboard.md`
**Phase 6: Integration Patterns**
*   `03-notes/prompt-engineering/workflow-guide.md`
All templates have been created with the specified metadata, folder paths, and Templater syntax. They are designed to integrate seamlessly with your Obsidian PKB, leveraging Dataview for powerful querying and visualization.