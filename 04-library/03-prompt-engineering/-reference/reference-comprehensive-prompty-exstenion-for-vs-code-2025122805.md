---
id: "20251228053404"
type: "reference"
source: "claude-sonnet"
status: "read"
confidence: "provisional"
maturity: "seedling"
priority: "low"
next-review: "2026-01-04"
review-count: 0
last-reviewed:
review-interval: 7
created: "2025-12-28T05:34:04"
modified: "2025-12-28T05:34:04"
week: "[[2026-W52]]"
month: "2025-12"
quarter: "2025-Q4"
year: "2025"
link-up:
 - "[[prompt-engineering-moc]]"
link-related:
 - "[[2025-12-28|Daily-Note]]"
tags:
 - "type/reference"
 - "source/claude-sonnet"
 - "maturity/seedling"
 - "confidence/provisional"
 - "status/read"
 - "priority/low"
 - "year/2025"
 - "advanced-prompting/multi-modal"
 - "information-architecture"
 - "prompt-engineering"
 - "prompt-pattern/framework"
 - "skill-acquisition"
 - "learning-processes"
aliases:
 - "prompty"
 - "Prompty"
 - "Comprehensive Reference: Prompty Exstension for VS Code"
title: "Comprehensive Reference: Prompty Exstension for VS Code"
---
### ðŸ“– Extracted Definitions From Cognitive Science
```dataviewjs
try {
 // Get the current file
 const currentPage = dv.current();
 // Load the content of the current file
 const content = await dv.io.load(currentPage.file.path);
 // Storage for definitions in current file
 let allDefinitions = [];
 // Extract bracketed inline fields from current file content
 const bracketedFieldRegex = /\[\*\*([^*]+?)\*\*::\s*([^\]]+?)\]/g;
 let match;
 while ((match = bracketedFieldRegex.exec(content)) !== null) {
  allDefinitions.push({
   key: match[1].trim(), // This is the clean term without ** markdown
   value: match[2].trim()
  });
 }
 // Display results
 if (allDefinitions.length > 0) {
  dv.header(3, `ðŸ“š Definitions in Current File (${allDefinitions.length} total)`);
  // Group by first letter (using the clean key)
  const grouped = {};
  allDefinitions.forEach(d => {
   const firstLetter = d.key[0].toUpperCase();
   if (!grouped[firstLetter]) grouped[firstLetter] = [];
   grouped[firstLetter].push(d);
  });
  // Sort letters alphabetically
  const sortedLetters = Object.keys(grouped).sort();
  // Display grouped results
  for (let letter of sortedLetters) {
   dv.header(4, `${letter} (${grouped[letter].length} terms)`);
   dv.table(
    ["ðŸ”‘ Term", "ðŸ“ Definition"],
    grouped[letter].map(d => [
     `**${d.key}**`,
     d.value
    ])
   );
   dv.paragraph(""); // Add spacing between groups
  }
 } else {
  dv.paragraph(`*No bracketed inline fields found in current file.*`);
 }
} catch (error) {
 console.error("Error in definitions script:", error);
 dv.paragraph("*Error loading definitions. Check console for details.*");
}
```
---





# Foundational Understanding
> [!definition] # Definition
> [**Note Title**:: [[**Comprehensive Reference: Prompty Exstension for VS Code**]]]
> [**Prompt Used**:: ]
> ##### [âœ…`= dateformat(this.file.ctime, "MMMM dd, yyyy")` - Initial Creation]


# Prompty VS Code Extension

> [!abstract] Executive Overview
> <span style='color: #FFC700;'>**Prompty**</span> is a Microsoft-developed VS Code extension that standardizes LLM prompt development through a unified <span style='color: #72FFF1;'>`.prompty`</span> file formatâ€”combining prompt content, model configuration, and sample data in a single version-controlled asset. While designed primarily for [[OpenAI]] and [[Azure OpenAI]] workflows, it accelerates the "developer inner loop" of prompt engineering through rapid testing, preview capabilities, and code generation for production frameworks.

> [!important] Critical Context for Your Setup
> <span style='color: #FF00DC;'>**Key Limitation:**</span> Prompty does NOT natively integrate with [[Anthropic]] (Claude) or Google (Gemini) models. Your current setup uses [[Claude Code]] and [[Gemini Code Assist]], which are **separate tools** from Prompty. This reference note explains what Prompty does, its limitations, and strategic integration approaches for your multi-LLM prompt engineering library.

## ðŸ“‘ Table of Contents

1. [Core Concepts & Architecture](#core-concepts--architecture)
2. [The .prompty File Format](#the-prompty-file-format)
3. [VS Code Integration Features](#vs-code-integration-features)
4. [Workflow & Development Patterns](#workflow--development-patterns)
5. [Limitations & Provider Support](#limitations--provider-support)
6. [Integration Strategy for Your Setup](#integration-strategy-for-your-setup)
7. [PKB & Obsidian Integration](#pkb--obsidian-integration)

---

## Core Concepts & Architecture

### What Is Prompty?

> [!definition] Prompty Asset Class
> [**Prompty**:: a file format specification (`.prompty`) and VS Code extension designed to enhance observability, understandability, and portability for LLM prompt development by unifying prompt content, execution context, and sample data in a single version-controlled markdown asset.]^verified-stable

<span style='color: #27FF00;'>**Three-Component Architecture:**</span>

1. **Specification** â€” The `.prompty` file format (markdown with YAML frontmatter)
2. **Tooling** â€” VS Code extension with preview, execution, and code generation
3. **Runtime** â€” Executable code in Python and C# for integration into applications

> [!principle-point] Accelerating the Developer Inner Loop
> Prompty's primary goal is to <span style='color: #FFC700;'>reduce friction in the iterative prompt engineering cycle</span>: write â†’ test â†’ observe â†’ refine. By colocating all prompt components in a single file, developers avoid context switching between configuration files, test data, and prompt content.

### Design Philosophy

The extension treats <span style='color: #FFC700;'>**prompts as first-class assets**</span>â€”not strings scattered across codebases, but standalone artifacts with:

- **Version control** via Git (markdown format is diff-friendly)
- **Testability** through embedded sample data
- **Portability** across frameworks ([[LangChain]], [[Semantic Kernel]], [[Prompt Flow]])
- **Observability** via verbose request/response tracing

---

## The .prompty File Format

### File Structure Anatomy

A `.prompty` file consists of two main parts:

```markdown
---
[YAML FRONTMATTER: Metadata & Configuration]
---

[MARKDOWN BODY: Prompt Template with Jinja2]
```

### YAML Frontmatter Specification

> [!what-this-does] Frontmatter Purpose
> The frontmatter defines **who** (model configuration), **what** (input/output schemas), **how** (parameters), and **test data** (sample values) for the prompt execution.

**Complete Frontmatter Structure:**

```yaml
---
name: Basic Prompt                          # Descriptive identifier
description: A comprehensive prompt example  # Purpose documentation
version: 1.0.0                              # Semantic versioning
authors:                                     # Attribution
  - your-name
tags:                                        # Categorization
  - category
  - use-case

model:
  api: chat                                 # API type: chat | completion | embeddings
  configuration:
    type: azure_openai                      # Provider: azure_openai | openai | serverless
    azure_endpoint: ${env:AZURE_OPENAI_ENDPOINT}  # Environment variable syntax
    azure_deployment: gpt-4o                # Deployment/model name
    api_key: ${env:AZURE_OPENAI_API_KEY}   # API authentication
    api_version: 2024-10-21                 # API version
  parameters:
    max_tokens: 3000                        # Token limit
    temperature: 0.7                        # Randomness (0.0-2.0)
    top_p: 0.95                            # Nucleus sampling
    response_format: { "type": "json_object" }  # Optional: JSON mode

inputs:                                      # Input schema definition
  firstName:
    type: string
    description: User's first name
  question:
    type: string
    description: Question to answer

sample:                                      # Test data for preview/execution
  firstName: Jane
  lastName: Doe
  question: What is the meaning of life?
---
```

> [!helpful-tip] Environment Variables Best Practice
> <span style='color: #27FF00;'>Always use `${env:VAR_NAME}` syntax</span> for API keys and endpoints. Store credentials in:
> - `.env` file in project root (add to `.gitignore`)
> - VS Code settings for team-shared configurations
> - System environment variables for global access

### Prompt Template Body

The markdown body uses <span style='color: #72FFF1;'>**Jinja2 templating**</span> for dynamic variable insertion:

```markdown
---

# System
You are an AI assistant who helps people find information.
As the assistant, you answer questions briefly, succinctly, 
and in a personable manner using markdown and even add some 
personal flair with appropriate emojis.

# Context
You are helping {{firstName}} {{lastName}} to find answers 
to their questions. Use their name to address them in your responses.

# User
{{question}}
```

**Jinja2 Syntax Patterns:**

- `{{variable}}` â€” Variable interpolation
- `{% if condition %}...{% endif %}` â€” Conditional logic
- `{% for item in list %}...{% endfor %}` â€” Loops
- `{{ list | join(', ') }}` â€” Filters

> [!example] Multi-Modal Input Support
> Prompty supports <span style='color: #72FFF1;'>images and other media</span> through Jinja2 template extensions, useful for vision-capable models like GPT-4o or Claude 3.5 Sonnet (if using proxy).

---

## VS Code Integration Features

### Core Extension Capabilities

> [!what-this-does] Prompty Extension Features
> The VS Code extension provides an **interactive prompt playground** directly in your editor, eliminating the need to context-switch to web interfaces or external tools.

**Key Features:**

1. **Quick Creation** â€” Right-click in Explorer â†’ "New Prompty" generates template
2. **Live Preview** â€” View rendered prompt with sample data substitution in real-time
3. **Execution** â€” Run prompts with <span style='color: #72FFF1;'>F5</span> or play button, see output in two views:
   - **Concise Output** â€” Final response only
   - **Verbose Output** â€” Full HTTP request/response trace for debugging
4. **Model Switching** â€” Quick-select between configured models via status bar
5. **Code Generation** â€” Right-click `.prompty` â†’ Generate integration code for:
   - [[Prompt Flow]] (Azure AI Studio)
   - [[LangChain]] (Python/JS frameworks)
   - [[Semantic Kernel]] (.NET framework)

### Model Configuration Hierarchy

<span style='color: #FFC700;'>**Configuration Precedence**</span> (highest to lowest):

1. **Inline** â€” `model:` block in `.prompty` file frontmatter
2. **Workspace** â€” `settings.json` in `.vscode/` folder (team-shared via Git)
3. **User** â€” Global VS Code settings (personal defaults)
4. **Extension Default** â€” Fallback Azure OpenAI configuration

> [!methodology-and-sources] Configuration Strategy
> For <span style='color: #27FF00;'>team collaboration</span>: Use workspace-level `settings.json` with environment variable placeholders. For <span style='color: #FFC700;'>personal experimentation</span>: Use user-level settings or inline configuration.

**Workspace Configuration Example** (`.vscode/settings.json`):

```json
{
  "prompty.modelConfigurations": [
    {
      "name": "default",
      "type": "azure_openai",
      "azure_endpoint": "${env:AZURE_OPENAI_ENDPOINT}",
      "azure_deployment": "gpt-4o",
      "api_key": "${env:AZURE_OPENAI_API_KEY}"
    },
    {
      "name": "fast",
      "type": "openai",
      "api_key": "${env:OPENAI_API_KEY}",
      "model": "gpt-4o-mini"
    }
  ]
}
```

---

## Workflow & Development Patterns

### The Developer Inner Loop

> [!methodology-and-sources] Iterative Prompt Engineering Cycle
> Prompty optimizes the <span style='color: #FFC700;'>write â†’ test â†’ observe â†’ refine</span> cycle:
>
> 1. **Write** prompt in markdown with Jinja2 templates
> 2. **Test** with sample data via live preview
> 3. **Observe** output and verbose API trace
> 4. **Refine** prompt based on results
> 5. **Version** changes in Git
> 6. **Generate** production code when satisfied

### Version Control Integration

<span style='color: #27FF00;'>**Git-Friendly Design:**</span>

- **Markdown format** creates readable diffs in pull requests
- **YAML frontmatter** enables clear configuration tracking
- **Environment variables** prevent credential leaks
- **Sample data** in file enables reproducible testing

> [!example] Git Workflow Pattern
>
> ```bash
> # Create feature branch for prompt iteration
> git checkout -b prompts/improve-customer-support
> 
> # Edit basic.prompty, test iterations
> # Commit when satisfied with results
> git add prompts/customer-support.prompty
> git commit -m "Improve customer support prompt tone and structure"
> 
> # Push for team review
> git push origin prompts/improve-customer-support
> ```

### Production Code Generation

> [!what-this-does] Framework Integration
> Right-click a `.prompty` file â†’ select target framework â†’ extension generates boilerplate code with:
> - Prompt loading from file
> - Variable injection
> - Model configuration
> - Response handling

**Generated Code Structure** (Python + Prompt Flow example):

```python
from promptflow.core import Prompty

# Load prompty with model override
override_model = {
    "configuration": {
        "api_key": "${env:AZURE_OPENAI_API_KEY}",
        "azure_endpoint": "${env:AZURE_OPENAI_ENDPOINT}"
    },
    "parameters": {"max_tokens": 512}
}

prompty = Prompty.load(
    source="customer-support.prompty", 
    model=override_model
)

# Execute with runtime data
result = prompty(
    firstName="Alice",
    question="How do I reset my password?"
)

print(result)
```

---

## Limitations & Provider Support

### Native Provider Support

> [!warning] Primary Limitation for Your Setup
> <span style='color: #FF00DC;'>**Prompty officially supports ONLY:**</span>
> - [[OpenAI]] (direct API)
> - [[Azure OpenAI]] (Azure deployment)
> - **Serverless** (GitHub Marketplace models)
> 
> <span style='color: #FF00DC;'>**NOT supported:**</span>
> - [[Anthropic]] (Claude models)
> - [[Google]] (Gemini models)
> - [[Cohere]], [[Mistral]], [[Groq]], etc.

### Workaround: Proxy Layers

<span style='color: #FFC700;'>**Potential Extension Path:**</span>

Use [[LiteLLM]] as an OpenAI-compatible proxy to route Prompty requests to other providers:

```bash
# Install LiteLLM proxy
pip install 'litellm[proxy]'

# Start proxy server supporting multiple providers
litellm --model anthropic/claude-sonnet-4-5-20250929 \
        --api_key $ANTHROPIC_API_KEY

# Configure Prompty to point to local proxy
# In .prompty file:
model:
  configuration:
    type: openai
    base_url: http://localhost:4000
    api_key: ${env:ANTHROPIC_API_KEY}
```

> [!warning] Complexity Trade-Off
> This approach adds <span style='color: #FF00DC;'>infrastructure complexity</span> and may not support all Prompty features (preview, tracing). Consider whether simpler approaches (direct API calls, alternative tools) better serve your workflow.

### Comparison with [[Claude Code]]

| Feature | Prompty | [[Claude Code]] |
|---------|---------|-----------------|
| **Primary Use** | Prompt asset management | Agentic coding assistant |
| **Provider** | Microsoft (OpenAI focus) | Anthropic (Claude only) |
| **File Format** | `.prompty` (standardized) | Terminal-based interaction |
| **Version Control** | Git-friendly `.prompty` files | Conversation history |
| **Code Generation** | Framework integration code | Direct codebase editing |
| **Integration** | VS Code extension | VS Code extension + terminal |
| **Team Collaboration** | Via `.prompty` file sharing | Via conversation sharing |

<span style='color: #FFC700;'>**Key Insight:**</span> Prompty and Claude Code serve **complementary but non-overlapping** purposes. Prompty = prompt asset standardization; Claude Code = AI pair programming.

---

## Integration Strategy for Your Setup

### Current Tool Landscape

Your setup involves:

1. **[[Claude Code]]** â€” Anthropic's agentic coding assistant (VS Code + terminal)
2. **[[Gemini Code Assist]]** â€” Google's coding assistant
3. **[[Obsidian]] PKB** â€” Markdown-based knowledge base run through VS Code
4. **Prompt Engineering Library** â€” Cognitive science-backed prompt collection

### Strategic Options for Prompty

> [!methodology-and-sources] Three Integration Approaches

**Option A: OpenAI-Only Workflow** (Simplest)
- Use Prompty exclusively for [[OpenAI]]/[[Azure OpenAI]] prompts
- Maintain separate workflows for [[Claude]] and [[Gemini]] prompts
- **Pro:** No complexity, works out-of-box
- **Con:** Fragmented prompt management, doesn't leverage Anthropic strengths

**Option B: Unified Format, Manual Execution** (Recommended)
- Adopt `.prompty` **file format** for ALL prompts (OpenAI, Claude, Gemini)
- Use Prompty extension only for OpenAI prompts
- Manually execute Claude/Gemini prompts via API or CLI
- Store all `.prompty` files in [[Obsidian]] vault for version control
- **Pro:** Standardized format, version control, PKB integration
- **Con:** No preview/execution for non-OpenAI prompts

**Option C: LiteLLM Proxy Layer** (Advanced)
- Deploy [[LiteLLM]] proxy to translate Prompty â†’ Claude/Gemini
- Use Prompty extension for all providers
- **Pro:** Unified workflow, full Prompty features
- **Con:** Infrastructure complexity, potential feature limitations

### Recommended Approach for Your PKB

> [!helpful-tip] Hybrid Format Strategy
> <span style='color: #27FF00;'>Adopt the `.prompty` format as your **standard prompt asset structure**</span>, even for non-OpenAI prompts. Benefits:
>
> 1. **Version Control** â€” Git-friendly markdown format
> 2. **Discoverability** â€” All prompts follow same schema
> 3. **Documentation** â€” Frontmatter captures metadata
> 4. **Portability** â€” Easy migration if tooling improves
> 5. **PKB Integration** â€” Markdown files work natively in [[Obsidian]]

**Implementation Pattern:**

```cs
your-pkb/
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ anthropic/
â”‚   â”‚   â”œâ”€â”€ code-review.prompty          # Claude-specific
â”‚   â”‚   â”œâ”€â”€ documentation.prompty
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ openai/
â”‚   â”‚   â”œâ”€â”€ summarization.prompty        # Works with Prompty extension
â”‚   â”‚   â”œâ”€â”€ classification.prompty
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ gemini/
â”‚   â”‚   â”œâ”€â”€ multimodal.prompty           # Gemini-specific
â”‚   â”‚   â”œâ”€â”€ long-context.prompty
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ _templates/
â”‚       â””â”€â”€ base.prompty                  # Template for new prompts
```

> [!example] Modified .prompty for Non-OpenAI Providers
> Create a custom frontmatter field to document target provider:
>
> ```yaml
> ---
> name: Code Review Assistant
> provider: anthropic                    # Custom field
> model_id: claude-sonnet-4-5-20250929  # Custom field
> execution: manual                      # Custom field: manual | api | cli
> 
> model:
>   api: chat
>   configuration:
>     # Leave as template - won't execute via Prompty
>     type: openai_compatible
>   parameters:
>     max_tokens: 4000
>     temperature: 0.3
> 
> sample:
>   code: |
>     def example():
>         pass
> ---
> # System
> You are an expert code reviewer...
> ```

---

## PKB & Obsidian Integration

### Prompty as PKB Assets

> [!what-this-does] Markdown Compatibility
> `.prompty` files are <span style='color: #27FF00;'>**pure markdown**</span> with YAML frontmatterâ€”exactly what [[Obsidian]] expects. They integrate seamlessly into your PKB as:
> - **Prompt library** â€” Searchable, version-controlled prompt collection
> - **Documentation** â€” Self-documenting with frontmatter metadata
> - **Templates** â€” Reusable prompt patterns
> - **Knowledge artifacts** â€” Linked to relevant PKB notes

### Obsidian-Specific Integration

**Strategy 1: Dedicated Prompt Vault Section**

```cs
your-obsidian-vault/
â”œâ”€â”€ 00-Inbox/
â”œâ”€â”€ 01-Projects/
â”œâ”€â”€ 02-Areas/
â”œâ”€â”€ 03-Resources/
â”‚   â””â”€â”€ Prompts/                        # Prompt library
â”‚       â”œâ”€â”€ _Index.md                   # MOC for prompt navigation
â”‚       â”œâ”€â”€ anthropic/
â”‚       â”œâ”€â”€ openai/
â”‚       â””â”€â”€ gemini/
â”œâ”€â”€ 04-Archives/
â””â”€â”€ templates/
```

**Strategy 2: Dataview Queries for Prompt Discovery**

Create a Prompt Library MOC using [[Dataview]]:

````markdown
# Prompt Library

## By Provider

### Anthropic (Claude)
```dataview
LIST
FROM "03-Resources/Prompts/anthropic"
WHERE file.extension = "prompty"
SORT file.name
```

### OpenAI
```dataview
TABLE 
  model.parameters.temperature as Temperature,
  model.parameters.max_tokens as "Max Tokens"
FROM "03-Resources/Prompts/openai"
WHERE file.extension = "prompty"
```
````

> [!helpful-tip] Prompty Metadata as Dataview Fields
> Since Prompty uses YAML frontmatter, [[Dataview]] can query on:
> - `name`, `description`, `version`, `tags`
> - `model.api`, `model.configuration.type`
> - `model.parameters.*`
> - Any custom fields you add

**Strategy 3: Wiki-Link Integration**

Reference prompts from project notes:

```markdown
# Project: Customer Support Chatbot

## Prompt Engineering Notes

Current iteration uses [[customer-support.prompty]] with 
temperature=0.7 for balance between consistency and personality.

Related prompts:
- [[error-handling.prompty]] for exception scenarios
- [[escalation.prompty]] for handoff to human agents
```

### Version Control Workflow

> [!methodology-and-sources] Git + Obsidian + Prompty Workflow
> 1. **Store** `.prompty` files in Obsidian vault (under version control)
> 2. **Open vault** in VS Code (where Obsidian and Prompty extensions coexist)
> 3. **Edit** prompts using Prompty features (OpenAI) or markdown (others)
> 4. **Commit** changes to Git
> 5. **Link** prompts from Obsidian notes using wiki-links
> 6. **Query** prompt library using Dataview

**Dual-Editor Setup:**

```cs
Terminal 1:  obsidian vault-directory/
Terminal 2:  code vault-directory/
```

This allows:

- <span style='color: #72FFF1;'>Obsidian</span> for note-taking, knowledge graph visualization
- <span style='color: #72FFF1;'>VS Code</span> for Prompty execution, Git operations, Claude Code

---

## ðŸŽ¯ Synthesis & Mastery

> [!the-philosophy] Prompty's Design Philosophy
> Prompty embodies the principle that <span style='color: #FFC700;'>**prompts are code**</span> and deserve the same engineering rigor: version control, testing, documentation, and reproducibility. By treating prompts as first-class assets rather than scattered strings, it enables:
>
> - **Collaboration** through standardized format
> - **Iteration** through rapid testing cycles
> - **Production** transition via code generation
> - **Observability** through request tracing

> [!mental-model-anchor] Prompt Engineering as Software Engineering
> The `.prompty` format parallels software engineering practices:
> - **Specification** (frontmatter) = function signature + configuration
> - **Implementation** (template) = function body
> - **Sample data** = unit test fixtures
> - **Execution** = function invocation
> - **Version control** = Git workflow

> [!application-context] When to Use Prompty
> <span style='color: #27FF00;'>**Ideal for:**</span>
> - [[OpenAI]] or [[Azure OpenAI]] prompt development
> - Team collaboration on prompt engineering
> - Production deployment via [[Prompt Flow]], [[LangChain]], [[Semantic Kernel]]
> - Rapid prototyping with sample data
> - Observable, traceable prompt iterations
> 
> <span style='color: #FF00DC;'>**Not ideal for:**</span>
> - [[Anthropic]] (Claude) exclusive workflows (use [[Claude Code]] instead)
> - [[Google]] (Gemini) exclusive workflows (no native support)
> - Non-technical prompt iteration (web UIs may be simpler)
> - Already-established workflows with other prompt management tools

---

## ðŸ”— PKB Integration

> [!connections-and-links] Related Concepts
> - [[Prompt Engineering]] â€” Broader discipline of LLM interaction design
> - [[Version Control]] â€” Git workflows for prompt asset management
> - [[Markdown]] â€” Format foundation enabling PKB integration
> - [[Jinja2]] â€” Templating engine for dynamic prompt content
> - [[Claude Code]] â€” Complementary tool for agentic coding
> - [[Obsidian]] â€” PKB platform for prompt library storage
> - [[LLM Development Tools]] â€” Ecosystem of prompt engineering platforms
> - [[YAML]] â€” Metadata format for structured configuration

> [!atomic-candidates] Concepts for Extraction
> - **Jinja2 Template Syntax** â€” Detailed templating patterns and filters
> - **LiteLLM Proxy Configuration** â€” Multi-provider routing setup
> - **Prompt Versioning Best Practices** â€” Git strategies for prompt assets
> - **Azure OpenAI Configuration** â€” Deployment and authentication patterns

> [!synthesis-opportunities] Cross-Domain Bridges
> - **Cognitive Load Theory Ã— Prompt Design** â€” How prompt structure affects LLM "comprehension"
> - **Zettelkasten Ã— Prompt Library** â€” Atomic prompt patterns as knowledge units
> - **Spaced Repetition Ã— Prompt Testing** â€” Iterative refinement schedules
> - **Information Architecture Ã— Model Configuration** â€” Hierarchical config management

---

## ðŸ“Š Metadata & Attribution

> [!methodology-and-sources] Research Methodology
> **Exploration Tree:** Depth-first across 5 primary dimensions
> 1. Core Functionality & Architecture
> 2. Technical Implementation (file format, Jinja2, execution)
> 3. Workflow Integration (VS Code, version control, providers)
> 4. Practical Applications (best practices, workflows)
> 5. Ecosystem & Tooling (alternatives, complementary tools)
> 
> **Total Searches:** 12 targeted queries
> **Primary Sources:**
> - [Microsoft Prompty GitHub Repository](https://github.com/microsoft/prompty)
> - [Prompty.ai Official Documentation](https://prompty.ai)
> - [VS Code Marketplace Listing](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.prompty)
> - Microsoft Prompt Flow Documentation
> - Various prompt engineering best practices sources
> 
> **Confidence Distribution:**
> - Core functionality: Verified (official documentation)
> - File format specification: Verified (schema + examples)
> - Provider support: Verified (official limitations documented)
> - Integration strategies: Confident (based on markdown format properties)
> - Obsidian integration: Confident (inferred from format compatibility)

---

## ðŸ”— Related Topics for PKB Expansion

### 1. **[[LiteLLM Unified API Gateway]]**

**Connection:** Proxy layer enabling Prompty to work with [[Anthropic]], [[Google]], and other non-OpenAI providers
**Depth Potential:** Complete technical setup guide with configuration examples, provider mappings, and troubleshooting
**Knowledge Graph Role:** Bridge between Prompty (Microsoft tooling) and multi-provider LLM ecosystem
**Priority:** High â€” directly solves your Claude/Gemini integration challenge

### 2. **[[Prompt Versioning & Git Workflows]]**

**Connection:** Best practices for treating prompts as code with proper version control
**Depth Potential:** Comprehensive strategies for branching, commit conventions, PR reviews, and semantic versioning of prompts
**Knowledge Graph Role:** Connects [[Version Control]] principles to [[Prompt Engineering]] practices
**Priority:** High â€” essential for maintaining your prompt engineering library

### 3. **[[Jinja2 Templating for Dynamic Prompts]]**

**Connection:** Deep dive into Prompty's templating engine for variable substitution, conditionals, and loops
**Depth Potential:** Complete Jinja2 syntax reference with prompt-specific patterns, filters, and best practices
**Knowledge Graph Role:** Technical skill note bridging [[Python]] templating to [[LLM]] prompt construction
**Priority:** Medium â€” improves prompt sophistication and reusability

### 4. **[[Obsidian + VS Code Dual-Editor Workflow]]**

**Connection:** Optimizing PKB management when using both Obsidian and VS Code simultaneously
**Depth Potential:** Setup guide, plugin recommendations, sync strategies, and workflow patterns
**Knowledge Graph Role:** Bridges [[Obsidian]] PKB practices with [[VS Code]] development workflows
**Priority:** Medium â€” enhances your specific setup efficiency

### 5. **[[Prompt Engineering Framework Comparison]]** *(Optional)*

**Connection:** Comparative analysis of [[LangChain]], [[Semantic Kernel]], [[Prompt Flow]], and standalone approaches
**Depth Potential:** Feature matrix, use case fit, learning curves, and integration patterns
**Knowledge Graph Role:** Survey note mapping the [[LLM Development Tools]] ecosystem
**Priority:** Low â€” useful for future tool selection, not immediate need

### 6. **[[Claude Code vs. GitHub Copilot vs. Cursor]]** *(Optional)*

**Connection:** Comparative analysis of AI coding assistants to inform tool selection
**Depth Potential:** Feature comparison, workflow differences, and strategic use cases
**Knowledge Graph Role:** Positions [[Claude Code]] within broader AI coding assistant landscape
**Priority:** Low â€” more about tool awareness than immediate actionability

---

**End of Reference Note**








# Gemini-3-Pro ToT [Tree of Thoughts Search Pattern] Reference Note on Prompty


<thinking>
### Phase 1: Research & Knowledge Extraction

#### 1. Initial Scope Mapping via Universal Decomposition

**Topic**: Scripting Platforms and PKM Automation with VS Code

- **Theoretical**
  - What defines a scripting platform?
  - How do scripting languages differ in purpose and design?
  - What is the theoretical basis for PKM automation?

  Sub-questions:
  - What are the core paradigms of scripting (e.g., interpreted vs compiled)?
  - How do scripting languages map to different problem domains?
  - What distinguishes general-purpose from domain-specific scripting?

- **Technical**
  - What are the key technical differences between scripting platforms?
  - What APIs, libraries, and frameworks are available per platform?
  - How does VS Code integrate with scripting languages?

  Sub-questions:
  - What are the performance characteristics of each scripting environment?
  - How do package managers (e.g., npm, pip) support script development?
  - What is the architecture of VS Codeâ€™s extension and scripting system?

- **Applied**
  - What are common use cases for scripting in PKM?
  - How do workflows differ by platform?
  - What are real-world examples of scripting for knowledge work?

  Sub-questions:
  - How can Python scripts automate Obsidian workflows?
  - What are typical tasks automated via JavaScript/Node.js in VS Code?
  - How do templating engines integrate into PKM?

- **Historical/Evolutionary**
  - How have scripting platforms evolved?
  - What historical trends shaped current scripting ecosystems?
  - How has PKM automation evolved over time?

  Sub-questions:
  - What were early scripting tools and their limitations?
  - How did the rise of Markdown and plaintext PKMs change scripting needs?
  - What role did VS Code play in democratizing scripting?

- **Epistemic**
  - What are the limitations of scripting for PKM?
  - What are the current controversies in scripting platform selection?
  - How confident can we be in cross-platform compatibility?

  Sub-questions:
  - Are there platform-specific constraints that limit interoperability?
  - What are the trade-offs between extensibility and stability?
  - What are the security risks of running scripts in PKM environments?

---

#### 2. Strategic Web Research via Adaptive ToT Pathing

**Search 1**  
**QUERY RATIONALE**: I'm searching for "scripting platforms comparison 2025" because I need to understand the current landscape of scripting languages and environments.  
**EXPECTED INSIGHT**: This should reveal major platforms (e.g., Python, JavaScript, Lua), their strengths, and typical use cases.  
**FINDINGS SUMMARY**:  
- Python remains dominant for automation, data science, and scripting in PKM.  
- JavaScript/Node.js is widely used in VS Code extensions and web-based scripting.  
- Lua is lightweight and used in embedded systems and games.  
- Bash is strong for system-level automation.  
- PowerShell is key for Windows environments.  
^verified

**Search 2**  
**QUERY RATIONALE**: I'm searching for "VS Code scripting capabilities and extensions" because I want to understand how VS Code integrates with scripting languages and supports PKM workflows.  
**EXPECTED INSIGHT**: This should reveal VS Codeâ€™s built-in scripting features, extension APIs, and integration with external tools.  
**FINDINGS SUMMARY**:  
- VS Code supports JavaScript/TypeScript via Node.js for extensions.  
- It integrates with Python, Bash, and PowerShell via terminal and extensions.  
- The Extension API allows for deep customization and automation.  
- Tasks and keybindings can be scripted for workflow automation.  
^established

**Search 3**  
**QUERY RATIONALE**: I'm searching for "PKM automation with scripting examples" because I want to see concrete use cases and workflows.  
**EXPECTED INSIGHT**: This should reveal practical examples of scripting for note-taking, linking, templating, and data transformation.  
**FINDINGS SUMMARY**:  
- Python scripts are used to generate daily notes, backlinks, and graph data.  
- JavaScript scripts automate Obsidian via its API and community plugins.  
- Templating systems (e.g., Handlebars) are used in Hugo, Obsidian, and VS Code.  
^verified

**Search 4**  
**QUERY RATIONALE**: I'm searching for "scripting language performance and use case mapping" because I want to understand how to match platforms to specific tasks.  
**EXPECTED INSIGHT**: This should reveal performance benchmarks, ecosystem maturity, and domain-specific strengths.  
**FINDINGS SUMMARY**:  
- Python excels in data processing and integration with libraries.  
- JavaScript is fast for event-driven tasks and UI scripting.  
- Bash is optimal for system-level automation.  
- Lua offers speed and embeddability for niche applications.  
^established

**Search 5**  
**QUERY RATIONALE**: I'm searching for "security and limitations of scripting in PKM tools" because I want to understand the risks and constraints.  
**EXPECTED INSIGHT**: This should reveal potential vulnerabilities, sandboxing issues, and platform-specific limitations.  
**FINDINGS SUMMARY**:  
- Scripts can pose security risks if not sandboxed or reviewed.  
- Obsidian disables arbitrary script execution by default.  
- VS Code requires careful permissions for extension scripting.  
- File-based scripting can be fragile if paths or formats change.  
^provisional

**Self-Verification Sweep**  
1. **Claim**: Python is widely used for PKM automation.  
   - Verified via multiple sources including Obsidian community forums and GitHub projects.  
   ^verified

2. **Claim**: VS Code supports JavaScript/TypeScript for extensions.  
   - Verified via official VS Code documentation and API references.  
   ^established

3. **Claim**: Bash is optimal for system-level automation.  
   - Verified via Unix/Linux scripting guides and DevOps documentation.  
   ^established

---

#### 3. Knowledge Gap & Conflict Analysis

**Gaps Identified**:
- Limited comparative analysis of scripting platforms for PKM-specific use cases.
- Sparse documentation on security implications of PKM scripting.
- Few resources on cross-platform compatibility and portability.

**Conflicts**:
- Some sources overstate JavaScriptâ€™s dominance outside of web contexts.
- Confusion between scripting and full programming languages in some forums.

**Prioritization**:
- Focus on practical, cross-domain utility.
- Emphasize VS Codeâ€™s extensibility and scripting interface.
- Highlight PKM-specific workflows and limitations.

---

### Phase 2: Structural Planning

#### STRUCTURAL REASONING

```
Root Node: Scripting Platforms and PKM Automation
â”œâ”€â”€ Branch A: Foundational Concepts
â”‚   â”œâ”€â”€ Scripting vs Programming
â”‚   â”œâ”€â”€ Language Families
â”‚   â”œâ”€â”€ PKM Automation Theory
â”œâ”€â”€ Branch B: Technical Specifications
â”‚   â”œâ”€â”€ Platform Comparison Table
â”‚   â”œâ”€â”€ VS Code Scripting Architecture
â”‚   â”œâ”€â”€ APIs and Libraries
â”œâ”€â”€ Branch C: Implementation & Application
â”‚   â”œâ”€â”€ PKM Scripting Workflows
â”‚   â”œâ”€â”€ VS Code Automation Examples
â”‚   â”œâ”€â”€ Templating and Integration
â”œâ”€â”€ Branch D: Advanced Considerations
â”‚   â”œâ”€â”€ Security and Limitations
â”‚   â”œâ”€â”€ Future Trends
â”‚   â”œâ”€â”€ Comparative Analysis
â””â”€â”€ Cross-Linking Strategy:
    - [[PKM Automation]]
    - [[VS Code Extensions]]
    - [[Scripting Platforms]]
    - [[Python for PKM]]
    - [[JavaScript for Automation]]
    - [[Templating Systems]]
```

**Justification**:
- This hierarchy supports mastery by building from theory to practice.
- The flow aligns with learning progression: concepts â†’ tools â†’ application â†’ critique.
- Dataview-queryable elements (e.g., `platform`, `use-case`, `confidence`) are embedded in tables and metadata.

</thinking>

---

## ðŸ“ OUTPUT SCAFFOLDING SPECIFICATION (Universal Agentic Template)

> [!comprehensive-reference] ðŸ“šComprehensive-Reference  
> - **Generated**:: [[2025-04-05]]  
> - **Version**:: 1.0  
> - **Type**:: Reference Documentation  
> - **Difficulty**:: [[advanced]]  
> - **Estimated-Study-Time**:: 25min  
> - **Domain-Volatility**:: moderate  

> [!abstract]  
> **Executive Overview**  
> Scripting platforms enable automation, integration, and extension of Personal Knowledge Management (PKM) systems. This reference explores the landscape of scripting languages, their technical characteristics, and practical applications in PKM tools like Obsidian and VS Code. It provides a framework for selecting the right platform based on use case, performance, and integration needs.

> [!how-to-use-this]  
> **Navigation Guide**  
> This reference note is organized into four major sections. Use TOC for navigation or search via [[wiki-links]]. Designed for Dataview, learning paths, and agentic workflows.

## ðŸ“‘ Table of Contents  
[Automatically generated from headers]

### Core Content Architecture  

## 1ï¸âƒ£ Foundational Concepts  
> [!definition]  
> - **Core-Term**:: [Scripting Platform]  
> - **Definition**:: A runtime environment or language framework designed for automating tasks, extending functionality, and integrating systems through lightweight, interpreted code.^established-consensus  

Scripting platforms differ from full programming languages in that they prioritize ease of use, rapid prototyping, and integration with existing systems. They typically execute in an interpreted or just-in-time compiled environment, allowing for dynamic behavior and quick iteration. In the context of PKM, scripting platforms are used to automate note-taking, link creation, metadata generation, and workflow orchestration.

Common scripting paradigms include event-driven scripting (e.g., JavaScript), procedural scripting (e.g., Bash), and data-oriented scripting (e.g., Python). Each paradigm maps to specific use cases in PKM, such as UI automation, system integration, and data transformation.

> [!key-claim]  
> **Central Principle**  
> The selection of a scripting platform should be driven by the specific task, the target environment, and the required performance characteristics.^verified-stable  

## 2ï¸âƒ£ Technical Specifications  

| Platform | Primary Use | Runtime | Package Manager | Integration | Notes |
|----------|-------------|---------|------------------|-------------|-------|
| Python   | Automation, Data | CPython, PyPy | pip | CLI, API | High-level, rich ecosystem^established |
| JavaScript | Web, Extensions | Node.js, V8 | npm | VS Code, Web | Event-driven, async^established |
| Bash     | System Tasks | Shell | N/A | CLI | Fast for system automation^established |
| Lua      | Embedded | LuaJIT | luarocks | Game Engines | Lightweight, fast^provisional |
| PowerShell | Windows | .NET | PSGallery | Windows CLI | Admin-focused^provisional |

> [!equation]  
> **Mathematical/Structural Foundation**  
> $ \text{Task}_{\text{efficiency}} = f(\text{Language}_{\text{type}}, \text{Runtime}_{\text{speed}}, \text{Ecosystem}_{\text{support}}) $  
> Where:  
> - $ \text{Language}_{\text{type}} $: Scripting paradigm (procedural, event-driven, etc.)  
> - $ \text{Runtime}_{\text{speed}} $: Execution performance (interpreted vs JIT)  
> - $ \text{Ecosystem}_{\text{support}} $: Availability of libraries and tools^established  

> [!diagram-idea]  
> **Mermaid Suggestion**  
> ```mermaid  
> graph TD  
>   A[Scripting Platform] --> B[Python]  
>   A --> C[JavaScript]  
>   A --> D[Bash]  
>   B --> E[PKM Automation]  
>   C --> F[VS Code Extensions]  
>   D --> G[System Tasks]  
> ```  
> *Suggested for visualizing platform-task mapping.*

## 3ï¸âƒ£ Implementation & Application  

> [!methodology-and-sources]  
> **Practical Framework**  
> 1. Identify the core task (e.g., note generation, data parsing).  
> 2. Match task to scripting platform based on performance and ecosystem.  
> 3. Integrate via CLI, API, or extension system.  
> 4. Automate via scheduled tasks, hooks, or UI triggers.  

> [!use-cases-and-examples]  
> **Real-World Applications**  
> 1. **Context**: Daily note generation in Obsidian  
>    **Application**: Python script using `datetime` and file I/O  
>    **Outcome**: Automated journaling with templated structure^provisional-volatile  
> 2. **Context**: VS Code task automation  
>    **Application**: JavaScript task runner via `tasks.json`  
>    **Outcome**: One-click build and deploy workflows^established  
> 3. **Context**: Markdown link repair  
>    **Application**: Bash script with `grep` and `sed`  
>    **Outcome**: Batch fix broken internal links^provisional  

## 4ï¸âƒ£ Advanced & Critical Considerations  

> [!warning]  
> **Critical Constraints**  
> - Security risks from untrusted scripts  
> - Platform-specific dependencies and compatibility  
> - Performance bottlenecks in interpreted environments^contested-active  

> [!thought-experiment]  
> **Conceptual Model**  
> Imagine a PKM system where every note is auto-tagged, linked, and versioned. Which scripting platform would handle this best? What trade-offs would you accept in performance vs. ease of implementation?  

> [!quick-reference]  
> **Rapid Lookup**  
> - ðŸ”‘ **Key Point 1**: Python is ideal for data-heavy PKM tasks  
> - ðŸ”‘ **Key Point 2**: JavaScript excels in VS Code extension development  

### Cross-Reference Architecture  

### ðŸ”— Internal Connections  
> [!connections-and-links]  
> **Related Concepts**  
> - See [[#Technical Specifications]] for platform comparison  
> - See [[#Implementation & Application]] for real-world use cases  

### ðŸŒ External Knowledge Graph  
> [!hub-moc]  
> **Broader Knowledge**  
> - Parent: [[PKM Automation]]  
> - Siblings: [[Templating Systems]], [[VS Code Extensions]]  
> - Children: [[Python for PKM]], [[JavaScript for Automation]]  

### Synthesis Section (MANDATORY)  

## ðŸŽ¯ Synthesis & Mastery  

> [!the-philosophy]  
> **Underlying Philosophy**  
> Scripting is the bridge between static knowledge and dynamic insight. It transforms PKM from a passive repository into an active, evolving system.^speculative-personal  

> [!analogy]  
> **Illuminating Comparison**  
> Scripting platforms are like different types of tools in a mechanicâ€™s shopâ€”each optimized for specific jobs, but all contributing to the same goal: building and maintaining a functional system.^established  

### Comparative Analysis  

| Platform | Strengths | Weaknesses | Use When | Confidence |
|----------|-----------|------------|----------|------------|
| Python | Rich libraries, readable syntax | Slower runtime | Data tasks, automation | `^verified` |
| JavaScript | Fast, async support | Complex ecosystem | VS Code, web | `^verified` |
| Bash | Fast, native | OS-dependent | System tasks | `^provisional` |
| Lua | Lightweight, embeddable | Niche ecosystem | Game engines | `^provisional` |

---

## ðŸ“Š Metadata & Attribution  

> [!methodology-and-sources]  
> **Research Methodology**  
> - Synthesis Approach: Tree-of-Thoughts decomposition  
> - Confidence Level: Per-claim tagging (40â€“60% density)  
> - Self-Verification: 3 key claims re-validated  

## ðŸ”„ Version History  

| Version | Date | Changes |
|--------|------|--------|
| 1.0 | 2025-04-05 | Initial comprehensive compilation |

---

### ðŸ”— Related Topics for PKB Expansion  
* [[Advanced Scripting Techniques]]  
* [[Historical Development of Scripting Languages]]  
* [[[Scripting] in Practice: Case Studies]]  
* [[Common Scripting Misconceptions]]  
* [[Future of Scripting: Emerging Trends]]  

### Section 1: PKB Integration  

> [!connections-and-links]  
> **Integration with Cognitive Frameworks**  
> - [[First Principles]] â†’ enables deconstruction of automation needs  
> - [[Systems Thinking]] â†’ maps interdependencies between tools  
> - [[Second-Order Effects]] â†’ anticipates ripple effects of automation  
> - [[Constraint Theory]] â†’ identifies bottlenecks in scripting workflows  
> - [[Mental Models]] â†’ supports cross-domain transfer  

### Section 2: Synthesis & Reflection  

> [!summary]  
> **Summary Synthesis**  
> Scripting platforms offer powerful tools for automating and extending PKM systems. The choice of platform depends on the nature of the task, the target environment, and the required performance. Python and JavaScript dominate due to their ecosystems and versatility, while Bash and Lua serve niche but critical roles.  

> [!ask-yourself-this]  
> **Reflective Questions for Personal Application**  
> *First Reflection*: What repetitive PKM tasks could you automate today using one of these platforms? How would that change your workflow?  
> *Second Reflection*: How might you design a modular PKM system where scripts act as interchangeable components, each handling a specific layer of automation?  

<application_context_markers>  
> [!application-context] Scripting Platforms  
> **Primary Domains**:  
> - [[PKM]] â€” knowledge structuring  
> - [[Education]] â€” instructional design  
> - [[Technical]] â€” system implementation  
>  
> **Trigger Conditions**:  
> - "Repetitive manual task" â†’ [Script it]  
>  
> **Anti-Patterns**:  
> - Over-engineering simple workflows  
>  
> **Implementation Notes**:  
> - Always sandbox external scripts  
> - Use version control for script repositories  

</application_context_markers>  

<epistemic_confidence_encoding>  
Apply ^verified, ^established, ^provisional, ^speculative, ^contested tags to all substantive claims.  
**Density**: 40â€“60%. **Visibility**: Clear in text.  
</epistemic_confidence_encoding>  

<atomic_extraction_signaling>  
> [!atomic-candidate] Scripting Platform Selection (2025)  
> **Slug**: `scripting-platform-selection`  
> **Type**: framework  
> **Priority**: high  
> **Extraction Trigger**: Clear decision matrix with confidence tags  

> [!atomic-candidate] PKM Automation Theory (2025)  
> **Slug**: `pkm-automation-theory`  
> **Type**: principle  
> **Priority**: medium  
> **Extraction Trigger**: Theoretical foundation for automation  

> [!atomic-candidate] VS Code Scripting API (2025)  
> **Slug**: `vscode-scripting-api`  
> **Type**: method  
> **Priority**: high  
> **Extraction Trigger**: Technical specification of extension system  

%%ATOMIC: scripting-paradigms | framework | medium | Core concept in scripting theory%%  
%%ATOMIC: bash-automation-patterns | method | low | Niche but useful%%  
</atomic_extraction_signaling>  

<synthesis_potential_markers>  
> [!synthesis-opportunity] Scripting for Interoperability  
> **Source Domain**: [[PKM]] â€” [[Scripting Platforms]]  
> **Target Domains**: [[Data Science]], [[DevOps]]  
> **Synthesis Type**: analogical  
> **Seed Questions**:  
> - How do scripting patterns in PKM mirror those in DevOps?  
> - Can PKM scripting workflows be generalized to data pipelines?  
> **Next Action**: Map common automation tasks across domains  

</synthesis_potential_markers>  

<temporal_decay_indicators>  
```yaml  
---  
freshness:  
  domain-volatility: moderate  
  last-verified: 2025-04-05  
  decay-rate: 12months  
  staleness-risk: low  
update-triggers:  
  - "New major scripting language release"  
  - "Observed performance degradation"  
  - "Major conference findings"  
review-history:  
  - date: 2025-04-05  
    action: "Initial compilation"  
    outcome: "Still current"  
---  
</temporal_decay_indicators>  

<mental_model_anchors>  
> [!mental-model-anchor] Scripting Platforms  
> **Primary Model Anchors**:  
> - [[First Principles]] â€” [Break down automation needs to core functions]  
> - [[Inversion]] â€” [What breaks if scripting is misapplied?]  
> - [[Systems Thinking]] â€” [Map interdependencies between platforms and tasks]  

> **Inverse Application**:  
> - Can be applied to: [[DevOps]], [[Data Science]]  

</mental_model_anchors>  

<counterexample_collection>  
> [!counterexample] Scripting for PKM  
> **Principle Statement**: Python is always the best choice for PKM scripting.^established  
> **Counterexample**: Bash is faster for simple file-based tasks like renaming or linking.  
> **Boundary Condition**: Task complexity and system integration  
> **Implication**: Match platform to task, not habit  
> **Revised Principle**: Choose the platform that best fits the taskâ€™s performance and integration needs.^verified  

</counterexample_collection>  

<prerequisite_dependency_mapping>  
```yaml  
---  
prerequisites:  
  hard: []  
  soft: [[PKM Fundamentals]]  
enables:  
  direct: [[PKM Automation]]  
  related: [[Templating Systems]]  
difficulty: advanced  
estimated-study-time: 25min  
optimal-sequence-position: middle  
dataview-query-key: "comprehensive-reference"  
---  
```  
</prerequisite_dependency_mapping>





> [!warning] ### ðŸ“… Review Intelligence
> **Next Review**: `= this.next-review` | **Review Count**: `= this.review-count`
> **Review Status**: `= choice(this.next-review < date(today), "ðŸ”´ OVERDUE", choice(this.next-review = date(today), "ðŸŸ¡ Due Today", choice(dateformat(this.next-review, "yyyy-MM-dd") <= dateformat(date(today) + dur(7 days), "yyyy-MM-dd"), "ðŸŸ¢ This Week", "âšª Scheduled")))`
> **Days Until Review**: `= choice(this.next-review, (this.next-review - date(today)).days + " days", "Not scheduled")`
> [!abstract] ### ðŸ·ï¸ Tag Intelligence
> **Tag Count**: `= length(this.tags)` | **Unique Domains**: `= length(filter(this.tags, (t) => contains(t, "/")))` hierarchical tags
> **Tag Density**: `= choice(length(this.tags) < 3, "âš ï¸Sparse", choice(length(this.tags) > 10, "ðŸ“šRich", "âœ…Balanced"))`
>




### Review Information
## ðŸ“… Review System
**Maturity Level**: `= this.maturity`  
**Confidence Level**: `= this.confidence`  
**Review Interval**: 1 week  
**Next Review**: 2026-01-04
### Active Review Task
- [ ] Review [[Comprehensive Reference: Prompty Exstension for VS Code]] (seedling | provisional) ðŸ“… 2026-01-04 ðŸ”¼ ðŸ” every 1 week #review
```tasks
not done
description includes [[Comprehensive Reference: Prompty Exstension for VS Code]]
description includes Review
```

---
