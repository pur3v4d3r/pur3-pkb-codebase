---
title: Reinforcement Learning from Human Feedback
id: 20251111-100858
type: ðŸ§¬concept
tags:
  - topic/
  - type/pur3
aliases:
  - Reinforcement Learning from Human Feedback
  - RLHF
link-up:
  - "[[permeant-note_moc]]"
link-related:
  - "[[Anthropic]]"
  - "[[03-notes/01_permanent-notes/04_prompt-engineering/Claude]]"
  - "[[Claude Project]]"
  - "[[Constitutional Ai]]"
  - "[[Generative Ai]]"
  - "[[Instruction Following]]"
maturity: seedling
confidence: speculative
status: active


review-last-reviewed: null
review-next-review: 2025-12-17
review-count: 0
review-interval: 3

review-priority: medium
---




> [!definition]
> - **Key-Term**:: [[Reinforcement Learning From Human Feedback]]
> - [**Definition**:: This is a machine learning technique that uses human preferences or evaluations as a reward signal to train a model, often a large language model, to align its behavior more closely with human values and intentions.]

> [!questions]
> Active Question During the creation or viewing of this Permeant Note.

