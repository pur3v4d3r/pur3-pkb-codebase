---
title: Cognitive Bias
id: 20251105-175215
type: ðŸ§¬concept
status: active
rating: ""
source: ""
url: https://gemini.google.com/gem/dbb1736d04bf/9f9de806cdaf3be7
tags:
  - cognitive-science
  - project/pur3v4d3r
  - cognitive-science
  - cognitive-science
  - cognitive-science
  - cognitive-science
  - permanent-note
  - permanent-note/project-pur3v4d3r
aliases:
  - Cognitive Bias
  - Cognitive Biases
  - cognitive bias
  - cognitive biases
  - Cognitive Biases_ðŸ†”20251105201124
link-up:
  - "[[self-learning-and-cognitive-development-moc]]"
link-related:
  - "[[Metacognition]]"
  - "[[Metacognitive Monitoring]]"
  - "[[Metacognitive Regulation]]"
  - "[[Calibration Theory]]"
  - "[[Cognitive Distortions]]"
  - "[[Constructivist Epistemology]]"
  - "[[Epistemic Cognition]]"
  - "[[Metacognitive Bias]]"
  - "[[Logical Fallacy]]"
date created: 2025-11-05T20:23:14
date modified: 2025-11-05T21:48:01
maturity: seedling
confidence: speculative


review-last-reviewed: null
review-next-review: 2025-12-17
review-count: 0
review-interval: 3

review-priority: medium
---

> [!comprehensive-reference] ðŸ“šComprehensive-Reference
>
>   - **Generated**:[[2025-11-05]]
>   - **Version**\*\*:1.0
>   - **Type**:Reference Documentation

> [!abstract]
> **Executive Overview**
> A **Cognitive Bias** is a systematic, predictable pattern of deviation from norm or rationality in judgment. These biases are not random errors but are innate "mental shortcuts" (heuristics) that the brain uses to simplify information processing, make rapid decisions, and navigate a complex world. While often adaptive and efficient, these shortcuts lead to consistent, observable errors in thinking, perception, and decision-making.

> [!how-to-use-this]
> **Navigation Guide**
> This reference note is organized into 8 major sections covering all aspects of [[Cognitive Biases]]. It moves from foundational theory (definitions, mechanisms, origins) to practical categorization, key examples, and actionable mitigation strategies. Use the table of contents for navigation.

# ðŸ“‘ TABLE OF CONTENTS

1.  [Foundational Concepts: The "What, How, and Why"](https://www.google.com/search?q=%231-EF-foundational-concepts-the-what-how-and-why)
      - [Definition: Bias vs. Heuristic](https://www.google.com/search?q=%23definition-bias-vs-heuristic)
      - [Core Mechanism: System 1 & System 2 Thinking](https://www.google.com/search?q=%23core-mechanism-system-1--system-2-thinking)
      - [Evolutionary Origins: Why Biases Exist](https://www.google.com/search?q=%23evolutionary-origins-why-biases-exist)
2.  [Intellectual History: The Heuristics & Biases Program](https://www.google.com/search?q=%232----intellectual-history-the-heuristics--biases-program)
      - [Kahneman & Tversky](https://www.google.com/search?q=%23kahneman--tversky)
      - [The Three Original Heuristics](https://www.google.com/search?q=%23the-three-original-heuristics)
3.  [A-Comprehensive-Categorization-The-Cognitive-Bias-Codex](https://www.google.com/search?q=%233---a-comprehensive-categorization-the-cognitive-bias-codex)
      - [Quadrant 1: Too Much Information](https://www.google.com/search?q=%23quadrant-1-too-much-information)
      - [Quadrant 2: Not Enough Meaning](https://www.google.com/search?q=%23quadrant-2-not-enough-meaning)
      - [Quadrant 3: The Need to Act Fast](https://www.google.com/search?q=%23quadrant-3-the-need-to-act-fast)
      - [Quadrant 4: What Should We Remember?](https://www.google.com/search?q=%23quadrant-4-what-should-we-remember)
4.  [Key Examples: A-Rapid-Reference-Catalog](https://www.google.com/search?q=%234---key-examples-a-rapid-reference-catalog)
      - [[Anchoring Bias]]
      - [[Availability Heuristic]]
      - [[Confirmation Bias]]
      - [[Dunning-Kruger Effect]]
      - [[Fundamental Attribution Error]]
      - [[Hindsight Bias]]
      - [[Sunk Cost Fallacy (Irrational Escalation)]]
      - [[Survivorship Bias]]
5.  [Key Theoretical Model: Prospect Theory](https://www.google.com/search?q=%235---key-theoretical-model-prospect-theory)
      - [Definition & Core Claims](https://www.google.com/search?q=%23definition--core-claims)
      - [The Asymmetric Value Function (Loss Aversion)](https://www.google.com/search?q=%23the-asymmetric-value-function-loss-aversion)
6.  [Real-World-Impacts](https://www.google.com/search?q=%236----real-world-impacts)
      - [Finance & Economics](https://www.google.com/search?q=%23finance--economics)
      - [Medicine](https://www.google.com/search?q=%23medicine)
      - [Management & Law](https://www.google.com/search?q=%23management--law)
7.  [Mitigation-Strategies-Debiasing](https://www.google.com/search?q=%237----mitigation-strategies-debiasing)
      - [A Framework for Debiasing](https://www.google.com/search?q=%23a-framework-for-debiasing)
8.  [Synthesis-Mastery](https://www.google.com/search?q=%238----synthesis--mastery)
      - [Cognitive Models](https://www.google.com/search?q=%23cognitive-models)
      - [Illuminating Analogy](https://www.google.com/search?q=%23illuminating-analogy)
      - [Comparative Analysis](https://www.google.com/search?q=%23comparative-analysis)
9.  [Connections & Metadata](https://www.google.com/search?q=%239----connections--metadata)
      - [Internal Connections](https://www.google.com/search?q=%23-internal-connections)
      - [External Knowledge Graph](https://www.google.com/search?q=%23-external-knowledge-graph)
      - [Metadata & Attribution](https://www.google.com/search?q=%23-metadata--attribution)
      - [Version History](https://www.google.com/search?q=%23-version-history)
      - [Related Topics for PKB Expansion](https://www.google.com/search?q=%23-related-topics-for-pkb-expansion)

-----

# 1. ðŸ§  FOUNDATIONAL CONCEPTS: THE "WHAT, HOW, AND WHY"

This section outlines the three pillars of understanding cognitive bias: its precise definition, the mental mechanism that powers it, and the evolutionary reason for its existence.

## DEFINITION: BIAS VS. HEURISTIC

> [!definition]
>
>   - **Key-Term**:[[Heuristic]]
>   - **Definition**:A mental shortcut or "rule of thumb" that the brain uses to make a complex judgment or decision quickly and with minimal cognitive effort. Heuristics are the *process*.
> 
> -----
>
>   - **Key-Term**:[[Cognitive Biases]]
>   - **Definition**:The systematic, predictable, and non-random *error* in judgment that results from the application of a heuristic. Biases are the *outcome*.

It is impossible to understand biases without understanding heuristics. The human brain has limited processing power and time. To compensate, it develops heuristics (shortcuts) that are "good enough" most of the time.

A [[Cognitive Biases]] is what happens when one of these shortcuts is applied in the wrong context, leading to a predictable error. For example, the [[Availability Heuristic]] (judging the likelihood of an event by how easily it comes to mind) is a fast shortcut. However, it reliably produces the [[Availability Bias]], where we overestimate the frequency of shark attacks because they are vivid and heavily reported, not because they are statistically common.
[[Availability Heuristic]]

## CORE MECHANISM: SYSTEM 1 & SYSTEM 2 THINKING

The most robust model for *how* biases occur comes from psychologists [[Daniel Kahneman]] and [[Keith Stanovich]], popularized in the book *Thinking, Fast and Slow*. It posits two distinct modes of thought operating in the brain.

> [!key-claim]
> **Central Principle**
> Cognitive biases are the domain of [[System 1]] thinking. They occur when the fast, intuitive System 1 provides a flawed answer and the slow, lazy [[System 2]] fails to engage and correct it.
[[System 2]]
[[System 1]]

| System | Characteristics | Function | Role in Bias |
|---|---|---|---|
| **[[System 1]]** | **Fast**, automatic, intuitive, emotional, unconscious, high-capacity, low-effort | Manages 98% of daily tasks: driving a car, understanding language, recognizing faces, reacting to danger | **The source of biases**. It uses heuristics to generate "good enough" answers instantly. |
| **[[System 2]]** | **Slow**, deliberate, analytical, logical, conscious, limited-capacity, high-effort | Manages complex tasks: solving a math problem, evaluating a complex argument, parallel parking | **The intended corrector of bias**. Its job is to audit, question, and override the flawed impulses of System 1, but it is "lazy" and easily fatigued. |

## EVOLUTIONARY ORIGINS: WHY BIASES EXIST

Cognitive biases are not arbitrary flaws; they are features of an evolved mind. They are adaptive shortcuts that were honed by natural selection to solve recurring problems in our ancestral environment, prioritizing speed and survival over 100% accuracy.

> [!the-philosophy]
> **Underlying Philosophy**
> Biases are the result of an "ancestral mind" operating in a "modern world." A shortcut that was highly adaptive for survival on the savanna (e.g., "rustling in the grass = predator = run") is often *maladaptive* when applied to complex, data-rich modern domains like financial markets or medical diagnosis.

Reasons for their existence include:

  - **Cognitive Efficiency**: [[System 2]] thinking is metabolically expensive (it burns glucose). Heuristics (System 1) are cheap and fast, saving precious cognitive energy for when it's truly needed.
  - **Error Management Theory**: In an uncertain world, not all errors are equal. A bias may persist if it leads to the *less costly* error.
      - **Example**: Believing a stick is a snake (a "false positive") costs little. Believing a snake is a stick (a "false negative") can be fatal. The brain is biased toward the "better safe than sorry" error.

-----

# 2. ðŸ“œ INTELLECTUAL HISTORY: THE HEURISTICS & BIASES PROGRAM

The formal study of cognitive bias was pioneered by two psychologists who would go on to win the Nobel Prize in Economics.

## KAHNEMAN & TVERSKY

In the 1970s, **[[Daniel Kahneman]]** and **[[Amos Tversky]]** launched the "Heuristics and Biases" program. Their research (Source 2.1, 2.2) overturned the prevailing "rational agent" model of economics, which assumed humans made logical, utility-maximizing decisions. They demonstrated, through simple and elegant experiments, that human decision-making is predictably *irrational* due to its reliance on a small set of mental shortcuts.

## THE THREE ORIGINAL HEURISTICS

Their initial work identified three foundational heuristics that explain a wide range of biases.

> [!quick-reference]
> **Rapid Lookup: The Original Heuristics**
>
> 1.  **[[Representativeness Heuristic]]**: Judging the probability of something based on how well it *matches* a prototype or stereotype, while ignoring statistical base rates.
>       - *Bias*: Believing a quiet, poetry-reading person is more likely to be an Ivy League Classics professor than a truck driver (ignoring the fact that there are thousands of times more truck drivers than Classics professors).
> 2.  **[[Availability Heuristic]]**: Judging the frequency of an event by the *ease* with which instances come to mind.
>       - *Bias*: Overestimating the risk of plane crashes after seeing one on the news.
> 3.  **[[Anchoring and Adjustment]]**: Making an estimate by starting from an initial value (the "anchor") and making insufficient *adjustments* away from it, even if the anchor is arbitrary.
>       - *Bias*: A negotiator's first offer, no matter how high, sets the "anchor" that all subsequent numbers are judged against.

-----

# 3. ðŸ—ºï¸ A COMPREHENSIVE CATEGORIZATION: THE COGNITIVE BIAS CODEX

While hundreds of biases have been identified, they are not random. They can be grouped by the *fundamental cognitive problem* they are trying to solve. This framework, based on Buster Benson's work (Source 1.1, 1.4, 1.6), organizes all biases into four main quadrants.

## QUADRANT 1: TOO MUCH INFORMATION

The world is an overwhelming flood of data. Our brains must filter almost all of it out. Biases in this quadrant help us select what to pay attention to.

> [!key-claim]
> **Central Principle**
> "We notice things that are primed, repeated, or confirm our existing beliefs. We ignore the rest."

  - **Key Biases Include**:
      - **[[Confirmation Bias]]**: We see and value evidence that supports what we already believe.
      - **[[Availability Heuristic]]**: We value information that is recent, vivid, or easy to recall.
      - **[[Anchoring Bias]]**: We over-value the first piece of information we receive.
      - **[[Frequency Illusion (Baader-Meinhof Phenomenon)]]**: We notice things more often *after* we first learn about them (e.g., you buy a red car and suddenly see red cars everywhere).

## QUADRANT 2: NOT ENOUGH MEANING

The world is ambiguous. Once we filter data, we must "fill in the gaps" to make sense of it and create a coherent story.

> [!key-claim]
> **Central Principle**
> "We find stories and patterns even in sparse data. We fill in the gaps with stereotypes, generalities, and our own assumptions."

  - **Key Biases Include**:
      - **[[Fundamental Attribution Error]]**: We attribute others' actions to their *character* ("he's a jerk") but our own to the *situation* ("I was in a hurry").
      - **[[Halo Effect]]**: When we see one good trait in a person (e.g., they are attractive), we assume other good traits (e.g., they are also smart and kind).
      - **[[Just-World Hypothesis]]**: A bias to believe the world is fair, which leads to blaming victims ("they must have done something to deserve it").
      - **[[Gambler's Fallacy]]**: Believing a "run" of random events (e.g., 5 coin flips of "heads") will be corrected by an opposite outcome ("tails must be due").

## QUADRANT 3: THE NEED TO ACT FAST

We are constrained by time and information. We must be able to make a decision quickly to survive and thrive. Biases in this quadrant help us jump to conclusions.

> [!key-claim]
> **Central Principle**
> "To act, we must be confident in our ability to make an impact and feel what we do is important. We favor simple, immediate options over complex, delayed ones."

  - **Key Biases Include**:
      - **[[Sunk Cost Fallacy]]**: We over-value things we have already invested time, money, or effort in, even if it's rational to abandon them.
      - **[[Optimism Bias]] / [[Overconfidence Bias]]**: We are overly optimistic about our own abilities and the likelihood of positive outcomes.
      - **[[Dunning-Kruger Effect]]**: The least competent people are the most likely to overestimate their competence.
      - **[[Status Quo Bias]]**: We prefer to keep things the same, as change (even for the better) is seen as a risk.

## QUADRANT 4: WHAT SHOULD WE REMEMBER?

We cannot store everything. We must choose what is important and worth saving. Our memories are *re-written* and *edited* over time to be more efficient.

> [!key-claim]
> **Central Principle**
> "We discard specifics to form generalities. We edit and reinforce memories after the fact to fit our current understanding, creating a feedback loop."

  - **Key Biases Include**:
      - **[[Hindsight Bias]] ("I-knew-it-all-along")**: After an event occurs, we misremember our past predictions and believe the outcome was obvious.
      - **[[Misinformation Effect]]**: Our memories of an event are altered if we are later exposed to misleading information about it.
      - **[[Peak-End Rule]]**: We judge an entire past experience based on its *peak* (most intense moment) and its *end*, rather than the average of the whole experience.
      - **[[Rosy Retrospection]]**: We remember the past as being better than it actually was.

-----

# 4\. ðŸ”‘ KEY EXAMPLES: A RAPID REFERENCE CATALOG

This section provides brief, technical definitions for the most commonly-cited biases.

> [!definition]
>
>   - **Key-Term**:**[[Anchoring Bias]]**
>   - **Definition**:The tendency to rely heavily on the first piece of information (the "anchor") offered when making decisions. Subsequent judgments are made by "adjusting" away from that anchor, but these adjustments are almost always insufficient.
>   - **Category**:[[\#Quadrant 1: Too Much Information]]

> [!definition]
>
>   - **Key-Term**:**[[Availability Heuristic]]**
>   - **Definition**:A mental shortcut for judging the likelihood of an event based on the *ease* with which examples come to mind. Vivid, recent, or emotionally charged events are perceived as more common than they are.
>   - **Category**:[[\#Quadrant 1: Too Much Information]]

> [!definition]
>
>   - **Key-Term**:**[[Confirmation Bias]]**
>   - **Definition**:The tendency to search for, interpret, favor, and recall information in a way that confirms or supports one's preexisting beliefs or hypotheses, while actively avoiding or devaluing contradictory evidence.
>   - **Category**:[[\#Quadrant 1: Too Much Information]]

> [!definition]
>
>   - **Key-Term**:**[[Dunning-Kruger Effect]]**
>   - **Definition**:A cognitive bias in which people with low ability at a task overestimate their ability. It is a metacognitive failure where the incompetence that prevents good performance also prevents the person from *recognizing* their own incompetence.
>   - **Category**:[[\#Quadrant 3: The Need to Act Fast]]

> [!definition]
>
>   - **Key-Term**:**[[Fundamental Attribution Error]]**
>   - **Definition**:The tendency to over-emphasize dispositional or personality-based explanations for others' behaviors while under-emphasizing situational explanations. (e.g., "He cut me off because he's a bad person," not "He might be in a medical emergency.")
>   - **Category**:[[\#Quadrant 2: Not Enough Meaning]]

> [!definition]
>
>   - **Key-Term**:**[[Hindsight Bias]]**
>   - **Definition**:The tendency, after an event has occurred, to see the event as having been predictable, despite there having been little or no objective basis for predicting it ("I knew it all along").
>   - **Category**:[[\#Quadrant 4: What Should We Remember?]]

> [!definition]
>
>   - **Key-Term**:**[[Sunk Cost Fallacy (Irrational Escalation)]]**
>   - **Definition**:The tendency to continue a behavior or endeavor as a result of previously invested resources (time, money, or effort), even when it is clear that the costs of continuing outweigh the expected benefits.
>   - **Category**:[[\#Quadrant 3: The Need to Act Fast]] (Also linked to [[\#Loss Aversion]])

> [!definition]
>
>   - **Key-Term**:**[[Survivorship Bias]]**
>   - **Definition**:A logical error of concentrating on the people or things that "survived" a process and inadvertently overlooking those that did not, leading to overly optimistic conclusions drawn from a skewed data set.
>   - **Category**:[[\#Quadrant 2: Not Enough Meaning]] (Also [[\#Quadrant 1: Too Much Information]])

-----

# 5. ðŸ“ˆ KEY THEORETICAL MODEL: PROSPECT THEORY

Beyond the general heuristics, [[Prospect Theory]] is the single most important model for understanding decision-making under risk, particularly in [[Behavioral Economics]].

## DEFINITION & CORE CLAIMS

> [!definition]
>
>   - **Key-Term**:**[[Prospect Theory]]**
>   - **Definition**:A descriptive model of decision-making (developed by [[Kahneman & Tversky]]) showing that people choose between probabilistic alternatives based on the *potential value of losses and gains* relative to a *reference point* (e.g., the status quo), rather than based on the final *absolute* outcome.

This theory makes two central claims:

1.  **Reference Point Dependence**: People evaluate outcomes as "gains" or "losses" relative to their current state (the reference point), not in terms of absolute wealth.
2.  **Loss Aversion**: The psychological and emotional impact of a loss is *far greater* than that of an equivalent gain.

## THE ASYMMETRIC VALUE FUNCTION (LOSS AVERSION)

The core of Prospect Theory is its S-shaped value function, which demonstrates [[Loss Aversion]].

> [!key-claim]
> **Central Principle: [[Loss Aversion]]**
> "Losses loom larger than gains." Research (Source 2.2, 2.8) suggests the pain of losing $100 is roughly **twice as powerful** as the pleasure of gaining $100.

This asymmetry explains several key biases:

  - **Risk-Averse for Gains**: People prefer a **sure gain** of $500 over a 50% chance of gaining $1,000 (they are afraid of getting nothing).
  - **Risk-Seeking for Losses**: People prefer a 50% chance of **losing** $1,000 over a **sure loss** of $500 (they will gamble to *avoid* the certain loss).
  - **[[Framing Effect]]**: How a choice is *framed* (as a gain vs. a loss) will determine the choice, even if the outcomes are identical.
      - *Frame 1 (Gain)*: "Program A will save 200 of 600 people." (People choose this *certain* save).
      - *Frame 2 (Loss)*: "Program B has a 1/3 chance of saving 600 people and a 2/3 chance of saving no one." (This is the same as 400 people dying, but people now become risk-seeking).
  - **[[Endowment Effect]]**: We overvalue things we own simply because we *own* them. Giving up an item is perceived as a "loss."

-----

# 6. ðŸŒ REAL-WORLD IMPACTS

Cognitive biases are not just academic curiosities. They have massive, measurable impacts on critical domains.

## FINANCE & ECONOMICS

(Source 6.1)

  - **[[Overconfidence Bias]]**: Traders believe they can "beat the market," leading to excessive trading and poor returns.
  - **[[Loss Aversion]] / [[Sunk Cost Fallacy]]**: Investors hold on to losing stocks for far too long (to avoid realizing the "loss") and sell winning stocks too early (to lock in the "gain").
  - **[[Herd Behavior]]**: Investors follow the crowd, piling into a hot asset (like a bubble) or panic-selling during a crash, amplifying market volatility.

## MEDICINE

(Source 6.2, 6.3)

  - **[[Anchoring Bias]]**: A physician's diagnosis "anchors" on the initial symptoms or a premature hypothesis, causing them to ignore or misinterpret subsequent contradictory data.
  - **[[Availability Bias]]**: A doctor who just treated a rare disease may overestimate its prevalence and misdiagnose a patient with common symptoms.
  - **[[Confirmation Bias]]**: A doctor orders tests not to find the *right* answer, but to *confirm* their initial diagnosis, while ignoring tests that might disprove it.

## MANAGEMENT & LAW

(Source 6.2)

  - **[[Halo Effect]] (Management)**: A manager sees that an employee is charismatic and assumes they are also competent and productive, leading to biased performance reviews.
  - **[[Fundamental Attribution Error]] (Management)**: A manager attributes a missed deadline to an employee's "laziness" (disposition) rather than an "unrealistic timeline" (situation).
  - **[[Hindsight Bias]] (Law)**: After a tragedy, juries and judges are prone to believe the negative outcome was "obvious" and that the defendant *should have* foreseen it.

-----

# 7. ðŸ›¡ï¸ MITIGATION STRATEGIES (DEBIASING)

Overcoming biases is exceptionally difficult because they are automatic and unconscious. Simple *awareness* of a bias is often insufficient to prevent it (Source 7.3). This is known as the **Bias Blind Spot**: the tendency to see biases in *others* but not in *ourselves*.

Effective mitigation (or "debiasing") focuses on building systems and processes that engage [[System 2]] thinking.

## A FRAMEWORK FOR DEBIASING

| Strategy | Description | Target Biases |
|---|---|---|
| ðŸ§  **Metacognition** | **"Thinking about your thinking."** Actively pausing to ask: "What am I assuming?", "What info might I be missing?", "How might I be wrong?" | All Biases, esp. Overconfidence |
| ðŸ¢ **Slow Down** | Force the use of **[[System 2]]**. Resist the immediate, intuitive answer from [[System 1]]. Use checklists, algorithms, or formal decision-making frameworks. | Anchoring, Availability, Framing |
| ðŸ˜ˆ **Seek Disconfirmation** | Actively search for evidence that *contradicts* your hypothesis. Appoint a "devil's advocate" in a group meeting. | **[[Confirmation Bias]]** (Primary counter) |
| ðŸ”­ **Take the "Outside View"** | Instead of analyzing your specific situation, ask "What happens to *most* people in this situation?" Use base rates and statistical data. | Planning Fallacy, Optimism Bias |
| ðŸ”„ **Re-frame the Problem** | Consciously re-state the problem in different ways. If framed as a "loss," re-frame it as a "gain" and see if your decision changes. | [[Framing Effect]], [[Loss Aversion]] |
| ðŸ“ **Conduct a Pre-Mortem** | Before starting a project, imagine it has failed spectacularly. Have the team write down *why* it failed. This surfaces risks normally hidden by optimism. | Optimism Bias, Planning Fallacy |

-----

# 8. ðŸŽ¯ SYNTHESIS & MASTERY

## COGNITIVE MODELS

  - **The Brain as a "Cognitive Miser"**: This model views the brain as having a limited budget of cognitive resources. It seeks to spend as little energy as possible, so it defaults to "low-cost" [[System 1]] heuristics, accepting the "cost" of occasional biases.
  - **The "Flawed Lens"**: Biases are not a "bug" in our software but a permanent *feature* of our hardware. They are the fixed lens through which we perceive reality. We cannot *remove* the lens, but we can learn *how* it distorts and use tools (like [[System 2]] processes) to correct for the distortion.

## ILLUMINATING ANALOGY

> [!analogy]
> **Illuminating Comparison: Optical vs. Cognitive Illusions**
> A cognitive bias is a "cognitive illusion" that is as powerful and predictable as an optical illusion.
>
> Consider the **MÃ¼ller-Lyer illusion** (two lines of equal length, but one *looks* longer due to inward vs. outward-facing arrows).
>
> 1.  **System 1** *sees* one line as longer. This is automatic and irresistible.
> 2.  **System 2** can *know* the truth. It can get a ruler and measure the lines, confirming they are identical.
> 3.  **The illusion does not go away.** Even after *knowing* the truth (System 2), your brain (System 1) *still perceives* the lines as different lengths.
> 
> This is exactly how cognitive biases work. You can "know" about [[Confirmation Bias]], but your System 1 will *still* automatically highlight confirming evidence. You must actively "get the ruler" (engage a System 2 process, like seeking disconfirmation) to find the truth.

## COMPARATIVE ANALYSIS

| Concept | Definition | Type | Example |
|---|---|---|---|
| **[[as]]** | A *systematic error* in thinking; a predictable deviation from rationality. | Unconscious Error (System 1) | **[[Sunk Cost Fallacy]]**: "I'll keep watching this bad movie because I already paid for the ticket." |
| **[[Logical Fallacy]]** | An *error in reasoning* or a flaw in the structure of a logical argument. | Conscious or Unconscious Error (System 2) | **Straw Man**: "You want to reduce defense spending? So you want to leave the country defenseless?" |
| **[[Heuristic]]** | A *mental shortcut* or rule of thumb used to simplify decision-making. | Unconscious Process (System 1) | **[[Availability Heuristic]]**: "I can think of 3 friends who got the flu, so it must be everywhere." |

-----

# 9. ðŸ”— CONNECTIONS & METADATA

## ðŸ”— INTERNAL CONNECTIONS

> [!connections-and-links]
> **Related Concepts Within This Note**
>
>   - See [[https://www.google.com/search?q=%231-EF-foundational-concepts-the-what-how-and-why]] for the core definition and mechanism.
>   - Compare [[\#Definition: Bias vs. Heuristic]] with [[\#Comparative Analysis]] to understand the distinction from logical fallacies.
>   - See [[https://www.google.com/search?q=%235---key-theoretical-model-prospect-theory]] for the deep-dive on [[Loss Aversion]] and [[Framing Effect]].
>   - See [[https://www.google.com/search?q=%237----mitigation-strategies-debiasing]] for practical counters to the biases listed in [[https://www.google.com/search?q=%234---key-examples-a-rapid-reference-catalog]].

## ðŸŒ EXTERNAL KNOWLEDGE GRAPH

> [!hub-moc]
> **Connection to Broader Knowledge**
>
>   - **Parent Topic**: [[Cognitive Science]], [[Psychology]], [[Behavioral Economics]]
>   - **Sibling Topics**: [[Logical Fallacies]], [[Mental Models]], [[Heuristics]]
>   - **Child Topics**: [[Confirmation Bias]], [[Anchoring]], [[Loss Aversion]], [[Prospect Theory]], [[System 1 and System 2 Thinking]], [[Availability Heuristic]], [[Sunk Cost Fallacy]]

-----

# ðŸ“Š METADATA & ATTRIBUTION

> [!methodology-and-sources]
> **Research Methodology**
>
>   - **Primary Sources**:
>     1.  [Cognitive bias definition and categorization](https://www.google.com/search?q=httpsAfter%2520seeing%2520several%2520news%2520reports%2520of%2520car%2520thefts%2520in%2520your%2520neighborhood%252C%2520you%2520might%2520start%2520to%2520believe%2520that%2520such%2520crimes%2520are%2520more%2520common%2520than%2520they%2520are.) (positivepsychology.com, ebsco.com, wikipedia.org)
>     2.  [Kahneman & Tversky's Heuristics and Biases Program](https://pubmed.ncbi.nlm.nih.gov/17835457/) (cambridge.org, pubmed.ncbi.nlm.nih.gov)
>     3.  [System 1 & System 2 Thinking](https://thedecisionlab.com/reference-guide/philosophy/system-1-and-system-2-thinking) (thedecisionlab.com, wikipedia.org)
>     4.  [Key Bias Examples (Confirmation, Anchoring, etc.)](https://www.verywellmind.com/cognitive-biases-distort-thinking-2794763) (verywellmind.com, pesec.no, achology.com)
>     5.  [Evolutionary Origins of Bias](https://law.vanderbilt.edu/charting-the-evolutionary-roots-of-cognitive-biases/) (vanderbilt.edu, researchgate.net)
>     6.  [Real-World Impact (Finance, Medicine)](https://ijsi.in/wp-content/uploads/2025/07/18.02.026.20251003.pdf) (ijsi.in, evidentiauniversity.com, rcpe.ac.uk)
>     7.  [Debiasing & Mitigation Strategies](https://fiveable.me/cognitive-psychology/unit-18/debiasing-techniques/study-guide/5ACTqJiW0fu6KJ16) (fiveable.me, journalofethics.ama-assn.org, wikipedia.org)
>     8.  [Cognitive Bias Codex Framework](https://buster.medium.com/cognitive-bias-cheat-sheet-55a472476b18) (medium.com, teachthought.com)
>     9.  [Prospect Theory & Loss Aversion](https://www.google.com/search?q=httpsaxzz5w4xZfJ0B) (investopedia.com, behavioraleconomics.com, corporatefinanceinstitute.com)
>   - **Research Queries**: `cognitive bias definition psychology and categorization`, `Kahneman and Tversky heuristics and biases program`, `System 1 and System 2 thinking cognitive bias`, `list of most common cognitive biases confirmation availability anchoring sunk cost`, `underlying causes and evolutionary origins of cognitive bias`, `real-world impact of cognitive bias in finance medicine`, `cognitive bias debiasing techniques and mitigation strategies`, `cognitive bias codex four quadrants explained`, `prospect theory and loss aversion kahneman tversky definition`
>   - **Synthesis Approach**: The note was synthesized by establishing the foundational "What/How/Why" (Definition, System 1/2, Evolution) as the base. The primary organizational structure (Section 3) is based on the "Cognitive Bias Codex" framework (Source 1.1, 1.4, 1.6), which groups biases by the cognitive problem they solve. This is a more robust, non-arbitrary structure than a simple alphabetical list. Key theories (Kahneman/Tversky's original heuristics, Prospect Theory) were given their own sections to provide proper historical and theoretical depth.
>   - **Confidence Level**: **High**. The research provided consistent, reinforcing, and authoritative information from academic, psychological, and economic sources, covering all aspects of the topic from history to practical application.

# ðŸ”„ VERSION HISTORY

| Version | Date | Changes |
|---|---|---|
| 1.0 | 2025-11-05 | Initial comprehensive compilation based on systematic research. |

-----

## ðŸ”— RELATED TOPICS FOR PKB EXPANSION

  - [[Advanced Debiasing Techniques]]
  - [[Historical Development of Behavioral Economics]]
  - [[Cognitive Biases in Practice: Case Studies]]
  - [[Common Logical Fallacies]]
  - [[List of all Cognitive Biases]]
  - [[Future of Cognitive Bias Research: AI & Nudging]]

This video [The Beautiful Cognitive Bias Codex](https://www.youtube.com/watch?v=XxSu_qnxJ54) provides a great visual overview of the "Codex" framework used in Section 3 to categorize the hundreds of known biases.

<http://googleusercontent.com/youtube_content/0>
